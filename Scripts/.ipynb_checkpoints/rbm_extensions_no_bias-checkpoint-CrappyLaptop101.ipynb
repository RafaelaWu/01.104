{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rbm_no_bias as rbm\n",
    "import projectLib as lib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training = lib.getTrainingData()\n",
    "validation = lib.getValidationData()\n",
    "# You could also try with the chapter 4 data\n",
    "# training = lib.getChapter4Data()\n",
    "\n",
    "trStats = lib.getUsefulStats(training)\n",
    "vlStats = lib.getUsefulStats(validation)\n",
    "\n",
    "K = 5\n",
    "alpha = 0.9\n",
    "\n",
    "# SET PARAMETERS HERE!!!\n",
    "# number of hidden units\n",
    "F = 5\n",
    "epochs = 30\n",
    "gradientLearningRate = 0.0001\n",
    "gradientLearningRate_v = 0.001\n",
    "gradientLearningRate_h = 0.001\n",
    "_lambda = 1\n",
    "minibatch_size = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_get(array, B):\n",
    "    ret = []\n",
    "    for i in range(int(len(array)/B)):\n",
    "        ret.append(array[i*B:i*B+B])\n",
    "    if len(array)%B != 0:\n",
    "        ret.append(array[len(array)/B:])\n",
    "    return ret\n",
    "\n",
    "def adaptiveLearn(learning_rate_type=['constant','adapt','time', 'step', 'exponential'], \n",
    "                  lr=0.0001, k=0.1, epoch=None, drop=0.5, epochs_drop=10.0):\n",
    "#time-based decay, step decay and exponential decay\n",
    "    if learning_rate_type == 'constant':\n",
    "        return lr\n",
    "    elif learning_rate_type == 'adapt':\n",
    "        return lr/(epoch**2)\n",
    "    elif learning_rate_type == 'time':\n",
    "        return lr/(1.0+k*epoch)\n",
    "    elif learning_rate_type == 'step':\n",
    "        return lr*np.power(drop,np.floor((1+epoch)/epochs_drop))\n",
    "    elif learning_rate_type == 'exponential':\n",
    "        return lr*np.exp(-k*epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_rbm(training=training, validation=validation, trStats=trStats, vlStats=vlStats, \n",
    "             K=5, F=5, epochs=30, gradientLearningRate=0.0001,gradientLearningRate_v = 0.001,\n",
    "             gradientLearningRate_h = 0.001, minibatch_size=10, alpha=0.9, \n",
    "             stopping=False, momentum=False, learning_rate_type='time', learning_rate_k=0.1, \n",
    "             learning_rate_drop=0.5, learning_rate_epochs_drop=10.0, _lambda = 0.3):\n",
    "    \n",
    "    # Print current hyperparams\n",
    "#     frame = inspect.currentframe()\n",
    "#     args, _, _, values = inspect.getargvalues(frame)\n",
    "#     print ('Training and Predicting with the following hyperparameters:')\n",
    "#     for i in args[4:]:\n",
    "#         print (\"    %s = %s\" % (i, values[i]))\n",
    "        \n",
    "    # Initialise all our arrays\n",
    "    num_movies=trStats[\"n_movies\"]\n",
    "    num_users=trStats[\"n_users\"]\n",
    "    W = rbm.getInitialWeights(trStats[\"n_movies\"], F, K)\n",
    "    posprods = np.zeros(W.shape)\n",
    "    negprods = np.zeros(W.shape)\n",
    "    grad_w = np.zeros(W.shape)\n",
    "    m_w=np.zeros((W.shape[0],F,5))\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    vis_bias=np.zeros((num_movies,5))\n",
    "    m_v=np.zeros((num_movies,5))\n",
    "    hid_bias=np.zeros((F,))\n",
    "    m_h=np.zeros((F,))\n",
    "    best_train_loss = 100\n",
    "    best_validation_loss = 100\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "    #     mini_batch_grads = []\n",
    "        # in each epoch, we'll visit all users in a random order\n",
    "        visitingOrder = np.array(trStats[\"u_users\"])\n",
    "        np.random.shuffle(visitingOrder)\n",
    "#         for i in range(0, visitingOrder.shape[0], minibatch_size):\n",
    "#                 # Get pair of (X, y) of the current minibatch/chunk\n",
    "#             visitingOrderMini = visitingOrder[i:i + minibatch_size]\n",
    "#                 y_train_mini = y_train[i:i + minibatch_size]\n",
    "#         for i in range(0, visitingOrder.shape[0], minibatch_size):\n",
    "        batches = batch_get(visitingOrder, minibatch_size)\n",
    "        for batch in batches:\n",
    "            prev_grad = grad_w\n",
    "            grad_w = np.zeros(W.shape)\n",
    "            for user in batch:\n",
    "                # get the ratings of that user\n",
    "                ratingsForUser = lib.getRatingsForUser(user, training)\n",
    "\n",
    "                # build the visible input\n",
    "                v = rbm.getV(ratingsForUser)\n",
    "\n",
    "                # get the weights associated to movies the user has seen\n",
    "                weightsForUser = W[ratingsForUser[:, 0], :, :]\n",
    "\n",
    "                ### LEARNING ###\n",
    "                # propagate visible input to hidden units\n",
    "                posHiddenProb = rbm.visibleToHiddenVec(v, weightsForUser) #, hid_bias)\n",
    "                # get positive gradient\n",
    "                # note that we only update the movies that this user has seen!\n",
    "                posprods[ratingsForUser[:, 0], :, :] += rbm.probProduct(v, posHiddenProb)\n",
    "\n",
    "                ### UNLEARNING ###\n",
    "                # sample from hidden distribution\n",
    "                sampledHidden = rbm.sample(posHiddenProb)\n",
    "                # propagate back to get \"negative data\"\n",
    "                negData = rbm.hiddenToVisible(sampledHidden, weightsForUser)#, vis_bias[ratingsForUser[:,0]])\n",
    "                # propagate negative data to hidden units\n",
    "                negHiddenProb = rbm.visibleToHiddenVec(negData, weightsForUser) #, hid_bias)\n",
    "                # get negative gradient\n",
    "                # note that we only update the movies that this user has seen!\n",
    "                negprods[ratingsForUser[:, 0], :, :] += rbm.probProduct(negData, negHiddenProb)\n",
    "\n",
    "                poshidact = sum(posHiddenProb)\n",
    "                posvisact = sum(v)\n",
    "                neghidact = sum(negHiddenProb)\n",
    "                negvisact = sum(negData)\n",
    "                # we average over the number of users in the batch (if we use mini-batch)\n",
    "    #             grad = (gradientLearningRate/epoch)*(posprods-negprods)\n",
    "                '''\n",
    "                Regularization - \n",
    "                '''\n",
    "                grad_w += adaptiveLearn(learning_rate_type=learning_rate_type, k=learning_rate_k, \n",
    "                                        drop=learning_rate_drop, epochs_drop=learning_rate_epochs_drop, \n",
    "                                        epoch=epoch)*((posprods-negprods)/trStats[\"n_users\"]-_lambda*W)\n",
    "        #         mini_batch_grads.append(grad)\n",
    "\n",
    "            #     m = alpha*m+grad\n",
    "                '''\n",
    "                Ask about the implementation of biases (should we create matrix of biases for hidden and visible layers?)\n",
    "                '''\n",
    "            m_w = alpha*m_w + grad_w\n",
    "#             m_v = alpha*m_v+(gradientLearningRate_v) * (posvisact - negvisact)\n",
    "#             m_h = alpha*m_h+(gradientLearningRate_h) * (poshidact - neghidact)\n",
    "\n",
    "            if momentum == False:\n",
    "                W += grad_w\n",
    "            else:\n",
    "                W += m_w\n",
    "\n",
    "            vis_bias += m_v\n",
    "            hid_bias += m_h\n",
    "\n",
    "        # Print the current RMSE for training and validation sets\n",
    "        # this allows you to control for overfitting e.g\n",
    "        # We predict over the training set\n",
    "        tr_r_hat = rbm.predict(trStats[\"movies\"], trStats[\"users\"], W, training) #, vis_bias, hid_bias, predictType='exp')\n",
    "    #     print (tr_r_hat)\n",
    "        trRMSE = lib.rmse(trStats[\"ratings\"], tr_r_hat)\n",
    "    #     print (trRMSE)\n",
    "        if trRMSE < best_train_loss:\n",
    "            best_train_loss = trRMSE\n",
    "            best_training_weights = W\n",
    "            best_train_predictions = tr_r_hat\n",
    "\n",
    "        # We predict over the validation set\n",
    "        vl_r_hat = rbm.predict(vlStats[\"movies\"], vlStats[\"users\"], W, training) #, vis_bias, hid_bias, predictType='exp')\n",
    "    #     vl_r_hat\n",
    "        vlRMSE = lib.rmse(vlStats[\"ratings\"], vl_r_hat)\n",
    "        if vlRMSE < best_validation_loss:\n",
    "            best_validation_loss = vlRMSE\n",
    "            best_validation_weights = W\n",
    "            best_validation_predictions = vl_r_hat\n",
    "#             best_momentum = momentum\n",
    "#             best_reg = regularization\n",
    "#             best_epoch = epoch\n",
    "#             best_alpha = alpha\n",
    "#             best_B = B\n",
    "#             best_F = F\n",
    "#             min_rmse = vlRMSE\n",
    "#                     print('Best RMSE:', min_rmse)\n",
    "\n",
    "        train_loss.append(trRMSE)\n",
    "        validation_loss.append(vlRMSE)\n",
    "\n",
    "        print (\"### EPOCH %d ###\" % epoch)\n",
    "        print (\"Training loss = %f\" % trRMSE)\n",
    "        print (\"Validation loss = %f\" % vlRMSE)\n",
    "\n",
    "    ### END ###\n",
    "    # This part you can write on your own\n",
    "    # you could plot the evolution of the training and validation RMSEs for example\n",
    "    # predictedRatings = np.array([predictForUser(user, W, training) for user in trStats[\"u_users\"]])\n",
    "    # np.savetxt(\"predictedRatings.txt\", predictedRatings)\n",
    "    # fig1 = plt.figure()\n",
    "    # ax1 = fig1.add_subplot(121)\n",
    "    # ax1.plot(train_loss)\n",
    "    # ax2 = fig1.add_subplot(122)\n",
    "    # ax2.plot(validation_loss)\n",
    "    \n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(validation_loss)\n",
    "    plt.show()\n",
    "    if stopping==True:\n",
    "        print('Best training loss = %f' % best_train_loss)\n",
    "        print('Best validation loss = %f' % best_validation_loss)\n",
    "    else:\n",
    "        print('Final training loss = %f' % trRMSE)\n",
    "        print('Final validation loss = %f' % vlRMSE)\n",
    "    return [best_validation_loss, best_validation_predictions]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### EPOCH 1 ###\n",
      "Training loss = 1.213511\n",
      "Validation loss = 1.236776\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-2c080c20cd0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m                                  \u001b[0mgradientLearningRate_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                                  \u001b[0mstopping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adapt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                                  learning_rate_drop=0.5, learning_rate_epochs_drop=10.0, _lambda = regularization)\n\u001b[0m\u001b[0;32m     28\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmin_rmse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                         \u001b[0mbest_momentum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-4985e1d898b8>\u001b[0m in \u001b[0;36mmain_rbm\u001b[1;34m(training, validation, trStats, vlStats, K, F, epochs, gradientLearningRate, gradientLearningRate_v, gradientLearningRate_h, minibatch_size, alpha, stopping, momentum, learning_rate_type, learning_rate_k, learning_rate_drop, learning_rate_epochs_drop, _lambda)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;31m# this allows you to control for overfitting e.g\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;31m# We predict over the training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mtr_r_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrStats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"movies\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrStats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"users\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#, vis_bias, hid_bias, predictType='exp')\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;31m#     print (tr_r_hat)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[0mtrRMSE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrStats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ratings\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_r_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\ESD\\Networked Life\\Project\\01.104 Test\\Scripts\\rbm_no_bias.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(movies, users, W, training, predictType)\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\ESD\\Networked Life\\Project\\01.104 Test\\Scripts\\rbm_no_bias.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\ESD\\Networked Life\\Project\\01.104 Test\\Scripts\\rbm_no_bias.py\u001b[0m in \u001b[0;36mpredictMovieForUser\u001b[1;34m(q, user, W, training, predictType)\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\ESD\\Networked Life\\Project\\01.104 Test\\Scripts\\rbm_no_bias.py\u001b[0m in \u001b[0;36mgetPredictedDistribution\u001b[1;34m(v, w, wq)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredictRatingMax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratingDistribution\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m     \u001b[1;31m### TO IMPLEMENT ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m     \u001b[1;31m# ratingDistribution is a probability distribution over possible ratings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;31m#   It is obtained from the getPredictedDistribution function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\ESD\\Networked Life\\Project\\01.104 Test\\Scripts\\rbm_no_bias.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;31m# w is the weights array for the current user, of size m x F x 5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;31m# wq is the weight matrix of size F x 5 for movie q\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[1;31m#   If W is the whole weights array, then wq = W[q, :, :]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;31m# You will need to perform the same steps done in the learning/unlearning:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;31m#   - Propagate the user input to the hidden units\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Hyperparameter tuning:\n",
    "\n",
    "# mrange = np.linspace(0.7,0.95, 5)\n",
    "mrange = [0.7,0.8, 0.9, 0.95]\n",
    "# rrange = np.linspace(0.1,0.9,5)\n",
    "rrange = [0.1]\n",
    "arange = [0.001]\n",
    "brange = [5, 10, 20, 30]\n",
    "frange = [5, 8, 10, 15]\n",
    "\n",
    "best_momentum = 0\n",
    "best_reg = 0\n",
    "best_lr = 0\n",
    "best_batch = 0\n",
    "best_F = 0\n",
    "\n",
    "min_rmse=10\n",
    "for momentum in mrange:\n",
    "    for regularization in rrange:\n",
    "        for learning_rate in arange:\n",
    "            for batch in brange:\n",
    "                for F in frange:\n",
    "                    prediction = main_rbm(training=training, validation=validation, trStats=trStats, vlStats=vlStats, \n",
    "                                 K=5, F=int(F), epochs=30, gradientLearningRate=learning_rate, gradientLearningRate_v = 0.001,\n",
    "                                 gradientLearningRate_h = 0.001, minibatch_size=int(batch), alpha=momentum, \n",
    "                                 stopping=True, momentum=True, learning_rate_type='adapt', learning_rate_k=learning_rate/30, \n",
    "                                 learning_rate_drop=0.5, learning_rate_epochs_drop=10.0, _lambda = regularization)\n",
    "                    if prediction[0] < min_rmse:\n",
    "                        best_momentum = momentum\n",
    "                        best_reg = regularization\n",
    "#                             best_epoch = epoch\n",
    "                        best_lr = learning_rate\n",
    "                        best_batch = batch\n",
    "                        best_F = F\n",
    "                        best_predict = prediction[1]\n",
    "                        min_rmse = prediction[0]\n",
    "\n",
    "# ## Final parameters:\n",
    "# alpha = 0.9\n",
    "# _lambda = 0.1 #tuned\n",
    "# gradientLearningRate = 0.001 #tuned\n",
    "# minibatch_size = 10\n",
    "# F = 8 \n",
    "\n",
    "# TODO: Tune other parameters and add biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-95de5c0a274e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m              \u001b[0mgradientLearningRate_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_momentum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m              \u001b[0mstopping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_lr\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m              learning_rate_drop=0.5, learning_rate_epochs_drop=10.0, _lambda = best_reg)\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpredictedRatings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictForUser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrStats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"u_users\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-224ae30928df>\u001b[0m in \u001b[0;36mmain_rbm\u001b[1;34m(training, validation, trStats, vlStats, K, F, epochs, gradientLearningRate, gradientLearningRate_v, gradientLearningRate_h, minibatch_size, alpha, stopping, momentum, learning_rate_type, learning_rate_k, learning_rate_drop, learning_rate_epochs_drop, _lambda)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m#                 y_train_mini = y_train[i:i + minibatch_size]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;31m#         for i in range(0, visitingOrder.shape[0], minibatch_size):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvisitingOrder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mprev_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-cbb1c90cbe2a>\u001b[0m in \u001b[0;36mbatch_get\u001b[1;34m(array, B)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbatch_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mB\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "W = main_rbm(training=training, validation=validation, trStats=trStats, vlStats=vlStats, \n",
    "             K=5, F=best_F, epochs=30, gradientLearningRate=best_lr, gradientLearningRate_v = 0.001,\n",
    "             gradientLearningRate_h = 0.001, minibatch_size=best_batch, alpha=best_momentum, \n",
    "             stopping=True, momentum=True, learning_rate_type='time', learning_rate_k=best_lr/30, \n",
    "             learning_rate_drop=0.5, learning_rate_epochs_drop=10.0, _lambda = best_reg)\n",
    "\n",
    "predictedRatings = np.array([rbm.predictForUser(user, W[2], training) for user in trStats[\"u_users\"]])\n",
    "np.savetxt(\"SpateggiChikenChoope+v3.txt\", predictedRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### EPOCH 1 ###\n",
      "Training loss = 1.209743\n",
      "Validation loss = 1.233003\n",
      "### EPOCH 2 ###\n",
      "Training loss = 1.209304\n",
      "Validation loss = 1.232739\n",
      "### EPOCH 3 ###\n",
      "Training loss = 1.208033\n",
      "Validation loss = 1.230316\n",
      "### EPOCH 4 ###\n",
      "Training loss = 1.206176\n",
      "Validation loss = 1.229580\n",
      "### EPOCH 5 ###\n",
      "Training loss = 1.203896\n",
      "Validation loss = 1.229520\n",
      "### EPOCH 6 ###\n",
      "Training loss = 1.201944\n",
      "Validation loss = 1.227766\n",
      "### EPOCH 7 ###\n",
      "Training loss = 1.200668\n",
      "Validation loss = 1.223519\n",
      "### EPOCH 8 ###\n",
      "Training loss = 1.198866\n",
      "Validation loss = 1.223506\n",
      "### EPOCH 9 ###\n",
      "Training loss = 1.196400\n",
      "Validation loss = 1.221919\n",
      "### EPOCH 10 ###\n",
      "Training loss = 1.195268\n",
      "Validation loss = 1.219678\n",
      "### EPOCH 11 ###\n",
      "Training loss = 1.193544\n",
      "Validation loss = 1.219258\n",
      "### EPOCH 12 ###\n",
      "Training loss = 1.190881\n",
      "Validation loss = 1.217550\n",
      "### EPOCH 13 ###\n",
      "Training loss = 1.190054\n",
      "Validation loss = 1.215462\n",
      "### EPOCH 14 ###\n",
      "Training loss = 1.188265\n",
      "Validation loss = 1.216136\n",
      "### EPOCH 15 ###\n",
      "Training loss = 1.186921\n",
      "Validation loss = 1.212408\n",
      "### EPOCH 16 ###\n",
      "Training loss = 1.186232\n",
      "Validation loss = 1.212376\n",
      "### EPOCH 17 ###\n",
      "Training loss = 1.184966\n",
      "Validation loss = 1.211241\n",
      "### EPOCH 18 ###\n",
      "Training loss = 1.183334\n",
      "Validation loss = 1.211240\n",
      "### EPOCH 19 ###\n",
      "Training loss = 1.182889\n",
      "Validation loss = 1.210610\n",
      "### EPOCH 20 ###\n",
      "Training loss = 1.182563\n",
      "Validation loss = 1.211519\n",
      "### EPOCH 21 ###\n",
      "Training loss = 1.181436\n",
      "Validation loss = 1.209556\n",
      "### EPOCH 22 ###\n",
      "Training loss = 1.181250\n",
      "Validation loss = 1.208103\n",
      "### EPOCH 23 ###\n",
      "Training loss = 1.180771\n",
      "Validation loss = 1.208026\n",
      "### EPOCH 24 ###\n",
      "Training loss = 1.180781\n",
      "Validation loss = 1.208347\n",
      "### EPOCH 25 ###\n",
      "Training loss = 1.179298\n",
      "Validation loss = 1.208238\n",
      "### EPOCH 26 ###\n",
      "Training loss = 1.180892\n",
      "Validation loss = 1.206614\n",
      "### EPOCH 27 ###\n",
      "Training loss = 1.179096\n",
      "Validation loss = 1.208841\n",
      "### EPOCH 28 ###\n",
      "Training loss = 1.179596\n",
      "Validation loss = 1.206961\n",
      "### EPOCH 29 ###\n",
      "Training loss = 1.180224\n",
      "Validation loss = 1.207080\n",
      "### EPOCH 30 ###\n",
      "Training loss = 1.179322\n",
      "Validation loss = 1.206971\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training loss = 1.179096\n",
      "Best validation loss = 1.206614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2066141627660765,\n",
       " [3.010688315911038,\n",
       "  2.964805492512201,\n",
       "  2.9531477638958874,\n",
       "  2.9941139260210172,\n",
       "  3.045551898586543,\n",
       "  3.023547941326912,\n",
       "  3.005427265262361,\n",
       "  3.002081437655323,\n",
       "  2.9889665485122667,\n",
       "  3.0215574560685945,\n",
       "  3.01724217743152,\n",
       "  3.0522987560917145,\n",
       "  2.8798686498688983,\n",
       "  3.0280798022895183,\n",
       "  3.040312632154436,\n",
       "  3.019302474733772,\n",
       "  2.9934893895026886,\n",
       "  3.2006529045625034,\n",
       "  3.1250876050397873,\n",
       "  2.9763024678222876,\n",
       "  3.002622379645401,\n",
       "  3.015447411373965,\n",
       "  2.9605427642472195,\n",
       "  2.969245421211238,\n",
       "  3.2542134255637905,\n",
       "  2.999473173135888,\n",
       "  2.920645583038726,\n",
       "  2.985753729018657,\n",
       "  3.004492572663605,\n",
       "  3.249264422759345,\n",
       "  3.184184156856422,\n",
       "  3.022427908909874,\n",
       "  3.0586417807642436,\n",
       "  2.999284958398318,\n",
       "  3.002802577666495,\n",
       "  3.1800457643308886,\n",
       "  3.058914557449434,\n",
       "  3.021062447241835,\n",
       "  2.967267204179277,\n",
       "  3.03696008280723,\n",
       "  2.995319102188849,\n",
       "  3.04204058897859,\n",
       "  3.0194070897505423,\n",
       "  3.059268017726894,\n",
       "  2.9862778223469584,\n",
       "  2.9969808879851367,\n",
       "  3.180680963451467,\n",
       "  2.965926927296162,\n",
       "  3.134696132162779,\n",
       "  3.0,\n",
       "  3.001455080786687,\n",
       "  3.010628018799427,\n",
       "  3.0048529523774863,\n",
       "  3.3092957748046796,\n",
       "  2.935705692588395,\n",
       "  2.9635812471201506,\n",
       "  2.981624039157655,\n",
       "  3.066793233089543,\n",
       "  3.21858844321838,\n",
       "  2.9571062383408027,\n",
       "  3.014531524031918,\n",
       "  2.943612499930751,\n",
       "  2.9533879529115765,\n",
       "  3.0276378552199894,\n",
       "  3.0067498206992016,\n",
       "  3.015112990100951,\n",
       "  3.039095909501147,\n",
       "  3.0028895060957796,\n",
       "  2.9977687571279366,\n",
       "  3.1143815156893044,\n",
       "  2.9845139149818056,\n",
       "  3.2856709524101335,\n",
       "  3.089928532623849,\n",
       "  3.010425170086743,\n",
       "  3.007176291789825,\n",
       "  3.2920400683389595,\n",
       "  2.9956048523054015,\n",
       "  3.047885992105588,\n",
       "  3.233810123304711,\n",
       "  3.0091369268337185,\n",
       "  3.0469086806729067,\n",
       "  3.036906508848173,\n",
       "  2.973386303096497,\n",
       "  3.0586417807642436,\n",
       "  3.1887245918561593,\n",
       "  2.9697663293243557,\n",
       "  3.2439553810134187,\n",
       "  3.021836583496558,\n",
       "  3.036633357201539,\n",
       "  2.9841176855570106,\n",
       "  3.020847341378281,\n",
       "  3.096457624024066,\n",
       "  3.1442761191189286,\n",
       "  3.0395377854761367,\n",
       "  2.9873677434569528,\n",
       "  2.9971583197713247,\n",
       "  3.028970896707287,\n",
       "  3.0094196777912536,\n",
       "  3.0802554292479742,\n",
       "  3.107507153265827,\n",
       "  3.04204058897859,\n",
       "  3.0065127345013987,\n",
       "  2.9849231946706016,\n",
       "  3.088880602722105,\n",
       "  3.0177520086666028,\n",
       "  3.0076449605661972,\n",
       "  2.902476530973741,\n",
       "  3.0117111442912705,\n",
       "  2.994714351473883,\n",
       "  3.232995912610132,\n",
       "  3.053222226992944,\n",
       "  3.2160435708823103,\n",
       "  2.981963195450831,\n",
       "  3.0824956182906593,\n",
       "  3.233810123304711,\n",
       "  3.230401140578334,\n",
       "  3.027164864256905,\n",
       "  2.987499510657079,\n",
       "  2.982370098987617,\n",
       "  3.0294370265198545,\n",
       "  3.1170017675420825,\n",
       "  2.937569074048146,\n",
       "  2.96755784968233,\n",
       "  3.070778550360263,\n",
       "  3.1990251816897035,\n",
       "  2.9774338829685423,\n",
       "  3.1548774815042684,\n",
       "  3.0176720329822024,\n",
       "  2.9694186393998434,\n",
       "  2.864565070747128,\n",
       "  3.017427630562441,\n",
       "  3.0890258612835475,\n",
       "  2.967919904368903,\n",
       "  2.858756412629682,\n",
       "  3.065638526059058,\n",
       "  2.9977077367482257,\n",
       "  3.0087755758883086,\n",
       "  2.960025514324158,\n",
       "  2.9811956977311524,\n",
       "  3.018465198216428,\n",
       "  3.0537814818753013,\n",
       "  3.007176291789825,\n",
       "  3.0001448020364205,\n",
       "  3.017956530561121,\n",
       "  3.0127929651013994,\n",
       "  3.011114431112581,\n",
       "  2.9450959205422773,\n",
       "  3.011690489079708,\n",
       "  3.044099121805718,\n",
       "  3.013614717459434,\n",
       "  2.99116201526645,\n",
       "  3.040351100353914,\n",
       "  3.039095909501147,\n",
       "  2.9873350360592825,\n",
       "  2.96014147527345,\n",
       "  3.0749722809332996,\n",
       "  2.9749572494084475,\n",
       "  3.006807003442288,\n",
       "  2.9350825451261966,\n",
       "  3.000743598833746,\n",
       "  3.0586417807642436,\n",
       "  2.9961751299098625,\n",
       "  2.9792327850590197,\n",
       "  3.010155288919064,\n",
       "  3.0011434865983686,\n",
       "  3.1511484319482164,\n",
       "  3.002081437655323,\n",
       "  2.930743486345862,\n",
       "  3.285463029097156,\n",
       "  3.049189720813362,\n",
       "  2.925693407023822,\n",
       "  3.0371011351626294,\n",
       "  3.0166205674306,\n",
       "  3.074087422090846,\n",
       "  3.00890092728791,\n",
       "  2.9906154801681546,\n",
       "  3.1548774815042684,\n",
       "  3.0152746564385486,\n",
       "  2.99737532872427,\n",
       "  3.0084992044965224,\n",
       "  3.0147659869533956,\n",
       "  3.017868591094337,\n",
       "  2.979070185379458,\n",
       "  2.9888848408801363,\n",
       "  3.0632802157991543,\n",
       "  2.976645620820925,\n",
       "  3.2018581756954774,\n",
       "  3.1857291022610514,\n",
       "  3.0183192593721575,\n",
       "  3.0192706389216557,\n",
       "  3.2505670825433826,\n",
       "  2.925878928167954,\n",
       "  2.985705831540993,\n",
       "  3.0135387754081178,\n",
       "  3.09202405273522,\n",
       "  2.9578774467421454,\n",
       "  2.892858668599886,\n",
       "  3.1018400935459676,\n",
       "  2.988730736193762,\n",
       "  3.0243039182951934,\n",
       "  3.010933798851883,\n",
       "  3.187580389319319,\n",
       "  2.993798378089229,\n",
       "  3.146016333611733,\n",
       "  2.9497842984459512,\n",
       "  3.069787337039654,\n",
       "  3.1631775672943947,\n",
       "  3.0298560382759594,\n",
       "  3.070201934466819,\n",
       "  3.0110159149819036,\n",
       "  3.0522987560917145,\n",
       "  3.144724896174567,\n",
       "  3.037019313082908,\n",
       "  3.0270770723840874,\n",
       "  3.0155995605324564,\n",
       "  2.858443134321007,\n",
       "  2.8591131049933045,\n",
       "  3.0099152064145818,\n",
       "  3.1712167155613002,\n",
       "  3.1328727798742766,\n",
       "  3.0714948735863437,\n",
       "  2.9835925879059157,\n",
       "  3.0069421675750023,\n",
       "  3.027690304339276,\n",
       "  3.027745717977962,\n",
       "  3.1585558719767275,\n",
       "  3.036234126568764,\n",
       "  2.9398805793616862,\n",
       "  3.0112693967376503,\n",
       "  2.98195755421802,\n",
       "  2.985721157114332,\n",
       "  3.24394006972668,\n",
       "  3.1331830066450657,\n",
       "  2.998712998148949,\n",
       "  3.005666620798069,\n",
       "  2.9595915936742765,\n",
       "  3.033076763392786,\n",
       "  3.0007495955657717,\n",
       "  3.1296889517288764,\n",
       "  2.997720862151288,\n",
       "  3.004677412409265,\n",
       "  2.977033882955684,\n",
       "  3.1511484319482164,\n",
       "  3.1415368320771675,\n",
       "  2.9805183721072037,\n",
       "  3.1032981363436916,\n",
       "  3.0011976642899976,\n",
       "  3.0101890988982927,\n",
       "  3.0352663239481985,\n",
       "  3.0,\n",
       "  3.031786756131813,\n",
       "  2.964977051764555,\n",
       "  2.8825688202357704,\n",
       "  2.9571062383408027,\n",
       "  3.1442761191189286,\n",
       "  3.0153620902359095,\n",
       "  2.9796029129274153,\n",
       "  3.002355281874779,\n",
       "  3.040603326318001,\n",
       "  3.019960356600518,\n",
       "  3.0054571755180324,\n",
       "  3.218651612884175,\n",
       "  3.0276378552199894,\n",
       "  2.9823152144335126,\n",
       "  2.982370098987617,\n",
       "  3.0159551917924174,\n",
       "  2.9497842984459512,\n",
       "  3.101733644643944,\n",
       "  3.262126784081315,\n",
       "  2.9971339532804904,\n",
       "  3.0184185693192864,\n",
       "  3.01414895849312,\n",
       "  3.040679058748424,\n",
       "  3.0269523879066975,\n",
       "  3.231436082826884,\n",
       "  3.0851538051596066,\n",
       "  3.032833459045668,\n",
       "  3.1016966994285564,\n",
       "  3.0108945323629523,\n",
       "  3.004240998667758,\n",
       "  3.1319215842552732,\n",
       "  2.9837704253514845,\n",
       "  2.990611061386603,\n",
       "  3.0825142217660773,\n",
       "  3.1655989900353774,\n",
       "  3.106597542170137,\n",
       "  3.210430003510554,\n",
       "  3.000523169167083,\n",
       "  2.975199111789245,\n",
       "  3.079337629906277,\n",
       "  3.153073139916806,\n",
       "  3.20427947309441,\n",
       "  3.0272204210509206,\n",
       "  3.0502362880500513,\n",
       "  3.0037952697894794,\n",
       "  2.937569074048146,\n",
       "  3.2882302896217857,\n",
       "  2.9943239695198365,\n",
       "  2.999605072732037,\n",
       "  2.9843053300741977,\n",
       "  2.9903618318496115,\n",
       "  3.315948920798385,\n",
       "  3.0467613563913623,\n",
       "  2.8591131049933045,\n",
       "  3.0483852265566966,\n",
       "  2.9605478254492565,\n",
       "  2.9894607721469324,\n",
       "  3.022341311008828,\n",
       "  3.0082869681492035,\n",
       "  3.0744440982291015,\n",
       "  3.0456298284069705,\n",
       "  3.1115747661089608,\n",
       "  2.978845241411132,\n",
       "  3.014021158516328,\n",
       "  2.986415619008697,\n",
       "  3.1415368320771675,\n",
       "  3.0109223790413764,\n",
       "  2.888849019304268,\n",
       "  3.1800457643308886,\n",
       "  3.098733868072991,\n",
       "  2.97586403045647,\n",
       "  3.0027497771840177,\n",
       "  3.101733644643944,\n",
       "  3.0177024182800616,\n",
       "  3.0302626675479996,\n",
       "  2.9809247711488593,\n",
       "  2.970250109863772,\n",
       "  3.1319215842552732,\n",
       "  2.988548508597655,\n",
       "  3.027843967039556,\n",
       "  3.007176291789825,\n",
       "  3.0481662630758874,\n",
       "  3.3325868488306813,\n",
       "  3.0131363928985966,\n",
       "  2.955909928268255,\n",
       "  3.143656195808684,\n",
       "  3.043561321403093,\n",
       "  2.986093874878785,\n",
       "  3.005159324913565,\n",
       "  2.8605446708387046,\n",
       "  2.9450138716766596,\n",
       "  3.0017058711016458,\n",
       "  3.1236956418047956,\n",
       "  3.0289178957493608,\n",
       "  3.001566326627786,\n",
       "  3.015651900030668,\n",
       "  3.0314562155867972,\n",
       "  3.140871090929837,\n",
       "  3.0276378552199894,\n",
       "  2.969047449348655,\n",
       "  3.0501759354726827,\n",
       "  3.0084331921009935,\n",
       "  3.153592447232701,\n",
       "  2.977384369099503,\n",
       "  3.0405931369588157,\n",
       "  2.969498028231516,\n",
       "  2.919599251103284,\n",
       "  3.016741498243978,\n",
       "  3.00354277787407,\n",
       "  3.0180936281078314,\n",
       "  2.998712998148949,\n",
       "  2.9937967062013184,\n",
       "  3.135036551595108,\n",
       "  3.00354277787407,\n",
       "  2.956971770122697,\n",
       "  3.013614717459434,\n",
       "  2.9660185501998297,\n",
       "  2.995829394714846,\n",
       "  3.0142579579807007,\n",
       "  2.9703181699435284,\n",
       "  3.2588563085066173,\n",
       "  3.1713203056631976,\n",
       "  3.0893727259912658,\n",
       "  2.9813917878677345,\n",
       "  3.0007913308374015,\n",
       "  3.2368987303185373,\n",
       "  3.0325662198645205,\n",
       "  2.9951743181116175,\n",
       "  3.0023191232096265,\n",
       "  3.129263708112213,\n",
       "  3.2174340741628358,\n",
       "  3.0272908845185595,\n",
       "  3.016959071227248,\n",
       "  3.1039317145647773,\n",
       "  2.9998702888928417,\n",
       "  2.975199111789245,\n",
       "  3.143821893625287,\n",
       "  3.2233569567762235,\n",
       "  3.076724358833678,\n",
       "  3.3040484281447693,\n",
       "  3.1342167956366733,\n",
       "  2.9896670252862614,\n",
       "  3.2226057919604645,\n",
       "  2.942443550580353,\n",
       "  3.231349014372455,\n",
       "  2.9873374781855855,\n",
       "  3.2148564187571287,\n",
       "  3.041451877634286,\n",
       "  3.0007913308374015,\n",
       "  3.042691450300663,\n",
       "  3.166294370760473,\n",
       "  3.135761773010519,\n",
       "  3.227036874570842,\n",
       "  3.107507153265827,\n",
       "  3.0390888227665993,\n",
       "  2.9775258133712716,\n",
       "  2.9756719720031857,\n",
       "  3.0204011135374733,\n",
       "  3.021286358988437,\n",
       "  3.0537814818753013,\n",
       "  2.977094124827798,\n",
       "  2.932645032421618,\n",
       "  2.996262592524787,\n",
       "  3.018102699731554,\n",
       "  2.9770229313227086,\n",
       "  3.021199405978661,\n",
       "  2.918049387044989,\n",
       "  2.9721797471726665,\n",
       "  3.057465395275948,\n",
       "  3.012898390126953,\n",
       "  3.0155995605324564,\n",
       "  3.0379012864483803,\n",
       "  3.004492572663605,\n",
       "  3.0131535273651937,\n",
       "  3.0136585567589846,\n",
       "  2.989591810592616,\n",
       "  2.9945490860599513,\n",
       "  3.0208660297174457,\n",
       "  3.010833541948422,\n",
       "  3.046585557375672,\n",
       "  2.977951960544453,\n",
       "  3.038495485039975,\n",
       "  3.003725425985128,\n",
       "  3.1062580968230495,\n",
       "  2.9724474892014308,\n",
       "  2.9771856102525343,\n",
       "  3.120589456526105,\n",
       "  3.0522987560917145,\n",
       "  2.9906154801681546,\n",
       "  2.9605223897199187,\n",
       "  2.990402229193628,\n",
       "  3.1416916766647414,\n",
       "  3.122773048587942,\n",
       "  3.0060559761636783,\n",
       "  3.003725425985128,\n",
       "  2.975199111789245,\n",
       "  3.002129926751219,\n",
       "  3.0057367789741862,\n",
       "  3.006840987643059,\n",
       "  2.9876835545534344,\n",
       "  3.044267609479575,\n",
       "  2.9540372433128326,\n",
       "  3.0204143919431656,\n",
       "  2.8605446708387046,\n",
       "  3.0447779657566993,\n",
       "  3.0028895060957796,\n",
       "  3.003728480496335,\n",
       "  2.903290198533719,\n",
       "  3.0091369268337185,\n",
       "  3.012604635877466,\n",
       "  2.9454764877667996,\n",
       "  2.957648252916198,\n",
       "  3.076642642005792,\n",
       "  3.090165474020007,\n",
       "  3.0095744436032987,\n",
       "  3.0124154072790326,\n",
       "  2.884801826100893,\n",
       "  3.212076538135626,\n",
       "  3.0147691909592282,\n",
       "  2.941085425459477,\n",
       "  3.01120244411806,\n",
       "  3.0055880324108895,\n",
       "  3.0002378717118976,\n",
       "  2.9971106274059958,\n",
       "  3.1685185762215085]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_rbm(training=training, validation=validation, trStats=trStats, vlStats=vlStats, \n",
    "             K=5, F=8, epochs=30, gradientLearningRate=0.001, gradientLearningRate_v = 0.001,\n",
    "             gradientLearningRate_h = 0.001, minibatch_size=10, alpha=0.9, \n",
    "             stopping=True, momentum=True, learning_rate_type='time', learning_rate_k=0.5, \n",
    "             learning_rate_drop=0.5, learning_rate_epochs_drop=10.0, _lambda = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
