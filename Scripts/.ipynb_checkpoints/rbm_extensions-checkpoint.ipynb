{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import projectLib as lib\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "\n",
    "# set highest rating\n",
    "K = 5\n",
    "F = 3\n",
    "eps=0.1\n",
    "\n",
    "def softmax(x):\n",
    "    # Numerically stable softmax function\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def ratingsPerMovie(training):\n",
    "    movies = [x[0] for x in training]\n",
    "    u_movies = np.unique(movies).tolist()\n",
    "    return np.array([[i, movie, len([x for x in training if x[0] == movie])] for i, movie in enumerate(u_movies)])\n",
    "\n",
    "def getV(ratingsForUser):\n",
    "    # ratingsForUser is obtained from the ratings for user library\n",
    "    # you should return a binary matrix ret of size m x K, where m is the number of movies\n",
    "    #   that the user has seen. ret[i][k] = 1 if the user\n",
    "    #   has rated movie ratingsForUser[i, 0] with k stars\n",
    "    #   otherwise it is 0\n",
    "    ret = np.zeros((len(ratingsForUser), K))\n",
    "    for i in range(len(ratingsForUser)):\n",
    "        ret[i, ratingsForUser[i, 1]-1] = 1.0\n",
    "    return ret\n",
    "\n",
    "def getInitialWeights(m, F, K):\n",
    "    # m is the number of visible units\n",
    "    # F is the number of hidden units\n",
    "    # K is the highest rating (fixed to 5 here)\n",
    "    return np.random.normal(0, 0.1, (m, F, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = lib.getTrainingData()\n",
    "ratingsForUser1 = lib.getRatingsForUser(1, training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ratingsPerMovie(training)\n",
    "v = getV(ratingsForUser1)\n",
    "# v.shape\n",
    "w = getInitialWeights(v.shape[0],F,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trStats = lib.getUsefulStats(training)\n",
    "W = getInitialWeights(trStats[\"n_movies\"], F, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # x is a real vector of size n\n",
    "    # ret should be a vector of size n where ret_i = sigmoid(x_i)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def visibleToHiddenVec(v, w, hid_bias):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # v is a matrix of size m x 5. Each row is a binary vector representing a rating\n",
    "    #    OR a probability distribution over the rating\n",
    "    # w is a list of matrices of size m x F x 5\n",
    "    # ret should be a vector of size F\n",
    "    m,f,K=w.shape\n",
    "    output=list()\n",
    "    for i in range(f):\n",
    "        summ=0\n",
    "        for k in range(K):\n",
    "            for j in range(m):\n",
    "                summ+=v[j,k]*w[j,i,k]\n",
    "        output.append(summ)\n",
    "\n",
    "    return sig(np.array(output)+hid_bias)\n",
    "    \n",
    "\n",
    "def hiddenToVisible(h, w, vis_bias):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # h is a binary vector of size F\n",
    "    # w is an array of size m x F x 5\n",
    "    # ret should be a matrix of size m x 5, where m\n",
    "    #   is the number of movies the user has seen.\n",
    "    #   Remember that we do not reconstruct movies that the user\n",
    "    #   has not rated! (where reconstructing means getting a distribution\n",
    "    #   over possible ratings).\n",
    "    #   We only do so when we predict the rating a user would have given to a movie.\n",
    "#     print(w.shape)\n",
    "    output=w[:,0,:]*h[0]\n",
    "    m,f,k=w.shape\n",
    "    for i in range(1,f):\n",
    "        output+=w[:,i,:]*h[i]\n",
    "    return sig(output+vis_bias)\n",
    "\n",
    "def probProduct(v, p):\n",
    "    # v is a matrix of size m x 5\n",
    "    # p is a vector of size F, activation of the hidden units\n",
    "    # returns the gradient for visible input v and hidden activations p\n",
    "    ret = np.zeros((v.shape[0], p.size, v.shape[1]))\n",
    "    for i in range(v.shape[0]):\n",
    "        for j in range(p.size):\n",
    "            for k in range(v.shape[1]):\n",
    "                ret[i, j, k] = v[i, k] * p[j]\n",
    "    return ret\n",
    "\n",
    "def sample(p):\n",
    "    # p is a vector of real numbers between 0 and 1\n",
    "    # ret is a vector of same size as p, where ret_i = Ber(p_i)\n",
    "    # In other word we sample from a Bernouilli distribution with\n",
    "    # parameter p_i to obtain ret_i\n",
    "    samples = np.random.random(p.size)\n",
    "    return np.array(samples <= p, dtype=int)\n",
    "\n",
    "def getPredictedDistribution(v, w, wq,vis_bias,hid_bias):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # This function returns a distribution over the ratings for movie q, if user data is v\n",
    "    # v is the dataset of the user we are predicting the movie for\n",
    "    #   It is a m x 5 matrix, where m is the number of movies in the\n",
    "    #   dataset of this user.\n",
    "    # w is the weights array for the current user, of size m x F x 5\n",
    "    # wq is the weight matrix of size F x 5 for movie q\n",
    "    #   If W is the whole weights array, then wq = W[q, :, :]\n",
    "    # You will need to perform the same steps done in the learning/unlearning:\n",
    "    #   - Propagate the user input to the hidden units\n",
    "    #   - Sample the state of the hidden units\n",
    "    #   - Backpropagate these hidden states to obtain\n",
    "    #       the distribution over the movie whose associated weights are wq\n",
    "    # ret is a vector of size 5\n",
    "    m,f,K=w.shape\n",
    "    p_learn = visibleToHiddenVec(v,w,hid_bias)\n",
    "    PG = probProduct(v, p_learn)\n",
    "    hidden_activations = sample(p_learn)\n",
    "    v_negative = hiddenToVisible(hidden_activations, wq.reshape(1,wq.shape[0],wq.shape[1]),vis_bias)\n",
    "\n",
    "    return v_negative.reshape(5)\n",
    "\n",
    "def predictRatingMax(ratingDistribution):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # ratingDistribution is a probability distribution over possible ratings\n",
    "    #   It is obtained from the getPredictedDistribution function\n",
    "    # This function is one of three you are to implement\n",
    "    # that returns a rating from the distribution\n",
    "    # We decide here that the predicted rating will be the one with the highest probability\n",
    "    max_indices = np.where(ratingDistribution == np.amax(ratingDistribution))[0]\n",
    "    if max_indices.shape[0] == 1:\n",
    "        result = max_indices.item()\n",
    "    else:\n",
    "        result = 2\n",
    "    return result+1\n",
    "\n",
    "def predictRatingMean(ratingDistribution):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # ratingDistribution is a probability distribution over possible ratings\n",
    "    #   It is obtained from the getPredictedDistribution function\n",
    "    # This function is one of three you are to implement\n",
    "    # that returns a rating from the distribution\n",
    "    # We decide here that the predicted rating will be the expectation over ratingDistribution\n",
    "    normalized = [i/sum(ratingDistribution) for i in ratingDistribution]\n",
    "    result = 0 \n",
    "    for k in range(ratingDistribution.shape[0]):\n",
    "        result += normalized[k]*(k+1)\n",
    "    return result\n",
    "\n",
    "def predictRatingExp(ratingDistribution):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # ratingDistribution is a probability distribution over possible ratings\n",
    "    #   It is obtained from the getPredictedDistribution function\n",
    "    # This function is one of three you are to implement\n",
    "    # that returns a rating from the distribution\n",
    "    # We decide here that the predicted rating will be the expectation over\n",
    "    # the softmax applied to ratingDistribution\n",
    "    softmax = np.exp(ratingDistribution)/sum(np.exp(ratingDistribution))\n",
    "#     print(softmax)\n",
    "    result = 0 \n",
    "    for k in range(len(ratingDistribution)):\n",
    "        result += softmax[k]*(k+1)\n",
    "#         print (result)\n",
    "    return result\n",
    "\n",
    "def predictMovieForUser(q, user, W, training,vis_bias,hid_bias, predictType=\"exp\"):\n",
    "    # movie is movie idx\n",
    "    # user is user ID\n",
    "    # type can be \"max\" or \"exp\"\n",
    "    ratingsForUser = lib.getRatingsForUser(user, training)\n",
    "    v = getV(ratingsForUser)\n",
    "#     print(np.where(ratingsForUser[:,0]==q))\n",
    "    ratingDistribution = getPredictedDistribution(v, W[ratingsForUser[:, 0], :, :], W[q, :, :],vis_bias[q],hid_bias)#[np.where(ratingsForUser[:,0]==q)[0][0]]\n",
    "    if predictType == \"max\":\n",
    "        return predictRatingMax(ratingDistribution)\n",
    "    elif predictType == \"mean\":\n",
    "        return predictRatingMean(ratingDistribution)\n",
    "    else:\n",
    "        return predictRatingExp(ratingDistribution)\n",
    "\n",
    "def predict(movies, users, W, training,vis_bias,hid_bias, predictType=\"exp\"):\n",
    "    # given a list of movies and users, predict the rating for each (movie, user) pair\n",
    "    # used to compute RMSE\n",
    "    return [predictMovieForUser(movie, user, W, training,vis_bias,hid_bias, predictType=predictType) for (movie, user) in zip(movies, users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptiveLearn(learning_rate_type=['constant','time', 'step', 'exponential'], \n",
    "                  lr=0.0001, k=0.1, epoch=None, drop=0.5, epochs_drop=10.0):\n",
    "#time-based decay, step decay and exponential decay\n",
    "    if learning_rate_type == 'constant':\n",
    "        return lr\n",
    "    elif learning_rate_type == 'time':\n",
    "        return lr/(1.0+k*epoch)\n",
    "    elif learning_rate_type == 'step':\n",
    "        return lr*np.power(drop,np.floor((1+epoch)/epochs_drop))\n",
    "    elif learning_rate_type == 'exponential':\n",
    "        return lr*np.exp(-k*epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "getPredictedDistribution() missing 2 required positional arguments: 'vis_bias' and 'hid_bias'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6bb1bbac94e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratingsForUser1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratingsForUser1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mratingdist1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetPredictedDistribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mratingsForUser1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#[index]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratingdist1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpredictRatingMax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratingdist1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: getPredictedDistribution() missing 2 required positional arguments: 'vis_bias' and 'hid_bias'"
     ]
    }
   ],
   "source": [
    "q = np.random.choice(ratingsForUser1[:, 0])\n",
    "index = np.where(ratingsForUser1[:, 0]==q)[0].item()\n",
    "ratingdist1 = getPredictedDistribution(v, W[ratingsForUser1[:, 0], :, :], W[q, :, :])#[index]\n",
    "print(ratingdist1)\n",
    "predictRatingMax(ratingdist1)\n",
    "# Problem remains - we need the index to know which movie to take the values from in the \n",
    "# getPredictedDistribution function. Need to check with TA and clarify that part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictForUser(user, W, training, predictType=\"exp\"):\n",
    "    ### TO IMPLEMENT\n",
    "    # given a user ID, predicts all movie ratings for the user\n",
    "    ratingsForUser = lib.getRatingsForUser(user, training)\n",
    "    v = getV(ratingsForUser)\n",
    "    ratings = []\n",
    "    for i in range(np.shape(ratingsForUser)[0]):\n",
    "        ratingDistribution = getPredictedDistribution(v, W[ratingsForUser[:, 0], :, :], W[ratingsForUser[:,0][i], :, :])[i]\n",
    "#         print(ratingDistribution)\n",
    "#     return v\n",
    "#     ratingDistribution = getPredictedDistribution(v, W[ratingsForUser[:, 0], :, :], )\n",
    "        if predictType == \"max\":\n",
    "            ratings.append(predictRatingMax(ratingDistribution))\n",
    "        elif predictType == \"mean\":\n",
    "            ratings.append(predictRatingMean(ratingDistribution))\n",
    "        else:\n",
    "#             print (predictRatingExp(ratingDistribution))\n",
    "            ratings.append(predictRatingExp(ratingDistribution))\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rbm\n",
    "import projectLib as lib\n",
    "\n",
    "training = lib.getTrainingData()\n",
    "validation = lib.getValidationData()\n",
    "# You could also try with the chapter 4 data\n",
    "# training = lib.getChapter4Data()\n",
    "\n",
    "trStats = lib.getUsefulStats(training)\n",
    "vlStats = lib.getUsefulStats(validation)\n",
    "\n",
    "K = 5\n",
    "alpha = 0.9\n",
    "\n",
    "# SET PARAMETERS HERE!!!\n",
    "# number of hidden units\n",
    "F = 5\n",
    "epochs = 30\n",
    "gradientLearningRate = 0.0001\n",
    "gradientLearningRate_v = 0.001\n",
    "gradientLearningRate_h = 0.001\n",
    "_lambda = 1\n",
    "minibatch_size = 10\n",
    "\n",
    "def main_rbm(training=training, validation=validation, trStats=trStats, vlStats=vlStats, \n",
    "             K=5, F=5, epochs=30, gradientLearningRate=0.0001,gradientLearningRate_v = 0.001,\n",
    "             gradientLearningRate_h = 0.001, minibatch_size=None, alpha=0.9, \n",
    "             stopping=False, momentum=False, learning_rate_type='time', learning_rate_k=0.1, \n",
    "             learning_rate_drop=0.5, learning_rate_epochs_drop=10.0, _lambda = 0.3):\n",
    "    frame = inspect.currentframe()\n",
    "    args, _, _, values = inspect.getargvalues(frame)\n",
    "    print ('Training and Predicting with the following hyperparameters:')\n",
    "    for i in args[4:]:\n",
    "        print (\"    %s = %s\" % (i, values[i]))\n",
    "#     return [(i, values[i]) for i in args]\n",
    "    # Initialise all our arrays\n",
    "    num_movies=trStats[\"n_movies\"]\n",
    "    num_users=trStats[\"n_users\"]\n",
    "    W = getInitialWeights(trStats[\"n_movies\"], F, K)\n",
    "    posprods = np.zeros(W.shape)\n",
    "    negprods = np.zeros(W.shape)\n",
    "    m_w=np.zeros((W.shape[0],F,5))\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    vis_bias=np.zeros((num_movies,5))\n",
    "    m_v=np.zeros((num_movies,5))\n",
    "    hid_bias=np.zeros((F,))\n",
    "    m_h=np.zeros((F,))\n",
    "    best_train_loss = 100\n",
    "    best_validation_loss = 100\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "    #     mini_batch_grads = []\n",
    "        # in each epoch, we'll visit all users in a random order\n",
    "        visitingOrder = np.array(trStats[\"u_users\"])\n",
    "        np.random.shuffle(visitingOrder)\n",
    "#         for i in range(0, visitingOrder.shape[0], minibatch_size):\n",
    "#                 # Get pair of (X, y) of the current minibatch/chunk\n",
    "#             visitingOrderMini = visitingOrder[i:i + minibatch_size]\n",
    "#                 y_train_mini = y_train[i:i + minibatch_size]\n",
    "#         for i in range(0, visitingOrder.shape[0], minibatch_size):\n",
    "        for user in visitingOrder:\n",
    "            # get the ratings of that user\n",
    "            ratingsForUser = lib.getRatingsForUser(user, training)\n",
    "\n",
    "            # build the visible input\n",
    "            v = getV(ratingsForUser)\n",
    "\n",
    "            # get the weights associated to movies the user has seen\n",
    "            weightsForUser = W[ratingsForUser[:, 0], :, :]\n",
    "\n",
    "            ### LEARNING ###\n",
    "            # propagate visible input to hidden units\n",
    "            posHiddenProb = visibleToHiddenVec(v, weightsForUser, hid_bias)\n",
    "            # get positive gradient\n",
    "            # note that we only update the movies that this user has seen!\n",
    "            posprods[ratingsForUser[:, 0], :, :] += probProduct(v, posHiddenProb)\n",
    "\n",
    "            ### UNLEARNING ###\n",
    "            # sample from hidden distribution\n",
    "            sampledHidden = sample(posHiddenProb)\n",
    "            # propagate back to get \"negative data\"\n",
    "            negData = hiddenToVisible(sampledHidden, weightsForUser, vis_bias[ratingsForUser[:,0]])\n",
    "            # propagate negative data to hidden units\n",
    "            negHiddenProb = visibleToHiddenVec(negData, weightsForUser, hid_bias)\n",
    "            # get negative gradient\n",
    "            # note that we only update the movies that this user has seen!\n",
    "            negprods[ratingsForUser[:, 0], :, :] += probProduct(negData, negHiddenProb)\n",
    "            \n",
    "            poshidact = sum(posHiddenProb)\n",
    "            posvisact = sum(v)\n",
    "            neghidact = sum(negHiddenProb)\n",
    "            negvisact = sum(negData)\n",
    "            # we average over the number of users in the batch (if we use mini-batch)\n",
    "#             grad = (gradientLearningRate/epoch)*(posprods-negprods)\n",
    "            grad_w = adaptiveLearn(learning_rate_type=learning_rate_type, k=learning_rate_k, drop=learning_rate_drop, \n",
    "                                 epochs_drop=learning_rate_epochs_drop, epoch=epoch)*(posprods-negprods+2*_lambda*W)\n",
    "    #         mini_batch_grads.append(grad)\n",
    "\n",
    "        #     m = alpha*m+grad\n",
    "            m_w = alpha*m_w+grad_w\n",
    "            m_v = alpha*m_v+(gradientLearningRate_v) * (posvisact - negvisact)\n",
    "            m_h = alpha*m_h+(gradientLearningRate_h) * (poshidact - neghidact)\n",
    "            \n",
    "            if momentum == False:\n",
    "                W += grad_w\n",
    "            else:\n",
    "                W += m_w\n",
    "            \n",
    "            vis_bias += m_v\n",
    "            hid_bias += m_h\n",
    "\n",
    "        # Print the current RMSE for training and validation sets\n",
    "        # this allows you to control for overfitting e.g\n",
    "        # We predict over the training set\n",
    "        tr_r_hat = predict(trStats[\"movies\"], trStats[\"users\"], W, training, vis_bias, hid_bias, predictType='exp')\n",
    "    #     print (tr_r_hat)\n",
    "        trRMSE = lib.rmse(trStats[\"ratings\"], tr_r_hat)\n",
    "    #     print (trRMSE)\n",
    "        if trRMSE < best_train_loss:\n",
    "            best_train_loss = trRMSE\n",
    "            best_training_weights = W\n",
    "            best_train_predictions = tr_r_hat\n",
    "\n",
    "        # We predict over the validation set\n",
    "        vl_r_hat = predict(vlStats[\"movies\"], vlStats[\"users\"], W, training, vis_bias, hid_bias, predictType='exp')\n",
    "    #     vl_r_hat\n",
    "        vlRMSE = lib.rmse(vlStats[\"ratings\"], vl_r_hat)\n",
    "        if vlRMSE < best_validation_loss:\n",
    "            best_validation_loss = vlRMSE\n",
    "            best_validation_weights = W\n",
    "            best_validation_predictions = vl_r_hat\n",
    "\n",
    "        train_loss.append(trRMSE)\n",
    "        validation_loss.append(vlRMSE)\n",
    "\n",
    "        print (\"### EPOCH %d ###\" % epoch)\n",
    "        print (\"Training loss = %f\" % trRMSE)\n",
    "        print (\"Validation loss = %f\" % vlRMSE)\n",
    "\n",
    "    ### END ###\n",
    "    # This part you can write on your own\n",
    "    # you could plot the evolution of the training and validation RMSEs for example\n",
    "    # predictedRatings = np.array([predictForUser(user, W, training) for user in trStats[\"u_users\"]])\n",
    "    # np.savetxt(\"predictedRatings.txt\", predictedRatings)\n",
    "    # fig1 = plt.figure()\n",
    "    # ax1 = fig1.add_subplot(121)\n",
    "    # ax1.plot(train_loss)\n",
    "    # ax2 = fig1.add_subplot(122)\n",
    "    # ax2.plot(validation_loss)\n",
    "    \n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(validation_loss)\n",
    "    plt.show()\n",
    "    if stopping==True:\n",
    "        print('Best training loss = %f' % best_train_loss)\n",
    "        print('Best validation loss = %f' % best_validation_loss)\n",
    "    else:\n",
    "        print('Final training loss = %f' % trRMSE)\n",
    "        print('Final validation loss = %f' % vlRMSE)\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(momentum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(learning_rate_type='time', learning_rate_k=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(learning_rate_type='step', learning_rate_drop=0.5, learning_rate_epochs_drop=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(learning_rate_type='exponential', learning_rate_k=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Predicting with the following hyperparameters:\n",
      "    K = 5\n",
      "    F = 5\n",
      "    epochs = 30\n",
      "    gradientLearningRate = 0.0001\n",
      "    gradientLearningRate_v = 0.001\n",
      "    gradientLearningRate_h = 0.001\n",
      "    minibatch_size = None\n",
      "    alpha = 0.9\n",
      "    stopping = True\n",
      "    momentum = True\n",
      "    learning_rate_type = time\n",
      "    learning_rate_k = 0.5\n",
      "    learning_rate_drop = 0.5\n",
      "    learning_rate_epochs_drop = 10.0\n",
      "    _lambda = 0.3\n",
      "### EPOCH 1 ###\n",
      "Training loss = 1.186179\n",
      "Validation loss = 1.211524\n",
      "### EPOCH 2 ###\n",
      "Training loss = 1.185353\n",
      "Validation loss = 1.215904\n",
      "### EPOCH 3 ###\n",
      "Training loss = 1.182904\n",
      "Validation loss = 1.220290\n",
      "### EPOCH 4 ###\n",
      "Training loss = 1.172924\n",
      "Validation loss = 1.219962\n",
      "### EPOCH 5 ###\n",
      "Training loss = 1.166702\n",
      "Validation loss = 1.219966\n",
      "### EPOCH 6 ###\n",
      "Training loss = 1.159346\n",
      "Validation loss = 1.219846\n",
      "### EPOCH 7 ###\n",
      "Training loss = 1.154766\n",
      "Validation loss = 1.214797\n",
      "### EPOCH 8 ###\n",
      "Training loss = 1.152552\n",
      "Validation loss = 1.217785\n",
      "### EPOCH 9 ###\n",
      "Training loss = 1.147269\n",
      "Validation loss = 1.216966\n",
      "### EPOCH 10 ###\n",
      "Training loss = 1.141807\n",
      "Validation loss = 1.211090\n",
      "### EPOCH 11 ###\n",
      "Training loss = 1.138243\n",
      "Validation loss = 1.209460\n",
      "### EPOCH 12 ###\n",
      "Training loss = 1.133574\n",
      "Validation loss = 1.214773\n",
      "### EPOCH 13 ###\n",
      "Training loss = 1.130502\n",
      "Validation loss = 1.217550\n",
      "### EPOCH 14 ###\n",
      "Training loss = 1.128792\n",
      "Validation loss = 1.217589\n",
      "### EPOCH 15 ###\n",
      "Training loss = 1.128524\n",
      "Validation loss = 1.220117\n",
      "### EPOCH 16 ###\n",
      "Training loss = 1.131583\n",
      "Validation loss = 1.222661\n",
      "### EPOCH 17 ###\n",
      "Training loss = 1.130525\n",
      "Validation loss = 1.221922\n",
      "### EPOCH 18 ###\n",
      "Training loss = 1.132221\n",
      "Validation loss = 1.219158\n",
      "### EPOCH 19 ###\n",
      "Training loss = 1.130394\n",
      "Validation loss = 1.219503\n",
      "### EPOCH 20 ###\n",
      "Training loss = 1.127282\n",
      "Validation loss = 1.217735\n",
      "### EPOCH 21 ###\n",
      "Training loss = 1.125638\n",
      "Validation loss = 1.219052\n",
      "### EPOCH 22 ###\n",
      "Training loss = 1.124064\n",
      "Validation loss = 1.216121\n",
      "### EPOCH 23 ###\n",
      "Training loss = 1.121675\n",
      "Validation loss = 1.214256\n",
      "### EPOCH 24 ###\n",
      "Training loss = 1.121537\n",
      "Validation loss = 1.216627\n",
      "### EPOCH 25 ###\n",
      "Training loss = 1.121487\n",
      "Validation loss = 1.212812\n",
      "### EPOCH 26 ###\n",
      "Training loss = 1.122060\n",
      "Validation loss = 1.215645\n",
      "### EPOCH 27 ###\n",
      "Training loss = 1.122332\n",
      "Validation loss = 1.215905\n",
      "### EPOCH 28 ###\n",
      "Training loss = 1.120651\n",
      "Validation loss = 1.215555\n",
      "### EPOCH 29 ###\n",
      "Training loss = 1.122716\n",
      "Validation loss = 1.216721\n",
      "### EPOCH 30 ###\n",
      "Training loss = 1.122815\n",
      "Validation loss = 1.213469\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training loss = 1.120651\n",
      "Best validation loss = 1.209460\n"
     ]
    }
   ],
   "source": [
    "main_rbm(F=5, learning_rate_type='time', learning_rate_k=0.5, stopping=True, momentum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_train_loss\n",
    "best_validation_loss\n",
    "min(best_train_predictions)\n",
    "max(best_train_predictions)\n",
    "min(best_validation_predictions)\n",
    "max(best_validation_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(posprods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlStats[\"users\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(vl_r_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(validation_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
