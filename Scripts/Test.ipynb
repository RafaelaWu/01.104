{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rbm\n",
    "import numpy as np\n",
    "import projectLib as lib\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F=5\n",
    "K=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = lib.getTrainingData()\n",
    "ratingsForUser1 = lib.getRatingsForUser(1, training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratingsPerMovie(training)\n",
    "v = rbm.getV(ratingsForUser1)\n",
    "# v.shape\n",
    "w = rbm.getInitialWeights(v.shape[0],F,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trStats = lib.getUsefulStats(training)\n",
    "W = rbm.getInitialWeights(trStats[\"n_movies\"], F, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_movies=trStats[\"n_movies\"]\n",
    "vis_bias=np.zeros((num_movies,5))\n",
    "hid_bias=np.zeros((F,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [0,1,0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm.hiddenToVisible(h, W[ratingsForUser1[:, 0], :, :])\n",
    "h = [0,1,0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm.getPredictedDistribution(v, W[ratingsForUser1[:, 0], :, :], W[1, :, :], vis_bias, hid_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load mainrbm.py\n",
    "import numpy as np\n",
    "import rbm\n",
    "import projectLib as lib\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "\n",
    "training = lib.getTrainingData()\n",
    "validation = lib.getValidationData()\n",
    "# You could also try with the chapter 4 data\n",
    "# training = lib.getChapter4Data()\n",
    "\n",
    "trStats = lib.getUsefulStats(training)\n",
    "vlStats = lib.getUsefulStats(validation)\n",
    "\n",
    "K = 5\n",
    "alpha = 0.9\n",
    "\n",
    "# SET PARAMETERS HERE!!!\n",
    "# number of hidden units\n",
    "F = 5\n",
    "epochs = 30\n",
    "gradientLearningRate = 0.0001\n",
    "gradientLearningRate_v = 0.001\n",
    "gradientLearningRate_h = 0.001\n",
    "_lambda = 1\n",
    "minibatch_size = 10\n",
    "\n",
    "def adaptiveLearn(learning_rate_type=['constant','adapt','time', 'step', 'exponential'], \n",
    "                  lr=0.0001, k=0.1, epoch=None, drop=0.5, epochs_drop=10.0):\n",
    "#time-based decay, step decay and exponential decay\n",
    "    if learning_rate_type == 'constant':\n",
    "        return lr\n",
    "    elif learning_rate_type == 'adapt':\n",
    "        return lr/(epoch**2)\n",
    "    elif learning_rate_type == 'time':\n",
    "        return lr/(1.0+k*epoch)\n",
    "    elif learning_rate_type == 'step':\n",
    "        return lr*np.power(drop,np.floor((1+epoch)/epochs_drop))\n",
    "    elif learning_rate_type == 'exponential':\n",
    "        return lr*np.exp(-k*epoch)\n",
    "    \n",
    "def batch_get(array, B):\n",
    "    ret = []\n",
    "    for i in range(int(len(array)/B)):\n",
    "        ret.append(array[i*B:i*B+B])\n",
    "    if len(array)%B != 0:\n",
    "        ret.append(array[len(array)/B:])\n",
    "    return ret\n",
    "\n",
    "def main_rbm(training=training, validation=validation, trStats=trStats, vlStats=vlStats, \n",
    "             K=5, F=5, epochs=30, gradientLearningRate=0.0001,gradientLearningRate_v = 0.0001,\n",
    "             gradientLearningRate_h = 0.0001, minibatch_size=10, alpha=0.9, \n",
    "             stopping=False, momentum=False, learning_rate_type='time', learning_rate_k=0.1, \n",
    "             learning_rate_drop=0.5, learning_rate_epochs_drop=10.0, _lambda = 0.3):\n",
    "    \n",
    "    # Print current hyperparams\n",
    "#     frame = inspect.currentframe()\n",
    "#     args, _, _, values = inspect.getargvalues(frame)\n",
    "#     print ('Training and Predicting with the following hyperparameters:')\n",
    "#     for i in args[4:]:\n",
    "#         print (\"    %s = %s\" % (i, values[i]))\n",
    "        \n",
    "    # Initialise all our arrays\n",
    "    num_movies=trStats[\"n_movies\"]\n",
    "    num_users=trStats[\"n_users\"]\n",
    "    W = rbm.getInitialWeights(trStats[\"n_movies\"], F, K)\n",
    "    posprods = np.zeros(W.shape)\n",
    "    negprods = np.zeros(W.shape)\n",
    "    grad_w = np.zeros(W.shape)\n",
    "    m_w=np.zeros((W.shape[0],F,5))\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    vis_bias=np.zeros((num_movies,5))\n",
    "    m_v=np.zeros((num_movies,5))\n",
    "    hid_bias=np.zeros((F,))\n",
    "    m_h=np.zeros((F,))\n",
    "    best_train_loss = 100\n",
    "    best_validation_loss = 100\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "    #     mini_batch_grads = []\n",
    "        # in each epoch, we'll visit all users in a random order\n",
    "        visitingOrder = np.array(trStats[\"u_users\"])\n",
    "        np.random.shuffle(visitingOrder)\n",
    "#         for i in range(0, visitingOrder.shape[0], minibatch_size):\n",
    "#                 # Get pair of (X, y) of the current minibatch/chunk\n",
    "#             visitingOrderMini = visitingOrder[i:i + minibatch_size]\n",
    "#                 y_train_mini = y_train[i:i + minibatch_size]\n",
    "#         for i in range(0, visitingOrder.shape[0], minibatch_size):\n",
    "        batches = batch_get(visitingOrder, minibatch_size)\n",
    "        poshidact,posvisact,neghidact,negvisact = 0,0,0,0\n",
    "        for batch in batches:\n",
    "            prev_grad = grad_w\n",
    "            grad_w = np.zeros(W.shape)\n",
    "            for user in batch:\n",
    "                # get the ratings of that user\n",
    "                ratingsForUser = lib.getRatingsForUser(user, training)\n",
    "\n",
    "                # build the visible input\n",
    "                v = rbm.getV(ratingsForUser)\n",
    "\n",
    "                # get the weights associated to movies the user has seen\n",
    "                weightsForUser = W[ratingsForUser[:, 0], :, :]\n",
    "\n",
    "                ### LEARNING ###\n",
    "                # propagate visible input to hidden units\n",
    "                posHiddenProb = rbm.visibleToHiddenVec(v, weightsForUser,hid_bias) #, hid_bias)\n",
    "                # get positive gradient\n",
    "                # note that we only update the movies that this user has seen!\n",
    "                posprods[ratingsForUser[:, 0], :, :] += rbm.probProduct(v, posHiddenProb)\n",
    "\n",
    "                ### UNLEARNING ###\n",
    "                # sample from hidden distribution\n",
    "                sampledHidden = rbm.sample(posHiddenProb)\n",
    "                # propagate back to get \"negative data\"\n",
    "                negData = rbm.hiddenToVisible(sampledHidden, weightsForUser,vis_bias[ratingsForUser[:,0]])#, vis_bias[ratingsForUser[:,0]])\n",
    "                # propagate negative data to hidden units\n",
    "                negHiddenProb = rbm.visibleToHiddenVec(negData, weightsForUser,hid_bias) #, hid_bias)\n",
    "                # get negative gradient\n",
    "                # note that we only update the movies that this user has seen!\n",
    "                negprods[ratingsForUser[:, 0], :, :] += rbm.probProduct(negData, negHiddenProb)\n",
    "\n",
    "                poshidact += sum(posHiddenProb)\n",
    "                posvisact += sum(v)\n",
    "                neghidact += sum(negHiddenProb)\n",
    "                negvisact += sum(negData)\n",
    "                # we average over the number of users in the batch (if we use mini-batch)\n",
    "    #             grad = (gradientLearningRate/epoch)*(posprods-negprods)\n",
    "                '''\n",
    "                Regularization - \n",
    "                '''\n",
    "                grad_w += adaptiveLearn(learning_rate_type=learning_rate_type, k=learning_rate_k, \n",
    "                                        drop=learning_rate_drop, epochs_drop=learning_rate_epochs_drop, \n",
    "                                        epoch=epoch)*((posprods-negprods)/trStats[\"n_users\"]-_lambda*W)\n",
    "        #         mini_batch_grads.append(grad)\n",
    "\n",
    "            #     m = alpha*m+grad\n",
    "                '''\n",
    "                Ask about the implementation of biases (should we create matrix of biases for hidden and visible layers?)\n",
    "                '''\n",
    "            m_w = alpha*m_w + grad_w\n",
    "            m_v = alpha*m_v+(gradientLearningRate_v) * (posvisact - negvisact)\n",
    "            m_h = alpha*m_h+(gradientLearningRate_h) * (poshidact - neghidact)\n",
    "\n",
    "            if momentum == False:\n",
    "                W += grad_w\n",
    "            else:\n",
    "                W += m_w\n",
    "\n",
    "            vis_bias += m_v\n",
    "            hid_bias += m_h\n",
    "\n",
    "        # Print the current RMSE for training and validation sets\n",
    "        # this allows you to control for overfitting e.g\n",
    "        # We predict over the training set\n",
    "        tr_r_hat = rbm.predict(trStats[\"movies\"], trStats[\"users\"], W, training,vis_bias,hid_bias) #, vis_bias, hid_bias, predictType='exp')\n",
    "    #     print (tr_r_hat)\n",
    "        trRMSE = lib.rmse(trStats[\"ratings\"], tr_r_hat)\n",
    "    #     print (trRMSE)\n",
    "        if trRMSE < best_train_loss:\n",
    "            best_train_loss = trRMSE\n",
    "            best_training_weights = W\n",
    "            best_train_predictions = tr_r_hat\n",
    "\n",
    "        # We predict over the validation set\n",
    "        vl_r_hat = rbm.predict(vlStats[\"movies\"], vlStats[\"users\"], W, training,vis_bias,hid_bias) #, vis_bias, hid_bias, predictType='exp')\n",
    "    #     vl_r_hat\n",
    "        vlRMSE = lib.rmse(vlStats[\"ratings\"], vl_r_hat)\n",
    "        if vlRMSE < best_validation_loss:\n",
    "            best_validation_loss = vlRMSE\n",
    "            best_validation_weights = W\n",
    "            best_validation_predictions = vl_r_hat\n",
    "            best_vis_bias = vis_bias\n",
    "            best_hid_bias = hid_bias\n",
    "#             best_momentum = momentum\n",
    "#             best_reg = regularization\n",
    "#             best_epoch = epoch\n",
    "#             best_alpha = alpha\n",
    "#             best_B = B\n",
    "#             best_F = F\n",
    "#             min_rmse = vlRMSE\n",
    "#                     print('Best RMSE:', min_rmse)\n",
    "\n",
    "        train_loss.append(trRMSE)\n",
    "        validation_loss.append(vlRMSE)\n",
    "\n",
    "        print (\"### EPOCH %d ###\" % epoch)\n",
    "        print (\"Training loss = %f\" % trRMSE)\n",
    "        print (\"Validation loss = %f\" % vlRMSE)\n",
    "\n",
    "    ### END ###\n",
    "    # This part you can write on your own\n",
    "    # you could plot the evolution of the training and validation RMSEs for example\n",
    "    # predictedRatings = np.array([predictForUser(user, W, training) for user in trStats[\"u_users\"]])\n",
    "    # np.savetxt(\"predictedRatings.txt\", predictedRatings)\n",
    "    # fig1 = plt.figure()\n",
    "    # ax1 = fig1.add_subplot(121)\n",
    "    # ax1.plot(train_loss)\n",
    "    # ax2 = fig1.add_subplot(122)\n",
    "    # ax2.plot(validation_loss)\n",
    "    \n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(validation_loss)\n",
    "    plt.show()\n",
    "    if stopping==True:\n",
    "        print('Best training loss = %f' % best_train_loss)\n",
    "        print('Best validation loss = %f' % best_validation_loss)\n",
    "    else:\n",
    "        print('Final training loss = %f' % trRMSE)\n",
    "        print('Final validation loss = %f' % vlRMSE)\n",
    "    return [best_validation_loss, best_validation_predictions, best_validation_weights, best_vis_bias, best_vis_bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### EPOCH 1 ###\n",
      "Training loss = 1.212948\n",
      "Validation loss = 1.237642\n",
      "### EPOCH 2 ###\n",
      "Training loss = 1.212118\n",
      "Validation loss = 1.237540\n",
      "### EPOCH 3 ###\n",
      "Training loss = 1.211934\n",
      "Validation loss = 1.236046\n",
      "### EPOCH 4 ###\n",
      "Training loss = 1.211350\n",
      "Validation loss = 1.235989\n",
      "### EPOCH 5 ###\n",
      "Training loss = 1.210170\n",
      "Validation loss = 1.234543\n",
      "### EPOCH 6 ###\n",
      "Training loss = 1.209901\n",
      "Validation loss = 1.232846\n",
      "### EPOCH 7 ###\n",
      "Training loss = 1.209575\n",
      "Validation loss = 1.233661\n",
      "### EPOCH 8 ###\n",
      "Training loss = 1.209530\n",
      "Validation loss = 1.233047\n",
      "### EPOCH 9 ###\n",
      "Training loss = 1.208982\n",
      "Validation loss = 1.232754\n",
      "### EPOCH 10 ###\n",
      "Training loss = 1.208381\n",
      "Validation loss = 1.232253\n",
      "### EPOCH 11 ###\n",
      "Training loss = 1.207930\n",
      "Validation loss = 1.232406\n",
      "### EPOCH 12 ###\n",
      "Training loss = 1.207668\n",
      "Validation loss = 1.231578\n",
      "### EPOCH 13 ###\n",
      "Training loss = 1.207554\n",
      "Validation loss = 1.232297\n",
      "### EPOCH 14 ###\n",
      "Training loss = 1.207176\n",
      "Validation loss = 1.231020\n",
      "### EPOCH 15 ###\n",
      "Training loss = 1.206571\n",
      "Validation loss = 1.230581\n",
      "### EPOCH 16 ###\n",
      "Training loss = 1.206043\n",
      "Validation loss = 1.229861\n",
      "### EPOCH 17 ###\n",
      "Training loss = 1.205318\n",
      "Validation loss = 1.228929\n",
      "### EPOCH 18 ###\n",
      "Training loss = 1.204692\n",
      "Validation loss = 1.228984\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter tuning:\n",
    "\n",
    "# mrange = np.linspace(0.7,0.95,5)\n",
    "mrange = [0.7, 0.8, 0.9]\n",
    "# rrange = np.linspace(0.1,0.9,5)\n",
    "rrange = [0.1, 0.5, 0.9]\n",
    "arange = [0.0001, 0.001, 0.01]\n",
    "brange = [10]\n",
    "frange = [5, 10, 20]\n",
    "\n",
    "best_momentum = 0\n",
    "best_reg = 0\n",
    "best_lr = 0\n",
    "best_batch = 0\n",
    "best_F = 0\n",
    "\n",
    "min_rmse=10\n",
    "for momentum in mrange:\n",
    "    for regularization in rrange:\n",
    "        for learning_rate in arange:\n",
    "            for batch in brange:\n",
    "                for F in frange:\n",
    "                    prediction = main_rbm(training=training, validation=validation, trStats=trStats, vlStats=vlStats, \n",
    "                                 K=5, F=int(F), epochs=30, gradientLearningRate=learning_rate, \n",
    "                                 gradientLearningRate_v = 0.0001/int(batch), gradientLearningRate_h = 0.0001/int(batch), \n",
    "                                 minibatch_size=int(batch), alpha=momentum, stopping=True, momentum=True, \n",
    "                                 learning_rate_type='time', learning_rate_k=0.5, learning_rate_drop=0.5, \n",
    "                                 learning_rate_epochs_drop=10.0, _lambda = regularization)\n",
    "                    if prediction[0] < min_rmse:\n",
    "                        best_momentum = momentum\n",
    "                        best_reg = regularization\n",
    "#                             best_epoch = epoch\n",
    "                        best_lr = learning_rate\n",
    "                        best_batch = batch\n",
    "                        best_F = F\n",
    "                        best_predict = prediction[1]\n",
    "                        min_rmse = prediction[0]\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Tune other parameters and add biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-edc49aa26048>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m              \u001b[0mgradientLearningRate_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_momentum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m              \u001b[0mstopping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_lr\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m              learning_rate_drop=0.5, learning_rate_epochs_drop=10.0, _lambda = best_reg)\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-05c74020e20a>\u001b[0m in \u001b[0;36mmain_rbm\u001b[1;34m(training, validation, trStats, vlStats, K, F, epochs, gradientLearningRate, gradientLearningRate_v, gradientLearningRate_h, minibatch_size, alpha, stopping, momentum, learning_rate_type, learning_rate_k, learning_rate_drop, learning_rate_epochs_drop, _lambda)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;31m#                 y_train_mini = y_train[i:i + minibatch_size]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;31m#         for i in range(0, visitingOrder.shape[0], minibatch_size):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvisitingOrder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mposhidact\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mposvisact\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mneghidact\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnegvisact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-05c74020e20a>\u001b[0m in \u001b[0;36mbatch_get\u001b[1;34m(array, B)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbatch_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mB\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "## Final parameters:\n",
    "alpha = 0.9\n",
    "_lambda = 0.1 #tuned\n",
    "gradientLearningRate = 0.001 #tuned\n",
    "minibatch_size = 10\n",
    "F = 8 \n",
    "\n",
    "W = main_rbm(training=training, validation=validation, trStats=trStats, vlStats=vlStats, \n",
    "             K=5, F=best_F, epochs=30, gradientLearningRate=best_lr, gradientLearningRate_v = 0.0001/best_batch,\n",
    "             gradientLearningRate_h = 0.0001/best_batch, minibatch_size=best_batch, alpha=best_momentum, \n",
    "             stopping=True, momentum=True, learning_rate_type='time', learning_rate_k=best_lr/30, \n",
    "             learning_rate_drop=0.5, learning_rate_epochs_drop=10.0, _lambda = best_reg)\n",
    "\n",
    "vis = W[3]\n",
    "hid = W[4]\n",
    "\n",
    "predictedRatings = np.array([rbm.predictForUser(user, W[2], training,vis,hid) for user in trStats[\"u_users\"]])\n",
    "np.savetxt(\"SpateggiChikenChoope+v2.txt\", predictedRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W[3][1]\n",
    "W[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictMovieForUser1(q, user, W, training,vis_bias,hid_bias, predictType=\"exp\"):\n",
    "    # movie is movie idx\n",
    "    # user is user ID\n",
    "    # type can be \"max\" or \"exp\"\n",
    "    ratingsForUser = lib.getRatingsForUser(user, training)\n",
    "    v = rbm.getV(ratingsForUser)\n",
    "    ratingDistribution = rbm.getPredictedDistribution(v, W[ratingsForUser[:, 0], :, :], W[q, :, :],vis_bias,hid_bias)\n",
    "    if predictType == \"max\":\n",
    "        return rbm.predictRatingMax(ratingDistribution)\n",
    "    elif predictType == \"mean\":\n",
    "        return rbm.predictRatingMean(ratingDistribution)\n",
    "    else:\n",
    "        return rbm.predictRatingExp(ratingDistribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsForUser1 = lib.getRatingsForUser(0, training)\n",
    "ratingsForUser1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictForUser1(user, W, training,vis_bias,hid_bias, predictType=\"exp\"):\n",
    "    ### TO IMPLEMENT\n",
    "    # given a user ID, predicts all movie ratings for the user\n",
    "    return [predictMovieForUser1(movie, user, W, training,vis_bias,hid_bias, predictType=predictType) for movie in lib.getUsefulStats(training)[\"u_movies\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedRatings = np.array([predictForUser1(user, W[2], training,vis,hid) for user in trStats[\"u_users\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
