{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import projectLib as lib\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "\n",
    "# set highest rating\n",
    "K = 5\n",
    "F = 3\n",
    "eps=0.1\n",
    "\n",
    "def softmax(x):\n",
    "    # Numerically stable softmax function\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def ratingsPerMovie(training):\n",
    "    movies = [x[0] for x in training]\n",
    "    u_movies = np.unique(movies).tolist()\n",
    "    return np.array([[i, movie, len([x for x in training if x[0] == movie])] for i, movie in enumerate(u_movies)])\n",
    "\n",
    "def getV(ratingsForUser):\n",
    "    # ratingsForUser is obtained from the ratings for user library\n",
    "    # you should return a binary matrix ret of size m x K, where m is the number of movies\n",
    "    #   that the user has seen. ret[i][k] = 1 if the user\n",
    "    #   has rated movie ratingsForUser[i, 0] with k stars\n",
    "    #   otherwise it is 0\n",
    "    ret = np.zeros((len(ratingsForUser), K))\n",
    "    for i in range(len(ratingsForUser)):\n",
    "        ret[i, ratingsForUser[i, 1]-1] = 1.0\n",
    "    return ret\n",
    "\n",
    "def getInitialWeights(m, F, K):\n",
    "    # m is the number of visible units\n",
    "    # F is the number of hidden units\n",
    "    # K is the highest rating (fixed to 5 here)\n",
    "    return np.random.normal(0, 0.1, (m, F, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = lib.getTrainingData()\n",
    "ratingsForUser1 = lib.getRatingsForUser(1, training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ratingsPerMovie(training)\n",
    "v = getV(ratingsForUser1)\n",
    "# v.shape\n",
    "w = getInitialWeights(v.shape[0],F,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trStats = lib.getUsefulStats(training)\n",
    "W = getInitialWeights(trStats[\"n_movies\"], F, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # x is a real vector of size n\n",
    "    # ret should be a vector of size n where ret_i = sigmoid(x_i)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def visibleToHiddenVec(v, w, hid_bias):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # v is a matrix of size m x 5. Each row is a binary vector representing a rating\n",
    "    #    OR a probability distribution over the rating\n",
    "    # w is a list of matrices of size m x F x 5\n",
    "    # ret should be a vector of size F\n",
    "    m,f,K=w.shape\n",
    "    output=list()\n",
    "    for i in range(f):\n",
    "        summ=0\n",
    "        for k in range(K):\n",
    "            for j in range(m):\n",
    "                summ+=v[j,k]*w[j,i,k]\n",
    "        output.append(summ)\n",
    "\n",
    "    return sig(np.array(output)+hid_bias)\n",
    "    \n",
    "\n",
    "def hiddenToVisible(h, w, vis_bias):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # h is a binary vector of size F\n",
    "    # w is an array of size m x F x 5\n",
    "    # ret should be a matrix of size m x 5, where m\n",
    "    #   is the number of movies the user has seen.\n",
    "    #   Remember that we do not reconstruct movies that the user\n",
    "    #   has not rated! (where reconstructing means getting a distribution\n",
    "    #   over possible ratings).\n",
    "    #   We only do so when we predict the rating a user would have given to a movie.\n",
    "#     print(w.shape)\n",
    "    output=w[:,0,:]*h[0]\n",
    "    m,f,k=w.shape\n",
    "    for i in range(1,f):\n",
    "        output+=w[:,i,:]*h[i]\n",
    "    return sig(output+vis_bias)\n",
    "\n",
    "def probProduct(v, p):\n",
    "    # v is a matrix of size m x 5\n",
    "    # p is a vector of size F, activation of the hidden units\n",
    "    # returns the gradient for visible input v and hidden activations p\n",
    "    ret = np.zeros((v.shape[0], p.size, v.shape[1]))\n",
    "    for i in range(v.shape[0]):\n",
    "        for j in range(p.size):\n",
    "            for k in range(v.shape[1]):\n",
    "                ret[i, j, k] = v[i, k] * p[j]\n",
    "    return ret\n",
    "\n",
    "def sample(p):\n",
    "    # p is a vector of real numbers between 0 and 1\n",
    "    # ret is a vector of same size as p, where ret_i = Ber(p_i)\n",
    "    # In other word we sample from a Bernouilli distribution with\n",
    "    # parameter p_i to obtain ret_i\n",
    "    samples = np.random.random(p.size)\n",
    "    return np.array(samples <= p, dtype=int)\n",
    "\n",
    "def getPredictedDistribution(v, w, wq,vis_bias,hid_bias):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # This function returns a distribution over the ratings for movie q, if user data is v\n",
    "    # v is the dataset of the user we are predicting the movie for\n",
    "    #   It is a m x 5 matrix, where m is the number of movies in the\n",
    "    #   dataset of this user.\n",
    "    # w is the weights array for the current user, of size m x F x 5\n",
    "    # wq is the weight matrix of size F x 5 for movie q\n",
    "    #   If W is the whole weights array, then wq = W[q, :, :]\n",
    "    # You will need to perform the same steps done in the learning/unlearning:\n",
    "    #   - Propagate the user input to the hidden units\n",
    "    #   - Sample the state of the hidden units\n",
    "    #   - Backpropagate these hidden states to obtain\n",
    "    #       the distribution over the movie whose associated weights are wq\n",
    "    # ret is a vector of size 5\n",
    "    m,f,K=w.shape\n",
    "    p_learn = visibleToHiddenVec(v,w,hid_bias)\n",
    "    PG = probProduct(v, p_learn)\n",
    "    hidden_activations = sample(p_learn)\n",
    "    v_negative = hiddenToVisible(hidden_activations, wq.reshape(1,wq.shape[0],wq.shape[1]),vis_bias)\n",
    "\n",
    "    return v_negative.reshape(5)\n",
    "\n",
    "def predictRatingMax(ratingDistribution):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # ratingDistribution is a probability distribution over possible ratings\n",
    "    #   It is obtained from the getPredictedDistribution function\n",
    "    # This function is one of three you are to implement\n",
    "    # that returns a rating from the distribution\n",
    "    # We decide here that the predicted rating will be the one with the highest probability\n",
    "    max_indices = np.where(ratingDistribution == np.amax(ratingDistribution))[0]\n",
    "    if max_indices.shape[0] == 1:\n",
    "        result = max_indices.item()\n",
    "    else:\n",
    "        result = 2\n",
    "    return result+1\n",
    "\n",
    "def predictRatingMean(ratingDistribution):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # ratingDistribution is a probability distribution over possible ratings\n",
    "    #   It is obtained from the getPredictedDistribution function\n",
    "    # This function is one of three you are to implement\n",
    "    # that returns a rating from the distribution\n",
    "    # We decide here that the predicted rating will be the expectation over ratingDistribution\n",
    "    normalized = [i/sum(ratingDistribution) for i in ratingDistribution]\n",
    "    result = 0 \n",
    "    for k in range(ratingDistribution.shape[0]):\n",
    "        result += normalized[k]*(k+1)\n",
    "    return result\n",
    "\n",
    "def predictRatingExp(ratingDistribution):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # ratingDistribution is a probability distribution over possible ratings\n",
    "    #   It is obtained from the getPredictedDistribution function\n",
    "    # This function is one of three you are to implement\n",
    "    # that returns a rating from the distribution\n",
    "    # We decide here that the predicted rating will be the expectation over\n",
    "    # the softmax applied to ratingDistribution\n",
    "    softmax = np.exp(ratingDistribution)/sum(np.exp(ratingDistribution))\n",
    "#     print(softmax)\n",
    "    result = 0 \n",
    "    for k in range(len(ratingDistribution)):\n",
    "        result += softmax[k]*(k+1)\n",
    "#         print (result)\n",
    "    return result\n",
    "\n",
    "def predictMovieForUser(q, user, W, training,vis_bias,hid_bias, predictType=\"exp\"):\n",
    "    # movie is movie idx\n",
    "    # user is user ID\n",
    "    # type can be \"max\" or \"exp\"\n",
    "    ratingsForUser = lib.getRatingsForUser(user, training)\n",
    "    v = getV(ratingsForUser)\n",
    "#     print(np.where(ratingsForUser[:,0]==q))\n",
    "    ratingDistribution = getPredictedDistribution(v, W[ratingsForUser[:, 0], :, :], W[q, :, :],vis_bias[q],hid_bias)#[np.where(ratingsForUser[:,0]==q)[0][0]]\n",
    "    if predictType == \"max\":\n",
    "        return predictRatingMax(ratingDistribution)\n",
    "    elif predictType == \"mean\":\n",
    "        return predictRatingMean(ratingDistribution)\n",
    "    else:\n",
    "        return predictRatingExp(ratingDistribution)\n",
    "\n",
    "def predict(movies, users, W, training,vis_bias,hid_bias, predictType=\"exp\"):\n",
    "    # given a list of movies and users, predict the rating for each (movie, user) pair\n",
    "    # used to compute RMSE\n",
    "    return [predictMovieForUser(movie, user, W, training,vis_bias,hid_bias, predictType=predictType) for (movie, user) in zip(movies, users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptiveLearn(learning_rate_type=['constant','adapt','time', 'step', 'exponential'], \n",
    "                  lr=0.0001, k=0.1, epoch=None, drop=0.5, epochs_drop=10.0):\n",
    "#time-based decay, step decay and exponential decay\n",
    "    if learning_rate_type == 'constant':\n",
    "        return lr\n",
    "    elif learning_rate_type == 'adapt':\n",
    "        return lr/(epoch**2)\n",
    "    elif learning_rate_type == 'time':\n",
    "        return lr/(1.0+k*epoch)\n",
    "    elif learning_rate_type == 'step':\n",
    "        return lr*np.power(drop,np.floor((1+epoch)/epochs_drop))\n",
    "    elif learning_rate_type == 'exponential':\n",
    "        return lr*np.exp(-k*epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# q = np.random.choice(ratingsForUser1[:, 0])\n",
    "# index = np.where(ratingsForUser1[:, 0]==q)[0].item()\n",
    "# ratingdist1 = getPredictedDistribution(v, W[ratingsForUser1[:, 0], :, :], W[q, :, :])#[index]\n",
    "# print(ratingdist1)\n",
    "# predictRatingMax(ratingdist1)\n",
    "# Problem remains - we need the index to know which movie to take the values from in the \n",
    "# getPredictedDistribution function. Need to check with TA and clarify that part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictForUser(user, W, training, predictType=\"exp\"):\n",
    "    ### TO IMPLEMENT\n",
    "    # given a user ID, predicts all movie ratings for the user\n",
    "    ratingsForUser = lib.getRatingsForUser(user, training)\n",
    "    v = getV(ratingsForUser)\n",
    "    ratings = []\n",
    "    for i in range(np.shape(ratingsForUser)[0]):\n",
    "        ratingDistribution = getPredictedDistribution(v, W[ratingsForUser[:, 0], :, :], W[ratingsForUser[:,0][i], :, :])[i]\n",
    "#         print(ratingDistribution)\n",
    "#     return v\n",
    "#     ratingDistribution = getPredictedDistribution(v, W[ratingsForUser[:, 0], :, :], )\n",
    "        if predictType == \"max\":\n",
    "            ratings.append(predictRatingMax(ratingDistribution))\n",
    "        elif predictType == \"mean\":\n",
    "            ratings.append(predictRatingMean(ratingDistribution))\n",
    "        else:\n",
    "#             print (predictRatingExp(ratingDistribution))\n",
    "            ratings.append(predictRatingExp(ratingDistribution))\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_get(array, B):\n",
    "    ret = []\n",
    "    for i in range(int(len(array)/B)):\n",
    "        ret.append(array[i*B:i*B+B])\n",
    "    if len(array)%B != 0:\n",
    "        ret.append(array[len(array)/B:])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rbm\n",
    "import projectLib as lib\n",
    "\n",
    "training = lib.getTrainingData()\n",
    "validation = lib.getValidationData()\n",
    "# You could also try with the chapter 4 data\n",
    "# training = lib.getChapter4Data()\n",
    "\n",
    "trStats = lib.getUsefulStats(training)\n",
    "vlStats = lib.getUsefulStats(validation)\n",
    "\n",
    "K = 5\n",
    "alpha = 0.9\n",
    "\n",
    "# SET PARAMETERS HERE!!!\n",
    "# number of hidden units\n",
    "F = 5\n",
    "epochs = 30\n",
    "gradientLearningRate = 0.0001\n",
    "gradientLearningRate_v = 0.001\n",
    "gradientLearningRate_h = 0.001\n",
    "_lambda = 1\n",
    "minibatch_size = 10\n",
    "\n",
    "def main_rbm(training=training, validation=validation, trStats=trStats, vlStats=vlStats, \n",
    "             K=5, F=5, epochs=30, gradientLearningRate=0.0001,gradientLearningRate_v = 0.001,\n",
    "             gradientLearningRate_h = 0.001, minibatch_size=10, alpha=0.9, \n",
    "             stopping=False, momentum=False, learning_rate_type='time', learning_rate_k=0.1, \n",
    "             learning_rate_drop=0.5, learning_rate_epochs_drop=10.0, _lambda = 0.3):\n",
    "    \n",
    "    # Print current hyperparams\n",
    "    frame = inspect.currentframe()\n",
    "    args, _, _, values = inspect.getargvalues(frame)\n",
    "    print ('Training and Predicting with the following hyperparameters:')\n",
    "    for i in args[4:]:\n",
    "        print (\"    %s = %s\" % (i, values[i]))\n",
    "        \n",
    "    # Initialise all our arrays\n",
    "    num_movies=trStats[\"n_movies\"]\n",
    "    num_users=trStats[\"n_users\"]\n",
    "    W = getInitialWeights(trStats[\"n_movies\"], F, K)\n",
    "    posprods = np.zeros(W.shape)\n",
    "    negprods = np.zeros(W.shape)\n",
    "    grad_w = np.zeros(W.shape)\n",
    "    m_w=np.zeros((W.shape[0],F,5))\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    vis_bias=np.zeros((num_movies,5))\n",
    "    m_v=np.zeros((num_movies,5))\n",
    "    hid_bias=np.zeros((F,))\n",
    "    m_h=np.zeros((F,))\n",
    "    best_train_loss = 100\n",
    "    best_validation_loss = 100\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "    #     mini_batch_grads = []\n",
    "        # in each epoch, we'll visit all users in a random order\n",
    "        visitingOrder = np.array(trStats[\"u_users\"])\n",
    "        np.random.shuffle(visitingOrder)\n",
    "#         for i in range(0, visitingOrder.shape[0], minibatch_size):\n",
    "#                 # Get pair of (X, y) of the current minibatch/chunk\n",
    "#             visitingOrderMini = visitingOrder[i:i + minibatch_size]\n",
    "#                 y_train_mini = y_train[i:i + minibatch_size]\n",
    "#         for i in range(0, visitingOrder.shape[0], minibatch_size):\n",
    "        batches = batch_get(visitingOrder, minibatch_size)\n",
    "        for batch in batches:\n",
    "            prev_grad = grad_w\n",
    "            grad_w = np.zeros(W.shape)\n",
    "            for user in batch:\n",
    "                # get the ratings of that user\n",
    "                ratingsForUser = lib.getRatingsForUser(user, training)\n",
    "\n",
    "                # build the visible input\n",
    "                v = getV(ratingsForUser)\n",
    "\n",
    "                # get the weights associated to movies the user has seen\n",
    "                weightsForUser = W[ratingsForUser[:, 0], :, :]\n",
    "\n",
    "                ### LEARNING ###\n",
    "                # propagate visible input to hidden units\n",
    "                posHiddenProb = visibleToHiddenVec(v, weightsForUser, hid_bias)\n",
    "                # get positive gradient\n",
    "                # note that we only update the movies that this user has seen!\n",
    "                posprods[ratingsForUser[:, 0], :, :] += probProduct(v, posHiddenProb)\n",
    "\n",
    "                ### UNLEARNING ###\n",
    "                # sample from hidden distribution\n",
    "                sampledHidden = sample(posHiddenProb)\n",
    "                # propagate back to get \"negative data\"\n",
    "                negData = hiddenToVisible(sampledHidden, weightsForUser, vis_bias[ratingsForUser[:,0]])\n",
    "                # propagate negative data to hidden units\n",
    "                negHiddenProb = visibleToHiddenVec(negData, weightsForUser, hid_bias)\n",
    "                # get negative gradient\n",
    "                # note that we only update the movies that this user has seen!\n",
    "                negprods[ratingsForUser[:, 0], :, :] += probProduct(negData, negHiddenProb)\n",
    "\n",
    "                poshidact = sum(posHiddenProb)\n",
    "                posvisact = sum(v)\n",
    "                neghidact = sum(negHiddenProb)\n",
    "                negvisact = sum(negData)\n",
    "                # we average over the number of users in the batch (if we use mini-batch)\n",
    "    #             grad = (gradientLearningRate/epoch)*(posprods-negprods)\n",
    "                '''\n",
    "                Regularization - \n",
    "                '''\n",
    "                grad_w += adaptiveLearn(learning_rate_type=learning_rate_type, k=learning_rate_k, drop=learning_rate_drop, \n",
    "                                     epochs_drop=learning_rate_epochs_drop, epoch=epoch)*((posprods-negprods)-2*_lambda*W/np.linalg.norm(W))\n",
    "        #         mini_batch_grads.append(grad)\n",
    "\n",
    "            #     m = alpha*m+grad\n",
    "                '''\n",
    "                Ask about the implementation of biases (should we create matrix of biases for hidden and visible layers?)\n",
    "                '''\n",
    "            m_w = alpha*m_v + grad\n",
    "            m_v = alpha*m_v+(gradientLearningRate_v) * (posvisact - negvisact)\n",
    "            m_h = alpha*m_h+(gradientLearningRate_h) * (poshidact - neghidact)\n",
    "\n",
    "            if momentum == False:\n",
    "                W += grad_w\n",
    "            else:\n",
    "                W += m_w\n",
    "\n",
    "            vis_bias += m_v\n",
    "            hid_bias += m_h\n",
    "\n",
    "        # Print the current RMSE for training and validation sets\n",
    "        # this allows you to control for overfitting e.g\n",
    "        # We predict over the training set\n",
    "        tr_r_hat = predict(trStats[\"movies\"], trStats[\"users\"], W, training, vis_bias, hid_bias, predictType='exp')\n",
    "    #     print (tr_r_hat)\n",
    "        trRMSE = lib.rmse(trStats[\"ratings\"], tr_r_hat)\n",
    "    #     print (trRMSE)\n",
    "        if trRMSE < best_train_loss:\n",
    "            best_train_loss = trRMSE\n",
    "            best_training_weights = W\n",
    "            best_train_predictions = tr_r_hat\n",
    "\n",
    "        # We predict over the validation set\n",
    "        vl_r_hat = predict(vlStats[\"movies\"], vlStats[\"users\"], W, training, vis_bias, hid_bias, predictType='exp')\n",
    "    #     vl_r_hat\n",
    "        vlRMSE = lib.rmse(vlStats[\"ratings\"], vl_r_hat)\n",
    "        if vlRMSE < best_validation_loss:\n",
    "            best_validation_loss = vlRMSE\n",
    "            best_validation_weights = W\n",
    "            best_validation_predictions = vl_r_hat\n",
    "#             best_momentum = momentum\n",
    "#             best_reg = regularization\n",
    "#             best_epoch = epoch\n",
    "#             best_alpha = alpha\n",
    "#             best_B = B\n",
    "#             best_F = F\n",
    "#             min_rmse = vlRMSE\n",
    "#                     print('Best RMSE:', min_rmse)\n",
    "\n",
    "        train_loss.append(trRMSE)\n",
    "        validation_loss.append(vlRMSE)\n",
    "\n",
    "        print (\"### EPOCH %d ###\" % epoch)\n",
    "        print (\"Training loss = %f\" % trRMSE)\n",
    "        print (\"Validation loss = %f\" % vlRMSE)\n",
    "\n",
    "    ### END ###\n",
    "    # This part you can write on your own\n",
    "    # you could plot the evolution of the training and validation RMSEs for example\n",
    "    # predictedRatings = np.array([predictForUser(user, W, training) for user in trStats[\"u_users\"]])\n",
    "    # np.savetxt(\"predictedRatings.txt\", predictedRatings)\n",
    "    # fig1 = plt.figure()\n",
    "    # ax1 = fig1.add_subplot(121)\n",
    "    # ax1.plot(train_loss)\n",
    "    # ax2 = fig1.add_subplot(122)\n",
    "    # ax2.plot(validation_loss)\n",
    "    \n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(validation_loss)\n",
    "    plt.show()\n",
    "    if stopping==True:\n",
    "        print('Best training loss = %f' % best_train_loss)\n",
    "        print('Best validation loss = %f' % best_validation_loss)\n",
    "    else:\n",
    "        print('Final training loss = %f' % trRMSE)\n",
    "        print('Final validation loss = %f' % vlRMSE)\n",
    "    return best_alpha_validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrange = np.linspace(0.7,0.95,5)\n",
    "rrange = np.linspace(0.1,0.9,10)\n",
    "brange = np.linspace(10,50,5)\n",
    "frange = np.linspace(5,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_momentum = 0\n",
    "best_reg = 0\n",
    "best_epoch = 0\n",
    "best_B = 0\n",
    "best_F = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for momentum in mrange:\n",
    "    for regularization in rrange:\n",
    "        for batch in brange:\n",
    "            for F in frange:\n",
    "                min_rmse=10\n",
    "                vlRMSE = main_rbm(training=training, validation=validation, trStats=trStats, vlStats=vlStats, \n",
    "                             K=5, F=F, epochs=30, gradientLearningRate=0.0001,gradientLearningRate_v = 0.001,\n",
    "                             gradientLearningRate_h = 0.001, minibatch_size=batch, alpha=momentum, \n",
    "                             stopping=True, momentum=True, learning_rate_type='adapt', learning_rate_k=0.1, \n",
    "                             learning_rate_drop=0.5, learning_rate_epochs_drop=10.0, _lambda = regularization)\n",
    "                if vlRMSE < min_rmse:\n",
    "                        best_momentum = momentum\n",
    "                        best_reg = regularization\n",
    "                        best_epoch = epoch\n",
    "                        best_alpha = alpha\n",
    "                        best_B = B\n",
    "                        best_F = F\n",
    "                        min_rmse = vlRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Predicting with the following hyperparameters:\n",
      "    K = 5\n",
      "    F = 5\n",
      "    epochs = 30\n",
      "    gradientLearningRate = 0.0001\n",
      "    gradientLearningRate_v = 0.001\n",
      "    gradientLearningRate_h = 0.001\n",
      "    minibatch_size = 10\n",
      "    alpha = 0.9\n",
      "    stopping = False\n",
      "    momentum = False\n",
      "    learning_rate_type = time\n",
      "    learning_rate_k = 0.1\n",
      "    learning_rate_drop = 0.5\n",
      "    learning_rate_epochs_drop = 10.0\n",
      "    _lambda = 0.3\n",
      "### EPOCH 1 ###\n",
      "Training loss = 1.211313\n",
      "Validation loss = 1.234726\n",
      "### EPOCH 2 ###\n",
      "Training loss = 1.210378\n",
      "Validation loss = 1.233583\n",
      "### EPOCH 3 ###\n",
      "Training loss = 1.210287\n",
      "Validation loss = 1.233736\n",
      "### EPOCH 4 ###\n",
      "Training loss = 1.210319\n",
      "Validation loss = 1.233667\n",
      "### EPOCH 5 ###\n",
      "Training loss = 1.209675\n",
      "Validation loss = 1.232671\n",
      "### EPOCH 6 ###\n",
      "Training loss = 1.209158\n",
      "Validation loss = 1.232595\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-07b2e627572f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain_rbm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-442d90d95881>\u001b[0m in \u001b[0;36mmain_rbm\u001b[1;34m(training, validation, trStats, vlStats, K, F, epochs, gradientLearningRate, gradientLearningRate_v, gradientLearningRate_h, minibatch_size, alpha, stopping, momentum, learning_rate_type, learning_rate_k, learning_rate_drop, learning_rate_epochs_drop, _lambda)\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[1;31m# get negative gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[1;31m# note that we only update the movies that this user has seen!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m                 \u001b[0mnegprods\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mratingsForUser\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mprobProduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnegData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegHiddenProb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[0mposhidact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposHiddenProb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main_rbm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(momentum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(learning_rate_type='time', learning_rate_k=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(learning_rate_type='step', learning_rate_drop=0.5, learning_rate_epochs_drop=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(learning_rate_type='exponential', learning_rate_k=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Predicting with the following hyperparameters:\n",
      "    K = 5\n",
      "    F = 8\n",
      "    epochs = 30\n",
      "    gradientLearningRate = 0.0001\n",
      "    gradientLearningRate_v = 0.001\n",
      "    gradientLearningRate_h = 0.001\n",
      "    minibatch_size = 10\n",
      "    alpha = 0.9\n",
      "    stopping = True\n",
      "    momentum = True\n",
      "    learning_rate_type = time\n",
      "    learning_rate_k = 0.5\n",
      "    learning_rate_drop = 0.5\n",
      "    learning_rate_epochs_drop = 10.0\n",
      "    _lambda = 0.3\n",
      "### EPOCH 1 ###\n",
      "Training loss = 1.209519\n",
      "Validation loss = 1.233369\n",
      "### EPOCH 2 ###\n",
      "Training loss = 1.209034\n",
      "Validation loss = 1.232448\n",
      "### EPOCH 3 ###\n",
      "Training loss = 1.210040\n",
      "Validation loss = 1.233395\n",
      "### EPOCH 4 ###\n",
      "Training loss = 1.209857\n",
      "Validation loss = 1.233978\n",
      "### EPOCH 5 ###\n",
      "Training loss = 1.209707\n",
      "Validation loss = 1.234003\n",
      "### EPOCH 6 ###\n",
      "Training loss = 1.209413\n",
      "Validation loss = 1.232723\n",
      "### EPOCH 7 ###\n",
      "Training loss = 1.209471\n",
      "Validation loss = 1.232981\n",
      "### EPOCH 8 ###\n",
      "Training loss = 1.209625\n",
      "Validation loss = 1.233306\n",
      "### EPOCH 9 ###\n",
      "Training loss = 1.210113\n",
      "Validation loss = 1.233525\n",
      "### EPOCH 10 ###\n",
      "Training loss = 1.210560\n",
      "Validation loss = 1.235371\n",
      "### EPOCH 11 ###\n",
      "Training loss = 1.208998\n",
      "Validation loss = 1.233030\n",
      "### EPOCH 12 ###\n",
      "Training loss = 1.208525\n",
      "Validation loss = 1.232405\n",
      "### EPOCH 13 ###\n",
      "Training loss = 1.208289\n",
      "Validation loss = 1.232800\n",
      "### EPOCH 14 ###\n",
      "Training loss = 1.208478\n",
      "Validation loss = 1.232893\n",
      "### EPOCH 15 ###\n",
      "Training loss = 1.209221\n",
      "Validation loss = 1.234561\n",
      "### EPOCH 16 ###\n",
      "Training loss = 1.208161\n",
      "Validation loss = 1.232699\n",
      "### EPOCH 17 ###\n",
      "Training loss = 1.207374\n",
      "Validation loss = 1.232311\n",
      "### EPOCH 18 ###\n",
      "Training loss = 1.205711\n",
      "Validation loss = 1.230620\n",
      "### EPOCH 19 ###\n",
      "Training loss = 1.205371\n",
      "Validation loss = 1.229125\n",
      "### EPOCH 20 ###\n",
      "Training loss = 1.205423\n",
      "Validation loss = 1.229055\n",
      "### EPOCH 21 ###\n",
      "Training loss = 1.204007\n",
      "Validation loss = 1.228642\n",
      "### EPOCH 22 ###\n",
      "Training loss = 1.204166\n",
      "Validation loss = 1.228301\n",
      "### EPOCH 23 ###\n",
      "Training loss = 1.203851\n",
      "Validation loss = 1.228374\n",
      "### EPOCH 24 ###\n",
      "Training loss = 1.203446\n",
      "Validation loss = 1.229466\n",
      "### EPOCH 25 ###\n",
      "Training loss = 1.203416\n",
      "Validation loss = 1.228770\n",
      "### EPOCH 26 ###\n",
      "Training loss = 1.202805\n",
      "Validation loss = 1.227657\n",
      "### EPOCH 27 ###\n",
      "Training loss = 1.202464\n",
      "Validation loss = 1.228168\n",
      "### EPOCH 28 ###\n",
      "Training loss = 1.201739\n",
      "Validation loss = 1.226921\n",
      "### EPOCH 29 ###\n",
      "Training loss = 1.202672\n",
      "Validation loss = 1.228367\n",
      "### EPOCH 30 ###\n",
      "Training loss = 1.202942\n",
      "Validation loss = 1.229268\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8VdW99/HPL3MghCEJk0wioCACakSc5xanola9WkvtLU5t9Xa6rb32Fu3VPrb2tnZ46oCKU6lDlVbro3WqFhVBAyKCICBjmDIwJYGM5/f8sTYQETIeOEn4vl+v8zrn7OFkLY/s79lrrb22uTsiIiJJiS6AiIi0DQoEEREBFAgiIhJRIIiICKBAEBGRiAJBREQABYKIiEQUCCIiAigQREQkkpLoAjRHbm6uDxo0KNHFEBFpV+bMmVPi7nmNbdeuAmHQoEEUFBQkuhgiIu2Kma1qynZqMhIREUCBICIiEQWCiIgACgQREYkoEEREBFAgiIhIRIEgIiJAEwLBzKaaWZGZLdjH+qvMbH70mGlmo6PlGWb2npl9aGYLzexn9fZ5xMxWmNm86DEmflWSNqOuBuY+BhWliS6JiDRBU84QHgHGN7B+BXCau48CbgemRMurgDPdfTQwBhhvZuPq7fdDdx8TPeY1v+jSptXVwvRr4fmb4OmJ4b2ItGmNBoK7zwA2NbB+prtvjt7OAvpFy93dy6PlqdHDW1dcaRdiMXju27Dwr3DEBbDqHXjjjkSXSkQaEe8+hEnASzvfmFmymc0DioBX3X12vW1/HjUz3W1m6fv6QDO7zswKzKyguLg4zsWVuHOHF74L85+EM/4brpgGx34d3r4bPvlHoksnIg2IWyCY2RmEQLh55zJ3r3P3MYSzhrFmNjJa9V/AEcBxQI/6++zJ3ae4e7675+flNTo3kySSO7x0M8x9FE75Tzjth2H5+F9C71Hw1+thc5OmVBGRBIhLIJjZKOBBYIK7f64H0d23AG8S9UW4+/qoSakKeBgYG49ySAK5w6uT4b374YQb4cz/3r0uNQMufzRs85erobYqceVsrfcegFd+CrG6RJdEJO5aHQhmNgCYDkx09yX1lueZWbfodSZwNrA4et8nejbgImCvI5ikHXnzTpj5ezjuGvjCHWD22fU9BsNFf4R1H8DLP0lMGVvDHV67DV78z1DP5/8j9JWIdCCNTn9tZk8ApwO5ZlYI3EroIMbd7wMmAznAPeH4Tq275wN9gEfNLJkQPE+7+wvRx04zszzAgHnADfGslBxgM/4X/vVLOHoinPurz4fBTsMvDGcP7/5fGDAOjrr0wJazpWJ1IQgKpkL+N6BTDsz4FaRmwnkN1FeknWk0ENz9ykbWXwNcs5fl84Gj97HPmU0tYLtXsyOMsqncGn5RxmrB68JzrC486r/vMTgcONvLQebdP8I/b4dR/wYX/g6SGjnpPPs2KCwIv7B7j4K8YQeilC1XVxP6PhY8Cyd/H86aHJbX7AjBlpoJ5/xP+/m+RBrQrm6Q025UlcOyV+Hj52DJK1BT0bz9h18IF/4eOvXYP+WLl/cegJdvgREXwYR7ICm58X2SU+HSqXD/KfD01+Da1yGt8/4va0tUbw99HktfgbN/Bid/d/e6L9wRQmHm70P5T/9x4sopEicKhHip3AZLXoaP/wbLXoPaSuicB6P/LYzFzz4kHDCTksGSISkletRbZkmhWeL1/4HCk+CSKXDoKYmu2d7NfTw0oxx+Hnz5QUhuxv9KXQ8J+zx+Cbzwfbj4vrb3C7tyK/z5Clj9LlzwW8j/98+uN4Pz/jd8z2/eGc4UTvpOYsoqEicHRyC4758Dzo7N8MlL4Uzg039CXTV06QPHXA0jvgQDTmjar+b6TvqPEALPXgOPXggnfw/OuCX8sk6EWAzKN8LWNbBldXhs+hQ+mAZDzobLHmlZ2Q47M/yqfvNOGHhCuFahragogT9dAhsXwqUPwcgv7327pCT40h/CmcKrkyG1E4y99sCWVSSODo5AeP9BWPl2OLDmHd76z9u0HN64ExZOD23/XfvDcdfCiAnQ77jG29Eb0/douH4G/OPH8PZvYPmb4Rd1zmGtL/ve1FZDyRIo+jhcJ7Bl1e4A2FoYgq6+zB5w5EVw0b2Qss9rCht36g9h9Sx48Uehzn1Gt64e8bC1EB6/GLasgSufhKHnNLx9UnI4k6utDGdMqZlw9FcPTFlF4szc289sEvn5+V5QUND8Hd+9B974OdRsh6Muh9NvDp23zVW2Af51V7jwKik1NCMcdSn0PWb/NXl8/Fw0xLE2jGgZfWXr/lZ5MWz8CDYsCL+ANy6A4k8gVrN7m849odsA6NY/PHeNnne+Ts9qfb12qiiB+04JwXLdm5DZLX6f3Vyln8JjE0Jz0VeeDmcuTVVTCU9eCZ++EcK7vYygkoOCmc2JRn82vN1BEQgQZtx857ehI7SuOvyKO/WH4aDXmB1b4J3fwax7w4HzmKvhtB9Bl94tK0tzbS2E6dfDqrfhyEvggrsbPnC6Q0VxOMBt+jQc8DcuCCFQUbR7uy59oNdI6HUk9D4qPHcfFH7lHkirZ8HD54WzqxNvCr/KW3Pm0RIbPgpnBu4wcXrLzlaqt8O0S0N9Ln8Mhl8Q/3KKtIACYV/KNsBbv4E5D4f3x/47nPL9vR/cq7fDe1PCPDyVW+Coy0KzU0vOLlorVhfK8ead4UB+yRTIHRaar3Ye+Hc9L4fqst37JqdB3hG7D/q9RoZH55wDX499+eBP8OqtsL0E0rvCiAth5KVw6KnN74eB0L9TOAc2r4CqbWHkV3U5VJWFx67X0fLyIsjqBV/7G+QObXk9qsrgsYtgw3y48onQzyKSYAqExmxZEy4u+uBP4YA59lo46bvhIFlXE5b/65dQth6GfhHO+mk4oCZa4Rx4dlI40NVnyaFJp8fg0NfQ47DoeTB0G9i8UUCJUlcLK/4FHz0Di/4eQi2rVzgrOupSOOTYvTeXxeqgeDGseQ8K3w+PkiWf3SYpBdK7QFqX0OSVlhXep2eFZZ26w/E3QNd+ra/Hjs1hQEDJUjjyYuiXH85+eh7ZPr4H6XAUCE1V+mnoF5j/VBhPPuaqcA3BpuXQfxycfSsMPDG+f7O1qspCR3ly+u6Df7cBkJKW6JLFT82OMIx3wTPhua46NGeNvDQ0xZQX7Q6AtXN3nxF1yoF+Y8NBuP9YyBseDvwp6Qd2aGtFSehkXvl2aL4DSMkMnef98neHRHbfA1cmOWgpEJqraHFojvn4b+GX3FmTYdgX2974+IPRji2w+IVw5rDiX+DRHEKWHJrA+o/dHQI9Bret78w9jNoqLIge74fmpJ0jt7IPCWc+R14MIy9JbFmlw1IgtNT2TZDRtWXt1rL/lW0M13x06x9+bbfVq5wbUlsVOrEL3w8hsWZ2GOZ71mQ45QeJLp10QE0NBDVo7qmtTxdxsOvSC8Y0OL1W25eSvrvZCEKf1d++Fa5QryqDs25tW2c5ctBQIIgkWnIqXHx/ONt5++4w8uncu1p/gaNIMykQRNqCpKRwfUl6Fsz8A1RXhGkxNCpJDiD93ybSVpjBObdDena4sr66PFz1fKAv0pODls5JRdoSs3AV/BfvhEXPw5NfCRdIihwACgSRtuiEb4Umo2Wvh+kwKrclukRyEFAgiLRVx3wtTL+9ZjY89qUwJFpkP1IgiLRlI78M/zYNNn4cJgAs25DoEkkHpkAQaesOHw9X/SXcn+Lhc2HTisb3EWmBJgWCmU01syIzW7CP9VeZ2fzoMdPMRkfLM8zsPTP70MwWmtnP6u1zqJnNNrOlZvaUmXWgiXhE4mzwafC150Kz0YNnwaqZiS6RdEBNPUN4BBjfwPoVwGnuPgq4HZgSLa8CznT30cAYYLyZjYvW/RK4292HApuBSc0su8jBpf9xcO0/IbM7PPqlcBtTkThqUiC4+wxgnz1a7j7T3TdHb2cB/aLl7u7l0fLU6OFmZsCZwDPRukeBi5pffJGDTM5hcM1rMOgkeO5b8MpPw/TfInGwP/oQJgEv7XxjZslmNg8oAl5199lADrDF3WujzQqBQ/ZDWUQ6nszucNUzcNw1MPP38NRXw3QXIq0U10AwszMIgXDzzmXuXufuYwhnDWPNbCSwt5m79jrtqpldZ2YFZlZQXFwcz+KKtF/JqXD+r+HcX8GSf8DUL4abPom0QtwCwcxGAQ8CE9y9dM/17r4FeJPQF1ECdDOznVNn9APW7e1z3X2Ku+e7e35eXl68iivSMRx/XThb2LIGHjgD1ryf6BJJOxaXQDCzAcB0YKK7L6m3PM/MukWvM4GzgcUebsLwBnBptOnVwHPxKIvIQWfIWXDNq+G2oI+cD/P/kugSxd/S12Da5eHuhlvXJro0HVZTh50+AbwLHG5mhWY2ycxuMLMbok0mE/oF7jGzeWa28y42fYA3zGw+8D6hD+GFaN3NwPfNbFm070NxqpPIwSfv8DACqd9xMP0a+OcdEIslulStt3UtPDURpn0Z1s0Nk/79diT86VL4+HmorU50Cfe/wjnwxJUH5KJE3TFNpCOprYb/93344HHIOwI650FqJ0jNrPe883VGeE7Lgt5HQa+RbWe67boamH0fvHEneB2c+kM48SbYtg7mTQtDbsvWhfqNvgKO/hrkDUt0qeNr7dxwW9+lr0BmD7h0Khx2Ros+SrfQFDlYuUPB1HAf6ppKqNkONTuix/bdz3uO40jtHO7iNmBcePQ7DtK7HPjyr54FL3wfihbCsPFw7i+h+6DPbhOrCxP/zX00dKrHamHACXD0RDjyovZ5a9Wd1n0Ab/4i1CuzewjCsde16rtQIIjIvrmHezvXbIfKLeHX6OpZsGYWbFwIHgNLgl5HQv9xu0Oia7/9V6aKUnhtMnzwJ8juF4LgiPMbv51oeRF8+ATMfQxKl0FaFzj2ajjtZsjI3n/ljbf1H4Yg+ORFyOgGJ94IY6+PSx0UCCLSMpXboPD9MMvq6llQWAA1FWFdlz7QexT0GbX7udvA1t0DOhYLTVyv3RruKX3Ct8PBvLm/8t1Deec8DPOfhi69w61Ih1/Ytu9RveGjEASLX4CMrnDCjXD89eF1nCgQRCQ+6mph40ewenbo2F0/H0o+CWcREA5cvUeFfoidIZF7OCQlQ21luMHPrqaqit1NVtXbw61CC6ZC4Xsw8KRwbUXP4a0vc+EceOE74WA7bDyc9yvoNqD1nxtPJUvh9f8JN0JKzw5BePwNkNkt7n9KgSAi+0/NjjAl94YPQ0BsmB+ammorw3pLjgKjCceXTrnwhTtC53A8f8nX1UYd0z8P78+4BY7/Zss6ziu3QlJK/PomPn4e/npDaJYb981wQ6TM7vH57L1QIIjIgVVXC6VLd59BWHIY0ZTWud7Ipk6Q1mn369RMyO4bnveXLavhxR/Bkpeg11Fw4W9D53lD3EPILX0lXANR+F44E7rgt6HTuqVisTByaMZdodP+8schu0/LP6+JFAgiIju5hzb6F38EZevhuElw1uTPttPv2ALL34Clr8Ky16B8Y1jeZwwMORs+fT2MABp9Zejwbm4bf+U2+Ov1odP46K/C+b+BlPT41bEBCgQRkT1VlcE/fw7v3R+uYTjzv6GiOITAmvfCNQ8ZXeGws2DoOeG5S6+wb11NuFL6rf8No6Auvi/MOtsUpZ+Gi8tKl8H4X8DYaw9oR7cCQURkX9bOhRe+G4Z6QugMH3oODP0CHJLfcD/Dmvdh+rWweSWc9B9wxk8a/qW/9DV45hvhMy97FA49Ja5VaQoFgohIQ+pqYfW7kDs0DFFtjqpyePmWcGFcr6Pgyw98fnSUO7zzO3jttnAV+BXToPvAuBW/OZoaCLqnsogcnJJTwq/15oYBQHoWfOn3cMUToU/i/tPg3Xt2zx9VvR2evSZcW3HkRTDp5YSFQXO0kYlLRETaoSPOC6OFnr8JXv6vMN3EGT+BF/8zXANx1mQ4+ftt+8K4ehQIIiKtkZUHVz4Rmo/+cQtM/UK40OwrT8GwLya6dM2iQBARaS0zOPbrMOgUmH1/uL1pO5x9VYEgIhIvOYfBeXcluhQtpk5lEREBFAgiIhJRIIiICKBAEBGRiAJBREQABYKIiEQaDQQzm2pmRWa2YB/rrzKz+dFjppmNjpb3N7M3zGyRmS00s+/U2+c2M1trZvOix3nxq5KIiLREU84QHgHGN7B+BXCau48CbgemRMtrgR+4+3BgHPBtMxtRb7+73X1M9Hix+UUXEZF4ajQQ3H0GsKmB9TPdfXP0dhbQL1q+3t3nRq/LgEXAIa0usYiI7Bfx7kOYBLy050IzGwQcDcyut/jGqJlpqpnt82aiZnadmRWYWUFxcXGciysiIjvFLRDM7AxCINy8x/Is4Fngu+6+LVp8L3AYMAZYD/x6X5/r7lPcPd/d8/Py8uJVXBER2UNcAsHMRgEPAhPcvbTe8lRCGExz9+k7l7v7Rnevc/cY8AAwNh7lEBGRlmt1IJjZAGA6MNHdl9RbbsBDwCJ3/80e+/Sp9/ZiYK8jmERE5MBpdLZTM3sCOB3INbNC4FYgFcDd7wMmAznAPSEDqI1u1XYSMBH4yMzmRR93SzSi6C4zGwM4sBK4Po51EhGRFtA9lUVEOjjdU1lERJpFgSAiIoACQUREIgoEEREBFAgiIhJRIIiICKBAEBGRiAJBREQABYKIiEQUCCIiAigQREQkokAQERFAgSAiIhEFgoiIAAoEERGJKBBERARQIIiISESBICIigAJBREQiCgQREQGaEAhmNtXMisxswT7WX2Vm86PHTDMbHS3vb2ZvmNkiM1toZt+pt08PM3vVzJZGz93jVyUREWmJppwhPAKMb2D9CuA0dx8F3A5MiZbXAj9w9+HAOODbZjYiWvdj4HV3Hwq8Hr0XEZEEajQQ3H0GsKmB9TPdfXP0dhbQL1q+3t3nRq/LgEXAIdF2E4BHo9ePAhe1qPQiIhI38e5DmAS8tOdCMxsEHA3Mjhb1cvf1EIID6BnncoiISDOlxOuDzOwMQiCcvMfyLOBZ4Lvuvq0Fn3sdcB3AgAED4lBSERHZm7icIZjZKOBBYIK7l9ZbnkoIg2nuPr3eLhvNrE+0TR+gaF+f7e5T3D3f3fPz8vLiUVwREdmLVgeCmQ0ApgMT3X1JveUGPAQscvff7LHb88DV0eurgedaWw4REWmdRpuMzOwJ4HQg18wKgVuBVAB3vw+YDOQA94QMoNbd84GTgInAR2Y2L/q4W9z9ReAXwNNmNglYDVwWz0qJiEjzmbsnugxNlp+f7wUFBYkuhohIu2Jmc6If6g3SlcoiIgIoEEREJKJAEBERQIEgIiIRBYKIiAAKBBERiSgQREQEUCCIiEhEgSAiIoACQUREIgoEEREBFAgiIhJRIIiICKBAEBGRiAJBREQABYKIiEQUCCIiAigQREQkokAQERFAgSAiIpFGA8HMpppZkZkt2Mf6q8xsfvSYaWajG9vXzG4zs7VmNi96nNf6qoiISGs05QzhEWB8A+tXAKe5+yjgdmBKE/e9293HRI8Xm1AOERHZjxoNBHefAWxqYP1Md98cvZ0F9GvqviIi0nbEuw9hEvBSE7e9MWpmmmpm3eNcDhERaaa4BYKZnUEIhJubsPm9wGHAGGA98OsGPvc6Mysws4Li4uK4lFVERD4vLoFgZqOAB4EJ7l7a2PbuvtHd69w9BjwAjG1g2ynunu/u+Xl5efEoroiI7EWrA8HMBgDTgYnuvqSJ+/Sp9/ZiYK8jmERE5MBJaWwDM3sCOB3INbNC4FYgFcDd7wMmAznAPWYGUOvu+fva190fAu4yszGAAyuB6+NaKxERaTZz90SXocny8/O9oKAg0cUQEWlXzGzOzh/qDdGVyiIiAigQREQkokAQERFAgSAiIhEFgoiIAAoEERGJKBBERARQIIiISESBICIigAJBREQiCgQREQEUCCIiElEgiIgIoEAQEZGIAkFERAAFgoiIRBQIIiICKBBERCSiQBAREUCBICIikZREF0A6nljMWVpUzqzlpSzeUMaVY/szql+3RBdLRBrRaCCY2VTgAqDI3UfuZf1VwM3R23Lgm+7+YUP7mlkP4ClgELASuNzdN7eqJpIw9QNg1vJSZq/YxKaKagDSkpP42wdr+cOVR3P2iF4JLqmINMTcveENzE4lHOgf20cgnAgscvfNZnYucJu7H9/QvmZ2F7DJ3X9hZj8Gurv7zXt+9p7y8/O9oKCgGdWT/aGhADikWybjBucwbnAPxg3OIT01iUmPFLBw3VZ+9qUjmXjCoMQWXuQgZGZz3D2/se0aPUNw9xlmNqiB9TPrvZ0F9GvCvhOA06PXjwJvsvsso82LxZznPlzL3FVbyOuSTq/sdHpmZ9CrSwa9u2bQvVMqZpboYsZV0bZK3lpawltLi3l7WQkl5bsD4IzDe+4KgP49On1u36euH8dNf/6Anz63kMLNO7h5/BEkJXWs/z4iHUG8+xAmAS81Ybte7r4ewN3Xm1nPfW1oZtcB1wEMGDAgLoVsjVnLS7nj/33MgrXbyEpPobyq9nPbpCUn7QqKXtkZ9MrOIDcrjR6d08nJSiOncxo5Wen06JxGdkZKmwyPypo63l+5ibeWljBjSTGLN5QBkNM5jZOH5nLSkFxO2EcA7KlTWgr3TzyW2/6+kPtnLKdwyw5+fdloMlKT93c1RKQZ4hYIZnYGIRBOjtdnArj7FGAKhCajeH52c6wsqeDOlxbx8sKN9Omawd3/NpoJow+hJhajuKyKjduqKNpWycZtlWzY+bqskqVF5by9rISyys8HB0BqstGjcxo5UVj0zs5gSM8shvXqwpCeWRzSLXO//5qurYuxeXsNG7ZWMmt5KTOWFvPeik1U1cZIS04if1B3bh5/BKcMzWVEn+wWlSclOYnbJ4ykf/dO3PnSYoq2VfLA1/Lp1iltP9RIRFoiLoFgZqOAB4Fz3b20CbtsNLM+0dlBH6AoHuXYl5c+Ws+iDWWcOjSX0f27kZrc9NG2W7fX8Pt/LuWxd1eSmpzED84ZxjWnDCYzLfy6TU9Kpl/3TvTr3vAv5araOjZVVFNaXk1pRTWbKqp2vS4tr2JTRTUl5dUs3lDMX+YU7tovMzWZIT2zGNoziyG9shjaswtDe2bRv0cnkpOMWMyprotRVROjqraOyui5qnb3+7LKWkp3/r3yKkqiv7nz72/eXk39rqShPbO46viBnDIsl+MP7UGntPj8bjAzrj/tMPp2y+QHT3/IJffO5JGvj2VATuNnGSKy/7X6X7qZDQCmAxPdfUkTd3seuBr4RfT8XGvL0ZA5qzYz9Z0V/P71pWSlp3DCYTmcMjSXU4bmMSin016bbGrqYkybtYrfvr6UrTtquPzY/vzgC8PomZ3RojKkpyTTp2smfbpmNrrt1u01LC0qY2lROUs3lrO0qIx3l5cy/YO1u7ZJTTbMjOraWLPK0TUzlZysNHI7pzOkZxbHZ4Wzk9ysNHKz0hkzoFuTytgaF47uS6/sDK59rIBL7n2Hh64+jtH9NSxVJNGaMsroCUIHcC6wEbgVSAVw9/vM7EHgy8CqaJfanb3Ze9vX3R8ysxzgaWAAsBq4zN03NVbY1owy2rK9mpmflu7qGC3cvAMInaKnDsvl5CF5nDQkh66Zqby+qIj/8+IilpdUcOJhOfz3+SMY0Te7RX83nrZV1vBpUTlLi8pZXlwBQEZqEukpyaSnJJGemkRGSjLp9ZZlpCbTKS2ZvC7pdO+URlpK27kWcVlROf/+yHsUl1XxhyuP4Zw2Pix13potTJu1ipq6GN87ZxgDczonukgiTdLUUUaNBkJbEq9hp+7OqtLtvLW0mLeWlvDup6WUVdViBv26Z7Jm0w4G53bmlvOGc9bwnm2y07ejKC6rYtKj77Ng7VZuOnMoF4zqw5CeWW3mv3llTR3Pf7iOP81axfzCrXSOmgprYs71pw7mW6cP2dV8KNJWKRCaobYuxoeFW5ixpIR5a7Zw+uF5fHXcwGb1NUjLba+u5XtPzePlhRsB6NklnZOHhJFMJw3JpXfXljXTtcbKkgqmzV7F0wWFbN1Rw9CeWXzthIFcfEw/Kqpq+cVLi/nrB2s5pFsmP71gOF88snebCTGRPSkQpN1Zs2k77ywr4e1lJcz8tHTXxW5DemZx0mE5nDQkl3GH5ZCdkbpf/n5dzHljcRGPz1rFv5YUk5JkfPHI3kw8YSDHH9rjcwf891ZsYvJzC1i8oYxThuZy64VHMqRn1n4pm0hrKBCkXYvFnEUbtvHOshLeWVbK7BWlVNbESDIY1a8bw3plMaBHJ/r36MSA6NGjc1qTf6VX18bYuGuYcCXLisr5S0Eha7fsoFd2Ol8ZO5ArxvanVyODCGrrYkybvZpfv/IJ26vrmHTyodx01lCy0jVNmLQdCgTpUKpq6/hg9RbeWVbC7OWbWFFaQXFZ1We26ZyW/JmA6N+jE5lpyWzcWsn6bZVs3BoO/hu3Ve660rq+EwbnMPGEgZwzolezmwtLyqv41T8+4amCNfTsks5Pzh/Ol0b3VTOStAkKBOnwtlfXUrh5B6tLt7N6U3gUbt79urJm95DcHp3T6JWdQe/sdHp3DVeP94mee3fNoE92Jl07tb4pat6aLUx+bgHzC7cydlAPvnvOUE4YnKNgkIRSIMhBzd0pLq+iqiZGz+x00lMO3EigWMx5umANv3r5E0orqhnRJ5tvnHwoF47uc0DLIbKTAkEkwSpr6vjbB2uZ+s4KlmwsJzcrnYnjBnLVuAHkZqUnunhyEFEgiLQR7s7by0p46O0VvPlJMWkpSVw0pi/fOPlQjuid+AsepeOL2/TXItI6ZsYpQ/M4ZWgey4rKePidlTw7t5CnCwo5eUguk04+lNOG5WlKcEk4nSGIJMDmimr+/N5qHnt3JRu3VTG6fzemXp1PjpqSZD9o6hmCLsUVSYDundP49hlDePvmM7nr0lEsXr+Ny+5/l3VbdiS6aHIQUyCIJFBqchKX5/fn8UnHU7ytikvvncny4vJEF0sOUgoEkTZg7KE9eOK6cVTVxrjsvndZsHZrooskByEFgkgbMfKQrvzlhhNIT0niyimzeG9FozPCi8SVAkGkDRmcl8Uz3zyRvOx0vjZ1Nm8s3q83ExQuCN5SAAAMsElEQVT5DAWCSBvTt1smT19/AoflZXHtYwU8/+G6RBdJDhIKBJE2KDcrnSeuG8cxA7vznSc/YNrsVY3vJNJKCgSRNio7I5XHvjGWMw7vyU/+uoB73lyW6CJJB6dAEGnDMlKTuX/isUwY05e7/vEJd764iMqaukQXSzooTV0h0salJidx9+Vj6JKRwv0zlvPAW8s5LC+LI/tmc2Tfrozom82RfbPp1ikt0UWVdq7RQDCzqcAFQJG7j9zL+quAm6O35cA33f3DaN144HdAMvCgu/8iWv4IcBqwc7D11919XuuqItJxJSUZt08YyVnDe/HBqs0sXLeNWcs38bd5uzucD+mWyfA+2VFQZNOnaybVdXVU1caoqo1R/Znnul3vkwy+fEw/TZshjc9lZGanEg70j+0jEE4EFrn7ZjM7F7jN3Y83s2RgCXAOUAi8D1zp7h9HgfCCuz/TnMJqLiORzyotr+Lj9dtYuG7nYysrSipo7hRlfbtmcO9Xj2V0/277p6CSUHGb7dTdZ5jZoAbWz6z3dhbQL3o9Fljm7sujAj0JTAA+buxvikjT5GSl75pJdaeKqloWb9hGaXk16anJpCUnkZ6aRFpyEhmpSaQlJ+96n56axLKicr75p7lcdv+73DFhJJcf1z+BNZJEincfwiTgpej1IcCaeusKgePrvf+5mU0GXgd+7O6fvUGuiLRI5/QUjh3Yo8nbj+rXjb/fdDL/8cQH/OjZ+cxfu4XJFxxJWorGnBxs4vaNm9kZhEDY2Z+wt8ndd57I/hdwBHAc0KPePnv73OvMrMDMCoqLi+NVXBGpp0fnNB759+O4/rTB/GnWaq58YBYbt1UmulhygMUlEMxsFPAgMMHdS6PFhUD9c89+wDoAd1/vQRXwMKF5aa/cfYq757t7fl5e3r42E5FWSklO4r/OHc4fv3IMi9Zv44I/vE3BSs2ndDBpdSCY2QBgOjDR3ZfUW/U+MNTMDjWzNOAK4Plonz7RswEXAQtaWw4RiY/zR/Xhr986ic5pyVwxZRaPv7uS9nQjLWm5RgPBzJ4A3gUON7NCM5tkZjeY2Q3RJpOBHOAeM5tnZgUA7l4L3Ai8DCwCnnb3hdE+08zsI+AjIBe4I661EpFWObx3F5678WROHZbHT59byA+fmd/kC+Jq6mJsqqgmFlOItDe6haaI7FMs5vzu9aX87vWlHHVIV245bzjbq2spKa+ipLya4rKq6HUVpeXVlJRXsXl7DQCd05IZ1rsLR/TO5ojeXTi8dxeO6N1FF9AlQFOHnSoQRKRRr328ke89NY+yqtrPLO+SnkJOVhq5Wenh0SW87pKRyurSChZtKOOTDWVs3VGza59e2ekc3jub4VFIHNm3K0N7ZpGUtLdxKBIPcbsOQUTk7BG9eOX7p/Lxum3kZKWTG4VARmpyo/u6Oxu3VbF4wzY+iQJi8YYyHv60lOq6GABdMlI4ekB38gd259iB3RndvxtZ6To8HWg6QxCRhKiti7GytIL5hVspWLWZuas288nGMtwhyeCI3tkcO7A7+YO6c8yA7vTrnkkYhyLNpSYjEWl3tlXWMG/1Fuas2szc1Zv5YPUWyqNmqtysNLpkpJKcZKQkGSnJRnJSEqlJFpYlGylJSaQkGf26Z3L+qL7kD+yupigUCCLSAdTFnE82lDFn9WYWFG5lR00ddTGnNhajts6pjTl1MaemLhYtD+uWFZVTWROjd3YG54/qw4Wj+zK6X9eD9gxDfQgi0u4lJxkj+mYzom92s/arqKrltUUbeWH+eh5/dxUPvb2C/j0yuWBUXy4Y1YcRfbLbXTi4+34vs84QRKRD27qjhlc/3sjfP1zH28tKqIs5g/M6c2EUDkN6ZrWZcKiujbF603ZWlFSwvLic5cUVLC8pZ0VJBfdcdSxjD236HFX1qclIRGQPmyqq+ceCDfz9w3XMWlGKO3TNTGXEzvtIHBJuOjQ4tzMpyft3cr8VJRW8+2lpOPCXVLCipILVm7ZTV++CvtysNAbnZjE4rzNXnziI4X2ad6a0kwJBRKQBRdsqeX1xER+t3crCddtYvH4bVbVhGGx6ShJH9O7CiL5dd91waHif7CYNs21M4ebt/O61pTw7t5CYh791aG5nBud13nXwH5yXxaG5nemamdrqvwcKBBGRZqmti7G8pIKF67aycO3uGw5tqwyjnLIzUrjkmH5cdfwAhvbq0uzPLyqr5J43PmXa7FUYxlfHDeTqEwfSv3un/T4SSoEgItJK7k7h5h0sXLeVFz/awEsL1lNT54w9tAdXHT+A8SN7k57S8FnDlu3V3D9jOY+8s5LquhiX5/fjpjOH0rdb5gGqhQJBRCTuSsqreGZOIX+evZrVm7bTo3Mal+X34ytjBzAwp/Nntq2oquXhd1Zw/4zllFXW8qXRffneOcM4NLfzPj59/1EgiIjsJ7GY8/ayEqbNXsVri4qoizmnDM3lquMHcsrQXJ56fw1/fGMZpRXVnD28Fz/4wrAWdwjHgwJBROQA2LitkqfeX8MT761m/dZKkpOMuphz4mE5/OcXD+eYAd0TXUQFgojIgVRbF+PNT4r515Jixo/szUlDchNdpF10pbKIyAGUkpzE2SN6cfaIXokuSovt3ysvRESk3VAgiIgIoEAQEZGIAkFERIAmBoKZTTWzIjNbsI/1V5nZ/Ogx08xG11s33sw+MbNlZvbjessPNbPZZrbUzJ4yM915W0QkgZp6hvAIML6B9SuA09x9FHA7MAXAzJKBPwLnAiOAK81sRLTPL4G73X0osBmY1OzSi4hI3DQpENx9BrCpgfUz3X1z9HYW0C96PRZY5u7L3b0aeBKYYGHy8TOBZ6LtHgUuakH5RUQkTvZHH8Ik4KXo9SHAmnrrCqNlOcAWd6/dY7mIiCRIXC9MM7MzCIFw8s5Fe9nMG1i+t8+8DrgueltuZp+0sHi5QEkL922rOlqdOlp9oOPVqaPVBzpenfZWn4FN2TFugWBmo4AHgXPdvTRaXAj0r7dZP2AdobDdzCwlOkvYufxz3H0KUZ9EK8tX0JRLt9uTjlanjlYf6Hh16mj1gY5Xp9bUJy5NRmY2AJgOTHT3JfVWvQ8MjUYUpQFXAM97mEDpDeDSaLurgefiURYREWmZJp0hmNkTwOlArpkVArcCqQDufh8wmdAvcE90s+pad89391ozuxF4GUgGprr7wuhjbwaeNLM7gA+Ah+JWKxERabYmBYK7X9nI+muAa/ax7kXgxb0sX04YhXSgtLrZqQ3qaHXqaPWBjlenjlYf6Hh1anF92tX01yIisv9o6goREQEOkkDY1/QZ7ZWZrTSzj8xsnpm1yzsG7W06FDPrYWavRtOZvGpmib/VVBPtoz63mdna6HuaZ2bnJbKMzWVm/c3sDTNbZGYLzew70fJ2+T01UJ92+z2ZWYaZvWdmH0Z1+lm0vEVTA3X4JqNo+owlwDmEYbDvA1e6+8cJLVgrmNlKIN/d2+3YaTM7FSgHHnP3kdGyu4BN7v6LKLi7u/vNiSxnU+2jPrcB5e7+v4ksW0uZWR+gj7vPNbMuwBzCjAJfpx1+Tw3U53La6fcUzfrQ2d3LzSwVeBv4DvB9YLq7P2lm9wEfuvu9jX3ewXCGsNfpMxJcpoPePqZDmUCYxgTa2XQmjU3v0h65+3p3nxu9LgMWEWYUaJffUwP1abc8KI/epkYPp4VTAx0MgbCv6TPaMwdeMbM50ZXcHUUvd18P4R8v0DPB5YmHG6NZgKe2l6aVvTGzQcDRwGw6wPe0R32gHX9PZpZsZvOAIuBV4FNaODXQwRAITZ4mox05yd2PIcwi++2ouULannuBw4AxwHrg14ktTsuYWRbwLPBdd9+W6PK01l7q066/J3evc/cxhBkfxgLD97ZZUz7rYAiEfU2f0W65+7rouQj4Kwf2eo79aWPUzruzvbcoweVpFXffGP1jjQEP0A6/p6hd+llgmrtPjxa32+9pb/XpCN8TgLtvAd4ExhFNDRStavIx72AIhL1On5HgMrWYmXWOOsQws87AF4C93rioHXqeMI0JdIDpTHYeNCMX086+p6jD8iFgkbv/pt6qdvk97as+7fl7MrM8M+sWvc4Ezib0jbRoaqAOP8oIIBpG9lt2T5/x8wQXqcXMbDDhrADCleZ/bo/1qT8dCrCRMB3K34CngQHAauAyd28XHbX7qM/phGYIB1YC1+9se28PzOxk4C3gIyAWLb6F0O7e7r6nBupzJe30e7IwqeijhGNbEvC0u/9PdJx4EuhBmBroq+5e1ejnHQyBICIijTsYmoxERKQJFAgiIgIoEEREJKJAEBERQIEgIiIRBYKIiAAKBBERiSgQREQEgP8Pq1eOXVBZyvkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training loss = 1.201739\n",
      "Best validation loss = 1.226921\n"
     ]
    }
   ],
   "source": [
    "main_rbm(F=8, learning_rate_type='time', learning_rate_k=0.5, stopping=True, momentum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_train_loss\n",
    "best_validation_loss\n",
    "min(best_train_predictions)\n",
    "max(best_train_predictions)\n",
    "min(best_validation_predictions)\n",
    "max(best_validation_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(posprods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlStats[\"users\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(vl_r_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(validation_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
