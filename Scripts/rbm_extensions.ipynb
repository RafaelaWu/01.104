{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import projectLib as lib\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "\n",
    "# set highest rating\n",
    "K = 5\n",
    "F = 3\n",
    "eps=0.1\n",
    "\n",
    "def softmax(x):\n",
    "    # Numerically stable softmax function\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def ratingsPerMovie(training):\n",
    "    movies = [x[0] for x in training]\n",
    "    u_movies = np.unique(movies).tolist()\n",
    "    return np.array([[i, movie, len([x for x in training if x[0] == movie])] for i, movie in enumerate(u_movies)])\n",
    "\n",
    "def getV(ratingsForUser):\n",
    "    # ratingsForUser is obtained from the ratings for user library\n",
    "    # you should return a binary matrix ret of size m x K, where m is the number of movies\n",
    "    #   that the user has seen. ret[i][k] = 1 if the user\n",
    "    #   has rated movie ratingsForUser[i, 0] with k stars\n",
    "    #   otherwise it is 0\n",
    "    ret = np.zeros((len(ratingsForUser), K))\n",
    "    for i in range(len(ratingsForUser)):\n",
    "        ret[i, ratingsForUser[i, 1]-1] = 1.0\n",
    "    return ret\n",
    "\n",
    "def getInitialWeights(m, F, K):\n",
    "    # m is the number of visible units\n",
    "    # F is the number of hidden units\n",
    "    # K is the highest rating (fixed to 5 here)\n",
    "    return np.random.normal(0, 0.1, (m, F, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = lib.getTrainingData()\n",
    "ratingsForUser1 = lib.getRatingsForUser(1, training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ratingsPerMovie(training)\n",
    "v = getV(ratingsForUser1)\n",
    "# v.shape\n",
    "w = getInitialWeights(v.shape[0],F,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trStats = lib.getUsefulStats(training)\n",
    "W = getInitialWeights(trStats[\"n_movies\"], F, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # x is a real vector of size n\n",
    "    # ret should be a vector of size n where ret_i = sigmoid(x_i)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def visibleToHiddenVec(v, w, hid_bias):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # v is a matrix of size m x 5. Each row is a binary vector representing a rating\n",
    "    #    OR a probability distribution over the rating\n",
    "    # w is a list of matrices of size m x F x 5\n",
    "    # ret should be a vector of size F\n",
    "    m,f,K=w.shape\n",
    "    output=list()\n",
    "    for i in range(f):\n",
    "        summ=0\n",
    "        for k in range(K):\n",
    "            for j in range(m):\n",
    "                summ+=v[j,k]*w[j,i,k]\n",
    "        output.append(summ)\n",
    "\n",
    "    return sig(np.array(output)+hid_bias)\n",
    "    \n",
    "\n",
    "def hiddenToVisible(h, w, vis_bias):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # h is a binary vector of size F\n",
    "    # w is an array of size m x F x 5\n",
    "    # ret should be a matrix of size m x 5, where m\n",
    "    #   is the number of movies the user has seen.\n",
    "    #   Remember that we do not reconstruct movies that the user\n",
    "    #   has not rated! (where reconstructing means getting a distribution\n",
    "    #   over possible ratings).\n",
    "    #   We only do so when we predict the rating a user would have given to a movie.\n",
    "#     print(w.shape)\n",
    "    output=w[:,0,:]*h[0]\n",
    "    m,f,k=w.shape\n",
    "    for i in range(1,f):\n",
    "        output+=w[:,i,:]*h[i]\n",
    "    return sig(output+vis_bias)\n",
    "\n",
    "def probProduct(v, p):\n",
    "    # v is a matrix of size m x 5\n",
    "    # p is a vector of size F, activation of the hidden units\n",
    "    # returns the gradient for visible input v and hidden activations p\n",
    "    ret = np.zeros((v.shape[0], p.size, v.shape[1]))\n",
    "    for i in range(v.shape[0]):\n",
    "        for j in range(p.size):\n",
    "            for k in range(v.shape[1]):\n",
    "                ret[i, j, k] = v[i, k] * p[j]\n",
    "    return ret\n",
    "\n",
    "def sample(p):\n",
    "    # p is a vector of real numbers between 0 and 1\n",
    "    # ret is a vector of same size as p, where ret_i = Ber(p_i)\n",
    "    # In other word we sample from a Bernouilli distribution with\n",
    "    # parameter p_i to obtain ret_i\n",
    "    samples = np.random.random(p.size)\n",
    "    return np.array(samples <= p, dtype=int)\n",
    "\n",
    "def getPredictedDistribution(v, w, wq,vis_bias,hid_bias):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # This function returns a distribution over the ratings for movie q, if user data is v\n",
    "    # v is the dataset of the user we are predicting the movie for\n",
    "    #   It is a m x 5 matrix, where m is the number of movies in the\n",
    "    #   dataset of this user.\n",
    "    # w is the weights array for the current user, of size m x F x 5\n",
    "    # wq is the weight matrix of size F x 5 for movie q\n",
    "    #   If W is the whole weights array, then wq = W[q, :, :]\n",
    "    # You will need to perform the same steps done in the learning/unlearning:\n",
    "    #   - Propagate the user input to the hidden units\n",
    "    #   - Sample the state of the hidden units\n",
    "    #   - Backpropagate these hidden states to obtain\n",
    "    #       the distribution over the movie whose associated weights are wq\n",
    "    # ret is a vector of size 5\n",
    "    m,f,K=w.shape\n",
    "    p_learn = visibleToHiddenVec(v,w,hid_bias)\n",
    "    PG = probProduct(v, p_learn)\n",
    "    hidden_activations = sample(p_learn)\n",
    "    v_negative = hiddenToVisible(hidden_activations, wq.reshape(1,wq.shape[0],wq.shape[1]),vis_bias)\n",
    "\n",
    "    return v_negative.reshape(5)\n",
    "\n",
    "def predictRatingMax(ratingDistribution):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # ratingDistribution is a probability distribution over possible ratings\n",
    "    #   It is obtained from the getPredictedDistribution function\n",
    "    # This function is one of three you are to implement\n",
    "    # that returns a rating from the distribution\n",
    "    # We decide here that the predicted rating will be the one with the highest probability\n",
    "    max_indices = np.where(ratingDistribution == np.amax(ratingDistribution))[0]\n",
    "    if max_indices.shape[0] == 1:\n",
    "        result = max_indices.item()\n",
    "    else:\n",
    "        result = 2\n",
    "    return result+1\n",
    "\n",
    "def predictRatingMean(ratingDistribution):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # ratingDistribution is a probability distribution over possible ratings\n",
    "    #   It is obtained from the getPredictedDistribution function\n",
    "    # This function is one of three you are to implement\n",
    "    # that returns a rating from the distribution\n",
    "    # We decide here that the predicted rating will be the expectation over ratingDistribution\n",
    "    normalized = [i/sum(ratingDistribution) for i in ratingDistribution]\n",
    "    result = 0 \n",
    "    for k in range(ratingDistribution.shape[0]):\n",
    "        result += normalized[k]*(k+1)\n",
    "    return result\n",
    "\n",
    "def predictRatingExp(ratingDistribution):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # ratingDistribution is a probability distribution over possible ratings\n",
    "    #   It is obtained from the getPredictedDistribution function\n",
    "    # This function is one of three you are to implement\n",
    "    # that returns a rating from the distribution\n",
    "    # We decide here that the predicted rating will be the expectation over\n",
    "    # the softmax applied to ratingDistribution\n",
    "    softmax = np.exp(ratingDistribution)/sum(np.exp(ratingDistribution))\n",
    "#     print(softmax)\n",
    "    result = 0 \n",
    "    for k in range(len(ratingDistribution)):\n",
    "        result += softmax[k]*(k+1)\n",
    "#         print (result)\n",
    "    return result\n",
    "\n",
    "def predictMovieForUser(q, user, W, training,vis_bias,hid_bias, predictType=\"exp\"):\n",
    "    # movie is movie idx\n",
    "    # user is user ID\n",
    "    # type can be \"max\" or \"exp\"\n",
    "    ratingsForUser = lib.getRatingsForUser(user, training)\n",
    "    v = getV(ratingsForUser)\n",
    "#     print(np.where(ratingsForUser[:,0]==q))\n",
    "    ratingDistribution = getPredictedDistribution(v, W[ratingsForUser[:, 0], :, :], W[q, :, :],vis_bias[q],hid_bias)#[np.where(ratingsForUser[:,0]==q)[0][0]]\n",
    "    if predictType == \"max\":\n",
    "        return predictRatingMax(ratingDistribution)\n",
    "    elif predictType == \"mean\":\n",
    "        return predictRatingMean(ratingDistribution)\n",
    "    else:\n",
    "        return predictRatingExp(ratingDistribution)\n",
    "\n",
    "def predict(movies, users, W, training,vis_bias,hid_bias, predictType=\"exp\"):\n",
    "    # given a list of movies and users, predict the rating for each (movie, user) pair\n",
    "    # used to compute RMSE\n",
    "    return [predictMovieForUser(movie, user, W, training,vis_bias,hid_bias, predictType=predictType) for (movie, user) in zip(movies, users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptiveLearn(learning_rate_type=['constant','adapt','time', 'step', 'exponential'], \n",
    "                  lr=0.0001, k=0.1, epoch=None, drop=0.5, epochs_drop=10.0):\n",
    "#time-based decay, step decay and exponential decay\n",
    "    if learning_rate_type == 'constant':\n",
    "        return lr\n",
    "    elif learning_rate_type == 'adapt':\n",
    "        return lr/(epoch**2)\n",
    "    elif learning_rate_type == 'time':\n",
    "        return lr/(1.0+k*epoch)\n",
    "    elif learning_rate_type == 'step':\n",
    "        return lr*np.power(drop,np.floor((1+epoch)/epochs_drop))\n",
    "    elif learning_rate_type == 'exponential':\n",
    "        return lr*np.exp(-k*epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# q = np.random.choice(ratingsForUser1[:, 0])\n",
    "# index = np.where(ratingsForUser1[:, 0]==q)[0].item()\n",
    "# ratingdist1 = getPredictedDistribution(v, W[ratingsForUser1[:, 0], :, :], W[q, :, :])#[index]\n",
    "# print(ratingdist1)\n",
    "# predictRatingMax(ratingdist1)\n",
    "# Problem remains - we need the index to know which movie to take the values from in the \n",
    "# getPredictedDistribution function. Need to check with TA and clarify that part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictForUser(user, W, training, predictType=\"exp\"):\n",
    "    ### TO IMPLEMENT\n",
    "    # given a user ID, predicts all movie ratings for the user\n",
    "    ratingsForUser = lib.getRatingsForUser(user, training)\n",
    "    v = getV(ratingsForUser)\n",
    "    ratings = []\n",
    "    for i in range(np.shape(ratingsForUser)[0]):\n",
    "        ratingDistribution = getPredictedDistribution(v, W[ratingsForUser[:, 0], :, :], W[ratingsForUser[:,0][i], :, :])[i]\n",
    "#         print(ratingDistribution)\n",
    "#     return v\n",
    "#     ratingDistribution = getPredictedDistribution(v, W[ratingsForUser[:, 0], :, :], )\n",
    "        if predictType == \"max\":\n",
    "            ratings.append(predictRatingMax(ratingDistribution))\n",
    "        elif predictType == \"mean\":\n",
    "            ratings.append(predictRatingMean(ratingDistribution))\n",
    "        else:\n",
    "#             print (predictRatingExp(ratingDistribution))\n",
    "            ratings.append(predictRatingExp(ratingDistribution))\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_get(array, B):\n",
    "    ret = []\n",
    "    for i in range(int(len(array)/B)):\n",
    "        ret.append(array[i*B:i*B+B])\n",
    "    if len(array)%B != 0:\n",
    "        ret.append(array[len(array)/B:])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rbm\n",
    "import rbm_sy\n",
    "import projectLib as lib\n",
    "\n",
    "training = lib.getTrainingData()\n",
    "validation = lib.getValidationData()\n",
    "# You could also try with the chapter 4 data\n",
    "# training = lib.getChapter4Data()\n",
    "\n",
    "trStats = lib.getUsefulStats(training)\n",
    "vlStats = lib.getUsefulStats(validation)\n",
    "\n",
    "K = 5\n",
    "alpha = 0.9\n",
    "\n",
    "# SET PARAMETERS HERE!!!\n",
    "# number of hidden units\n",
    "F = 5\n",
    "epochs = 30\n",
    "gradientLearningRate = 0.0001\n",
    "gradientLearningRate_v = 0.001\n",
    "gradientLearningRate_h = 0.001\n",
    "_lambda = 1\n",
    "minibatch_size = 10\n",
    "\n",
    "def main_rbm(training=training, validation=validation, trStats=trStats, vlStats=vlStats, \n",
    "             K=5, F=5, epochs=30, gradientLearningRate=0.0001,gradientLearningRate_v = 0.001,\n",
    "             gradientLearningRate_h = 0.001, minibatch_size=10, alpha=0.9, \n",
    "             stopping=False, momentum=False, learning_rate_type='time', learning_rate_k=0.1, \n",
    "             learning_rate_drop=0.5, learning_rate_epochs_drop=10.0, _lambda = 0.3):\n",
    "    \n",
    "    # Print current hyperparams\n",
    "#     frame = inspect.currentframe()\n",
    "#     args, _, _, values = inspect.getargvalues(frame)\n",
    "#     print ('Training and Predicting with the following hyperparameters:')\n",
    "#     for i in args[4:]:\n",
    "#         print (\"    %s = %s\" % (i, values[i]))\n",
    "        \n",
    "    # Initialise all our arrays\n",
    "    num_movies=trStats[\"n_movies\"]\n",
    "    num_users=trStats[\"n_users\"]\n",
    "    W = rbm_sy.getInitialWeights(trStats[\"n_movies\"], F, K)\n",
    "    posprods = np.zeros(W.shape)\n",
    "    negprods = np.zeros(W.shape)\n",
    "    grad_w = np.zeros(W.shape)\n",
    "    m_w=np.zeros((W.shape[0],F,5))\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    vis_bias=np.zeros((num_movies,5))\n",
    "    m_v=np.zeros((num_movies,5))\n",
    "    hid_bias=np.zeros((F,))\n",
    "    m_h=np.zeros((F,))\n",
    "    best_train_loss = 100\n",
    "    best_validation_loss = 100\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "    #     mini_batch_grads = []\n",
    "        # in each epoch, we'll visit all users in a random order\n",
    "        visitingOrder = np.array(trStats[\"u_users\"])\n",
    "        np.random.shuffle(visitingOrder)\n",
    "#         for i in range(0, visitingOrder.shape[0], minibatch_size):\n",
    "#                 # Get pair of (X, y) of the current minibatch/chunk\n",
    "#             visitingOrderMini = visitingOrder[i:i + minibatch_size]\n",
    "#                 y_train_mini = y_train[i:i + minibatch_size]\n",
    "#         for i in range(0, visitingOrder.shape[0], minibatch_size):\n",
    "        batches = batch_get(visitingOrder, minibatch_size)\n",
    "        for batch in batches:\n",
    "            prev_grad = grad_w\n",
    "            grad_w = np.zeros(W.shape)\n",
    "            for user in batch:\n",
    "                # get the ratings of that user\n",
    "                ratingsForUser = lib.getRatingsForUser(user, training)\n",
    "\n",
    "                # build the visible input\n",
    "                v = rbm_sy.getV(ratingsForUser)\n",
    "\n",
    "                # get the weights associated to movies the user has seen\n",
    "                weightsForUser = W[ratingsForUser[:, 0], :, :]\n",
    "\n",
    "                ### LEARNING ###\n",
    "                # propagate visible input to hidden units\n",
    "                posHiddenProb = rbm_sy.visibleToHiddenVec(v, weightsForUser) #, hid_bias)\n",
    "                # get positive gradient\n",
    "                # note that we only update the movies that this user has seen!\n",
    "                posprods[ratingsForUser[:, 0], :, :] += probProduct(v, posHiddenProb)\n",
    "\n",
    "                ### UNLEARNING ###\n",
    "                # sample from hidden distribution\n",
    "                sampledHidden = rbm_sy.sample(posHiddenProb)\n",
    "                # propagate back to get \"negative data\"\n",
    "                negData = rbm_sy.hiddenToVisible(sampledHidden, weightsForUser)#, vis_bias[ratingsForUser[:,0]])\n",
    "                # propagate negative data to hidden units\n",
    "                negHiddenProb = rbm_sy.visibleToHiddenVec(negData, weightsForUser) #, hid_bias)\n",
    "                # get negative gradient\n",
    "                # note that we only update the movies that this user has seen!\n",
    "                negprods[ratingsForUser[:, 0], :, :] += rbm_sy.probProduct(negData, negHiddenProb)\n",
    "\n",
    "                poshidact = sum(posHiddenProb)\n",
    "                posvisact = sum(v)\n",
    "                neghidact = sum(negHiddenProb)\n",
    "                negvisact = sum(negData)\n",
    "                # we average over the number of users in the batch (if we use mini-batch)\n",
    "    #             grad = (gradientLearningRate/epoch)*(posprods-negprods)\n",
    "                '''\n",
    "                Regularization - \n",
    "                '''\n",
    "                grad_w += adaptiveLearn(learning_rate_type=learning_rate_type, k=learning_rate_k, \n",
    "                                        drop=learning_rate_drop, epochs_drop=learning_rate_epochs_drop, \n",
    "                                        epoch=epoch)*((posprods-negprods)/trStats[\"n_users\"]-_lambda*W)\n",
    "        #         mini_batch_grads.append(grad)\n",
    "\n",
    "            #     m = alpha*m+grad\n",
    "                '''\n",
    "                Ask about the implementation of biases (should we create matrix of biases for hidden and visible layers?)\n",
    "                '''\n",
    "            m_w = alpha*m_w + grad_w\n",
    "#             m_v = alpha*m_v+(gradientLearningRate_v) * (posvisact - negvisact)\n",
    "#             m_h = alpha*m_h+(gradientLearningRate_h) * (poshidact - neghidact)\n",
    "\n",
    "            if momentum == False:\n",
    "                W += grad_w\n",
    "            else:\n",
    "                W += m_w\n",
    "\n",
    "            vis_bias += m_v\n",
    "            hid_bias += m_h\n",
    "\n",
    "        # Print the current RMSE for training and validation sets\n",
    "        # this allows you to control for overfitting e.g\n",
    "        # We predict over the training set\n",
    "        tr_r_hat = rbm_sy.predict(trStats[\"movies\"], trStats[\"users\"], W, training) #, vis_bias, hid_bias, predictType='exp')\n",
    "    #     print (tr_r_hat)\n",
    "        trRMSE = lib.rmse(trStats[\"ratings\"], tr_r_hat)\n",
    "    #     print (trRMSE)\n",
    "        if trRMSE < best_train_loss:\n",
    "            best_train_loss = trRMSE\n",
    "            best_training_weights = W\n",
    "            best_train_predictions = tr_r_hat\n",
    "\n",
    "        # We predict over the validation set\n",
    "        vl_r_hat = rbm_sy.predict(vlStats[\"movies\"], vlStats[\"users\"], W, training) #, vis_bias, hid_bias, predictType='exp')\n",
    "    #     vl_r_hat\n",
    "        vlRMSE = lib.rmse(vlStats[\"ratings\"], vl_r_hat)\n",
    "        if vlRMSE < best_validation_loss:\n",
    "            best_validation_loss = vlRMSE\n",
    "            best_validation_weights = W\n",
    "            best_validation_predictions = vl_r_hat\n",
    "#             best_momentum = momentum\n",
    "#             best_reg = regularization\n",
    "#             best_epoch = epoch\n",
    "#             best_alpha = alpha\n",
    "#             best_B = B\n",
    "#             best_F = F\n",
    "#             min_rmse = vlRMSE\n",
    "#                     print('Best RMSE:', min_rmse)\n",
    "\n",
    "        train_loss.append(trRMSE)\n",
    "        validation_loss.append(vlRMSE)\n",
    "\n",
    "        print (\"### EPOCH %d ###\" % epoch)\n",
    "        print (\"Training loss = %f\" % trRMSE)\n",
    "        print (\"Validation loss = %f\" % vlRMSE)\n",
    "\n",
    "    ### END ###\n",
    "    # This part you can write on your own\n",
    "    # you could plot the evolution of the training and validation RMSEs for example\n",
    "    # predictedRatings = np.array([predictForUser(user, W, training) for user in trStats[\"u_users\"]])\n",
    "    # np.savetxt(\"predictedRatings.txt\", predictedRatings)\n",
    "    # fig1 = plt.figure()\n",
    "    # ax1 = fig1.add_subplot(121)\n",
    "    # ax1.plot(train_loss)\n",
    "    # ax2 = fig1.add_subplot(122)\n",
    "    # ax2.plot(validation_loss)\n",
    "    \n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(validation_loss)\n",
    "    plt.show()\n",
    "    if stopping==True:\n",
    "        print('Best training loss = %f' % best_train_loss)\n",
    "        print('Best validation loss = %f' % best_validation_loss)\n",
    "    else:\n",
    "        print('Final training loss = %f' % trRMSE)\n",
    "        print('Final validation loss = %f' % vlRMSE)\n",
    "    return [best_validation_loss, best_validation_predictions, best_validation_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(training=training, validation=validation, trStats=trStats, vlStats=vlStats, \n",
    "                             K=5, F=8, epochs=30, gradientLearningRate=0.001,gradientLearningRate_v = 0.001,\n",
    "                             gradientLearningRate_h = 0.001, minibatch_size=10, alpha=0.9, \n",
    "                             stopping=True, momentum=True, learning_rate_type='adapt', learning_rate_k=0.5, \n",
    "                             learning_rate_drop=0.5, learning_rate_epochs_drop=10.0, _lambda = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mrange = np.linspace(0.7,0.95,5)\n",
    "mrange = [0.7, 0.8, 0.9, 0.95]\n",
    "# rrange = np.linspace(0.1,0.9,5)\n",
    "rrange = [0.001, 0.01, 0.1, 0.5]\n",
    "arange = [0.0001, 0.001, 0.01, 0.1]\n",
    "brange = [5, 10, 20, 30]\n",
    "frange = [5, 8, 10, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_momentum = 0\n",
    "best_reg = 0\n",
    "best_epoch = 0\n",
    "best_B = 0\n",
    "best_F = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_rmse=10\n",
    "for momentum in mrange:\n",
    "    for regularization in rrange:\n",
    "        for learning_rate in arange:\n",
    "            for batch in brange:\n",
    "                for F in frange:\n",
    "                    prediction = main_rbm(training=training, validation=validation, trStats=trStats, vlStats=vlStats, \n",
    "                                 K=5, F=int(F), epochs=30, gradientLearningRate=learning_rate, gradientLearningRate_v = 0.001,\n",
    "                                 gradientLearningRate_h = 0.001, minibatch_size=int(batch), alpha=momentum, \n",
    "                                 stopping=True, momentum=True, learning_rate_type='time', learning_rate_k=0.5, \n",
    "                                 learning_rate_drop=0.5, learning_rate_epochs_drop=10.0, _lambda = regularization)\n",
    "                    if prediction[0] < min_rmse:\n",
    "                        best_momentum = momentum\n",
    "                        best_reg = regularization\n",
    "#                             best_epoch = epoch\n",
    "                        best_alpha = learning_rate\n",
    "#                         print(best_alpha)\n",
    "                        best_B = batch\n",
    "                        best_F = F\n",
    "                        best_predict = prediction[1]\n",
    "                        min_rmse = prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_momentum\n",
    "best_reg\n",
    "best_alpha #this is the learning rate\n",
    "best_B\n",
    "best_F\n",
    "min_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predict\n",
    "np.savetxt(\"SpateggiChikenChoope+v1.txt\", best_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(momentum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(learning_rate_type='time', learning_rate_k=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(learning_rate_type='step', learning_rate_drop=0.5, learning_rate_epochs_drop=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(learning_rate_type='exponential', learning_rate_k=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(F=8, learning_rate_type='time', learning_rate_k=0.5, stopping=True, momentum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_train_loss\n",
    "best_validation_loss\n",
    "min(best_train_predictions)\n",
    "max(best_train_predictions)\n",
    "min(best_validation_predictions)\n",
    "max(best_validation_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = main_rbm(training=training, validation=validation, trStats=trStats, vlStats=vlStats, \n",
    "             K=5, F=30, epochs=30, gradientLearningRate=0.001, gradientLearningRate_v = 0.001,\n",
    "             gradientLearningRate_h = 0.001, minibatch_size=10, alpha=0.9, \n",
    "             stopping=True, momentum=True, learning_rate_type='time', learning_rate_k=0.5, \n",
    "             learning_rate_drop=0.5, learning_rate_epochs_drop=10.0, _lambda = 0.1)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedRatings = np.array([rbm_sy.predictForUser(user, W, training) for user in trStats[\"u_users\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"SpateggiChikenChoope+v1.txt\", predictedRatings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
