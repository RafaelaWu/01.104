{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import projectLib as lib\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "\n",
    "# set highest rating\n",
    "K = 5\n",
    "F = 3\n",
    "eps=0.1\n",
    "\n",
    "def softmax(x):\n",
    "    # Numerically stable softmax function\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def ratingsPerMovie(training):\n",
    "    movies = [x[0] for x in training]\n",
    "    u_movies = np.unique(movies).tolist()\n",
    "    return np.array([[i, movie, len([x for x in training if x[0] == movie])] for i, movie in enumerate(u_movies)])\n",
    "\n",
    "def getV(ratingsForUser):\n",
    "    # ratingsForUser is obtained from the ratings for user library\n",
    "    # you should return a binary matrix ret of size m x K, where m is the number of movies\n",
    "    #   that the user has seen. ret[i][k] = 1 if the user\n",
    "    #   has rated movie ratingsForUser[i, 0] with k stars\n",
    "    #   otherwise it is 0\n",
    "    ret = np.zeros((len(ratingsForUser), K))\n",
    "    for i in range(len(ratingsForUser)):\n",
    "        ret[i, ratingsForUser[i, 1]-1] = 1.0\n",
    "    return ret\n",
    "\n",
    "def getInitialWeights(m, F, K):\n",
    "    # m is the number of visible units\n",
    "    # F is the number of hidden units\n",
    "    # K is the highest rating (fixed to 5 here)\n",
    "    return np.random.normal(0, 0.1, (m, F, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = lib.getTrainingData()\n",
    "ratingsForUser1 = lib.getRatingsForUser(1, training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ratingsPerMovie(training)\n",
    "v = getV(ratingsForUser1)\n",
    "# v.shape\n",
    "w = getInitialWeights(v.shape[0],F,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trStats = lib.getUsefulStats(training)\n",
    "W = getInitialWeights(trStats[\"n_movies\"], F, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # x is a real vector of size n\n",
    "    # ret should be a vector of size n where ret_i = sigmoid(x_i)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def visibleToHiddenVec(v, w, hid_bias):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # v is a matrix of size m x 5. Each row is a binary vector representing a rating\n",
    "    #    OR a probability distribution over the rating\n",
    "    # w is a list of matrices of size m x F x 5\n",
    "    # ret should be a vector of size F\n",
    "    m,f,K=w.shape\n",
    "    output=list()\n",
    "    for i in range(f):\n",
    "        summ=0\n",
    "        for k in range(K):\n",
    "            for j in range(m):\n",
    "                summ+=v[j,k]*w[j,i,k]\n",
    "        output.append(summ)\n",
    "\n",
    "    return sig(np.array(output)+hid_bias)\n",
    "    \n",
    "\n",
    "def hiddenToVisible(h, w, vis_bias):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # h is a binary vector of size F\n",
    "    # w is an array of size m x F x 5\n",
    "    # ret should be a matrix of size m x 5, where m\n",
    "    #   is the number of movies the user has seen.\n",
    "    #   Remember that we do not reconstruct movies that the user\n",
    "    #   has not rated! (where reconstructing means getting a distribution\n",
    "    #   over possible ratings).\n",
    "    #   We only do so when we predict the rating a user would have given to a movie.\n",
    "#     print(w.shape)\n",
    "    output=w[:,0,:]*h[0]\n",
    "    m,f,k=w.shape\n",
    "    for i in range(1,f):\n",
    "        output+=w[:,i,:]*h[i]\n",
    "    return sig(output+vis_bias)\n",
    "\n",
    "def probProduct(v, p):\n",
    "    # v is a matrix of size m x 5\n",
    "    # p is a vector of size F, activation of the hidden units\n",
    "    # returns the gradient for visible input v and hidden activations p\n",
    "    ret = np.zeros((v.shape[0], p.size, v.shape[1]))\n",
    "    for i in range(v.shape[0]):\n",
    "        for j in range(p.size):\n",
    "            for k in range(v.shape[1]):\n",
    "                ret[i, j, k] = v[i, k] * p[j]\n",
    "    return ret\n",
    "\n",
    "def sample(p):\n",
    "    # p is a vector of real numbers between 0 and 1\n",
    "    # ret is a vector of same size as p, where ret_i = Ber(p_i)\n",
    "    # In other word we sample from a Bernouilli distribution with\n",
    "    # parameter p_i to obtain ret_i\n",
    "    samples = np.random.random(p.size)\n",
    "    return np.array(samples <= p, dtype=int)\n",
    "\n",
    "def getPredictedDistribution(v, w, wq,vis_bias,hid_bias):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # This function returns a distribution over the ratings for movie q, if user data is v\n",
    "    # v is the dataset of the user we are predicting the movie for\n",
    "    #   It is a m x 5 matrix, where m is the number of movies in the\n",
    "    #   dataset of this user.\n",
    "    # w is the weights array for the current user, of size m x F x 5\n",
    "    # wq is the weight matrix of size F x 5 for movie q\n",
    "    #   If W is the whole weights array, then wq = W[q, :, :]\n",
    "    # You will need to perform the same steps done in the learning/unlearning:\n",
    "    #   - Propagate the user input to the hidden units\n",
    "    #   - Sample the state of the hidden units\n",
    "    #   - Backpropagate these hidden states to obtain\n",
    "    #       the distribution over the movie whose associated weights are wq\n",
    "    # ret is a vector of size 5\n",
    "    m,f,K=w.shape\n",
    "    p_learn = visibleToHiddenVec(v,w,hid_bias)\n",
    "    PG = probProduct(v, p_learn)\n",
    "    hidden_activations = sample(p_learn)\n",
    "    v_negative = hiddenToVisible(hidden_activations, wq.reshape(1,wq.shape[0],wq.shape[1]),vis_bias)\n",
    "\n",
    "    return v_negative.reshape(5)\n",
    "\n",
    "def predictRatingMax(ratingDistribution):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # ratingDistribution is a probability distribution over possible ratings\n",
    "    #   It is obtained from the getPredictedDistribution function\n",
    "    # This function is one of three you are to implement\n",
    "    # that returns a rating from the distribution\n",
    "    # We decide here that the predicted rating will be the one with the highest probability\n",
    "    max_indices = np.where(ratingDistribution == np.amax(ratingDistribution))[0]\n",
    "    if max_indices.shape[0] == 1:\n",
    "        result = max_indices.item()\n",
    "    else:\n",
    "        result = 2\n",
    "    return result+1\n",
    "\n",
    "def predictRatingMean(ratingDistribution):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # ratingDistribution is a probability distribution over possible ratings\n",
    "    #   It is obtained from the getPredictedDistribution function\n",
    "    # This function is one of three you are to implement\n",
    "    # that returns a rating from the distribution\n",
    "    # We decide here that the predicted rating will be the expectation over ratingDistribution\n",
    "    normalized = [i/sum(ratingDistribution) for i in ratingDistribution]\n",
    "    result = 0 \n",
    "    for k in range(ratingDistribution.shape[0]):\n",
    "        result += normalized[k]*(k+1)\n",
    "    return result\n",
    "\n",
    "def predictRatingExp(ratingDistribution):\n",
    "    ### TO IMPLEMENT ###\n",
    "    # ratingDistribution is a probability distribution over possible ratings\n",
    "    #   It is obtained from the getPredictedDistribution function\n",
    "    # This function is one of three you are to implement\n",
    "    # that returns a rating from the distribution\n",
    "    # We decide here that the predicted rating will be the expectation over\n",
    "    # the softmax applied to ratingDistribution\n",
    "    softmax = np.exp(ratingDistribution)/sum(np.exp(ratingDistribution))\n",
    "#     print(softmax)\n",
    "    result = 0 \n",
    "    for k in range(len(ratingDistribution)):\n",
    "        result += softmax[k]*(k+1)\n",
    "#         print (result)\n",
    "    return result\n",
    "\n",
    "def predictMovieForUser(q, user, W, training,vis_bias,hid_bias, predictType=\"exp\"):\n",
    "    # movie is movie idx\n",
    "    # user is user ID\n",
    "    # type can be \"max\" or \"exp\"\n",
    "    ratingsForUser = lib.getRatingsForUser(user, training)\n",
    "    v = getV(ratingsForUser)\n",
    "#     print(np.where(ratingsForUser[:,0]==q))\n",
    "    ratingDistribution = getPredictedDistribution(v, W[ratingsForUser[:, 0], :, :], W[q, :, :],vis_bias[q],hid_bias)#[np.where(ratingsForUser[:,0]==q)[0][0]]\n",
    "    if predictType == \"max\":\n",
    "        return predictRatingMax(ratingDistribution)\n",
    "    elif predictType == \"mean\":\n",
    "        return predictRatingMean(ratingDistribution)\n",
    "    else:\n",
    "        return predictRatingExp(ratingDistribution)\n",
    "\n",
    "def predict(movies, users, W, training,vis_bias,hid_bias, predictType=\"exp\"):\n",
    "    # given a list of movies and users, predict the rating for each (movie, user) pair\n",
    "    # used to compute RMSE\n",
    "    return [predictMovieForUser(movie, user, W, training,vis_bias,hid_bias, predictType=predictType) for (movie, user) in zip(movies, users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptiveLearn(learning_rate_type=['constant','time', 'step', 'exponential'], \n",
    "                  lr=0.0001, k=0.1, epoch=None, drop=0.5, epochs_drop=10.0):\n",
    "#time-based decay, step decay and exponential decay\n",
    "    if learning_rate_type == 'constant':\n",
    "        return lr\n",
    "    elif learning_rate_type == 'time':\n",
    "        return lr/(1.0+k*epoch)\n",
    "    elif learning_rate_type == 'step':\n",
    "        return lr*np.power(drop,np.floor((1+epoch)/epochs_drop))\n",
    "    elif learning_rate_type == 'exponential':\n",
    "        return lr*np.exp(-k*epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# q = np.random.choice(ratingsForUser1[:, 0])\n",
    "# index = np.where(ratingsForUser1[:, 0]==q)[0].item()\n",
    "# ratingdist1 = getPredictedDistribution(v, W[ratingsForUser1[:, 0], :, :], W[q, :, :])#[index]\n",
    "# print(ratingdist1)\n",
    "# predictRatingMax(ratingdist1)\n",
    "# Problem remains - we need the index to know which movie to take the values from in the \n",
    "# getPredictedDistribution function. Need to check with TA and clarify that part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictForUser(user, W, training, predictType=\"exp\"):\n",
    "    ### TO IMPLEMENT\n",
    "    # given a user ID, predicts all movie ratings for the user\n",
    "    ratingsForUser = lib.getRatingsForUser(user, training)\n",
    "    v = getV(ratingsForUser)\n",
    "    ratings = []\n",
    "    for i in range(np.shape(ratingsForUser)[0]):\n",
    "        ratingDistribution = getPredictedDistribution(v, W[ratingsForUser[:, 0], :, :], W[ratingsForUser[:,0][i], :, :])[i]\n",
    "#         print(ratingDistribution)\n",
    "#     return v\n",
    "#     ratingDistribution = getPredictedDistribution(v, W[ratingsForUser[:, 0], :, :], )\n",
    "        if predictType == \"max\":\n",
    "            ratings.append(predictRatingMax(ratingDistribution))\n",
    "        elif predictType == \"mean\":\n",
    "            ratings.append(predictRatingMean(ratingDistribution))\n",
    "        else:\n",
    "#             print (predictRatingExp(ratingDistribution))\n",
    "            ratings.append(predictRatingExp(ratingDistribution))\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rbm\n",
    "import projectLib as lib\n",
    "\n",
    "training = lib.getTrainingData()\n",
    "validation = lib.getValidationData()\n",
    "# You could also try with the chapter 4 data\n",
    "# training = lib.getChapter4Data()\n",
    "\n",
    "trStats = lib.getUsefulStats(training)\n",
    "vlStats = lib.getUsefulStats(validation)\n",
    "\n",
    "K = 5\n",
    "alpha = 0.9\n",
    "\n",
    "# SET PARAMETERS HERE!!!\n",
    "# number of hidden units\n",
    "F = 5\n",
    "epochs = 30\n",
    "gradientLearningRate = 0.0001\n",
    "gradientLearningRate_v = 0.001\n",
    "gradientLearningRate_h = 0.001\n",
    "_lambda = 1\n",
    "minibatch_size = 10\n",
    "\n",
    "def main_rbm(training=training, validation=validation, trStats=trStats, vlStats=vlStats, \n",
    "             K=5, F=5, epochs=30, gradientLearningRate=0.0001,gradientLearningRate_v = 0.001,\n",
    "             gradientLearningRate_h = 0.001, minibatch_size=None, alpha=0.9, \n",
    "             stopping=False, momentum=False, learning_rate_type='time', learning_rate_k=0.1, \n",
    "             learning_rate_drop=0.5, learning_rate_epochs_drop=10.0, _lambda = 0.3):\n",
    "    frame = inspect.currentframe()\n",
    "    args, _, _, values = inspect.getargvalues(frame)\n",
    "    print ('Training and Predicting with the following hyperparameters:')\n",
    "    for i in args[4:]:\n",
    "        print (\"    %s = %s\" % (i, values[i]))\n",
    "#     return [(i, values[i]) for i in args]\n",
    "    # Initialise all our arrays\n",
    "    num_movies=trStats[\"n_movies\"]\n",
    "    num_users=trStats[\"n_users\"]\n",
    "    W = getInitialWeights(trStats[\"n_movies\"], F, K)\n",
    "    posprods = np.zeros(W.shape)\n",
    "    negprods = np.zeros(W.shape)\n",
    "    m_w=np.zeros((W.shape[0],F,5))\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    vis_bias=np.zeros((num_movies,5))\n",
    "    m_v=np.zeros((num_movies,5))\n",
    "    hid_bias=np.zeros((F,))\n",
    "    m_h=np.zeros((F,))\n",
    "    best_train_loss = 100\n",
    "    best_validation_loss = 100\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "    #     mini_batch_grads = []\n",
    "        # in each epoch, we'll visit all users in a random order\n",
    "        visitingOrder = np.array(trStats[\"u_users\"])\n",
    "        np.random.shuffle(visitingOrder)\n",
    "#         for i in range(0, visitingOrder.shape[0], minibatch_size):\n",
    "#                 # Get pair of (X, y) of the current minibatch/chunk\n",
    "#             visitingOrderMini = visitingOrder[i:i + minibatch_size]\n",
    "#                 y_train_mini = y_train[i:i + minibatch_size]\n",
    "#         for i in range(0, visitingOrder.shape[0], minibatch_size):\n",
    "        for user in visitingOrder:\n",
    "            # get the ratings of that user\n",
    "            ratingsForUser = lib.getRatingsForUser(user, training)\n",
    "\n",
    "            # build the visible input\n",
    "            v = getV(ratingsForUser)\n",
    "\n",
    "            # get the weights associated to movies the user has seen\n",
    "            weightsForUser = W[ratingsForUser[:, 0], :, :]\n",
    "\n",
    "            ### LEARNING ###\n",
    "            # propagate visible input to hidden units\n",
    "            posHiddenProb = visibleToHiddenVec(v, weightsForUser, hid_bias)\n",
    "            # get positive gradient\n",
    "            # note that we only update the movies that this user has seen!\n",
    "            posprods[ratingsForUser[:, 0], :, :] += probProduct(v, posHiddenProb)\n",
    "\n",
    "            ### UNLEARNING ###\n",
    "            # sample from hidden distribution\n",
    "            sampledHidden = sample(posHiddenProb)\n",
    "            # propagate back to get \"negative data\"\n",
    "            negData = hiddenToVisible(sampledHidden, weightsForUser, vis_bias[ratingsForUser[:,0]])\n",
    "            # propagate negative data to hidden units\n",
    "            negHiddenProb = visibleToHiddenVec(negData, weightsForUser, hid_bias)\n",
    "            # get negative gradient\n",
    "            # note that we only update the movies that this user has seen!\n",
    "            negprods[ratingsForUser[:, 0], :, :] += probProduct(negData, negHiddenProb)\n",
    "            \n",
    "            poshidact = sum(posHiddenProb)\n",
    "            posvisact = sum(v)\n",
    "            neghidact = sum(negHiddenProb)\n",
    "            negvisact = sum(negData)\n",
    "            # we average over the number of users in the batch (if we use mini-batch)\n",
    "#             grad = (gradientLearningRate/epoch)*(posprods-negprods)\n",
    "            '''\n",
    "            Regularization - \n",
    "            '''\n",
    "            grad_w = adaptiveLearn(learning_rate_type=learning_rate_type, k=learning_rate_k, drop=learning_rate_drop, \n",
    "                                 epochs_drop=learning_rate_epochs_drop, epoch=epoch)*(posprods-negprods+2*_lambda*W/np.linalg.norm(W))\n",
    "    #         mini_batch_grads.append(grad)\n",
    "\n",
    "        #     m = alpha*m+grad\n",
    "            '''\n",
    "            Ask about the implementation of biases (should we create matrix of biases for hidden and visible layers?)\n",
    "            '''\n",
    "            m_w = alpha*m_w+grad_w\n",
    "            m_v = alpha*m_v+(gradientLearningRate_v) * (posvisact - negvisact)\n",
    "            m_h = alpha*m_h+(gradientLearningRate_h) * (poshidact - neghidact)\n",
    "            \n",
    "            if momentum == False:\n",
    "                W += grad_w\n",
    "            else:\n",
    "                W += m_w\n",
    "            \n",
    "            vis_bias += m_v\n",
    "            hid_bias += m_h\n",
    "\n",
    "        # Print the current RMSE for training and validation sets\n",
    "        # this allows you to control for overfitting e.g\n",
    "        # We predict over the training set\n",
    "        tr_r_hat = predict(trStats[\"movies\"], trStats[\"users\"], W, training, vis_bias, hid_bias, predictType='exp')\n",
    "    #     print (tr_r_hat)\n",
    "        trRMSE = lib.rmse(trStats[\"ratings\"], tr_r_hat)\n",
    "    #     print (trRMSE)\n",
    "        if trRMSE < best_train_loss:\n",
    "            best_train_loss = trRMSE\n",
    "            best_training_weights = W\n",
    "            best_train_predictions = tr_r_hat\n",
    "\n",
    "        # We predict over the validation set\n",
    "        vl_r_hat = predict(vlStats[\"movies\"], vlStats[\"users\"], W, training, vis_bias, hid_bias, predictType='exp')\n",
    "    #     vl_r_hat\n",
    "        vlRMSE = lib.rmse(vlStats[\"ratings\"], vl_r_hat)\n",
    "        if vlRMSE < best_validation_loss:\n",
    "            best_validation_loss = vlRMSE\n",
    "            best_validation_weights = W\n",
    "            best_validation_predictions = vl_r_hat\n",
    "\n",
    "        train_loss.append(trRMSE)\n",
    "        validation_loss.append(vlRMSE)\n",
    "\n",
    "        print (\"### EPOCH %d ###\" % epoch)\n",
    "        print (\"Training loss = %f\" % trRMSE)\n",
    "        print (\"Validation loss = %f\" % vlRMSE)\n",
    "\n",
    "    ### END ###\n",
    "    # This part you can write on your own\n",
    "    # you could plot the evolution of the training and validation RMSEs for example\n",
    "    # predictedRatings = np.array([predictForUser(user, W, training) for user in trStats[\"u_users\"]])\n",
    "    # np.savetxt(\"predictedRatings.txt\", predictedRatings)\n",
    "    # fig1 = plt.figure()\n",
    "    # ax1 = fig1.add_subplot(121)\n",
    "    # ax1.plot(train_loss)\n",
    "    # ax2 = fig1.add_subplot(122)\n",
    "    # ax2.plot(validation_loss)\n",
    "    \n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(validation_loss)\n",
    "    plt.show()\n",
    "    if stopping==True:\n",
    "        print('Best training loss = %f' % best_train_loss)\n",
    "        print('Best validation loss = %f' % best_validation_loss)\n",
    "    else:\n",
    "        print('Final training loss = %f' % trRMSE)\n",
    "        print('Final validation loss = %f' % vlRMSE)\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(momentum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(learning_rate_type='time', learning_rate_k=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(learning_rate_type='step', learning_rate_drop=0.5, learning_rate_epochs_drop=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_rbm(learning_rate_type='exponential', learning_rate_k=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Predicting with the following hyperparameters:\n",
      "    K = 5\n",
      "    F = 5\n",
      "    epochs = 30\n",
      "    gradientLearningRate = 0.0001\n",
      "    gradientLearningRate_v = 0.001\n",
      "    gradientLearningRate_h = 0.001\n",
      "    minibatch_size = None\n",
      "    alpha = 0.9\n",
      "    stopping = True\n",
      "    momentum = True\n",
      "    learning_rate_type = time\n",
      "    learning_rate_k = 0.5\n",
      "    learning_rate_drop = 0.5\n",
      "    learning_rate_epochs_drop = 10.0\n",
      "    _lambda = 0.3\n",
      "### EPOCH 1 ###\n",
      "Training loss = 1.185182\n",
      "Validation loss = 1.216354\n",
      "### EPOCH 2 ###\n",
      "Training loss = 1.182419\n",
      "Validation loss = 1.210508\n",
      "### EPOCH 3 ###\n",
      "Training loss = 1.178705\n",
      "Validation loss = 1.214499\n",
      "### EPOCH 4 ###\n",
      "Training loss = 1.168371\n",
      "Validation loss = 1.218619\n",
      "### EPOCH 5 ###\n",
      "Training loss = 1.164149\n",
      "Validation loss = 1.211016\n",
      "### EPOCH 6 ###\n",
      "Training loss = 1.163718\n",
      "Validation loss = 1.216788\n",
      "### EPOCH 7 ###\n",
      "Training loss = 1.162464\n",
      "Validation loss = 1.214884\n",
      "### EPOCH 8 ###\n",
      "Training loss = 1.158345\n",
      "Validation loss = 1.217273\n",
      "### EPOCH 9 ###\n",
      "Training loss = 1.154328\n",
      "Validation loss = 1.217912\n",
      "### EPOCH 10 ###\n",
      "Training loss = 1.151834\n",
      "Validation loss = 1.214950\n",
      "### EPOCH 11 ###\n",
      "Training loss = 1.143910\n",
      "Validation loss = 1.218891\n",
      "### EPOCH 12 ###\n",
      "Training loss = 1.139741\n",
      "Validation loss = 1.220749\n",
      "### EPOCH 13 ###\n",
      "Training loss = 1.137970\n",
      "Validation loss = 1.218670\n",
      "### EPOCH 14 ###\n",
      "Training loss = 1.136909\n",
      "Validation loss = 1.215426\n",
      "### EPOCH 15 ###\n",
      "Training loss = 1.136981\n",
      "Validation loss = 1.221391\n",
      "### EPOCH 16 ###\n",
      "Training loss = 1.136422\n",
      "Validation loss = 1.223177\n",
      "### EPOCH 17 ###\n",
      "Training loss = 1.134484\n",
      "Validation loss = 1.220964\n",
      "### EPOCH 18 ###\n",
      "Training loss = 1.135790\n",
      "Validation loss = 1.215565\n",
      "### EPOCH 19 ###\n",
      "Training loss = 1.135275\n",
      "Validation loss = 1.224164\n",
      "### EPOCH 20 ###\n",
      "Training loss = 1.134572\n",
      "Validation loss = 1.222924\n",
      "### EPOCH 21 ###\n",
      "Training loss = 1.131800\n",
      "Validation loss = 1.222841\n",
      "### EPOCH 22 ###\n",
      "Training loss = 1.131985\n",
      "Validation loss = 1.221311\n",
      "### EPOCH 23 ###\n",
      "Training loss = 1.131252\n",
      "Validation loss = 1.218655\n",
      "### EPOCH 24 ###\n",
      "Training loss = 1.128205\n",
      "Validation loss = 1.217713\n",
      "### EPOCH 25 ###\n",
      "Training loss = 1.128779\n",
      "Validation loss = 1.222963\n",
      "### EPOCH 26 ###\n",
      "Training loss = 1.126975\n",
      "Validation loss = 1.220549\n",
      "### EPOCH 27 ###\n",
      "Training loss = 1.126088\n",
      "Validation loss = 1.218836\n",
      "### EPOCH 28 ###\n",
      "Training loss = 1.125902\n",
      "Validation loss = 1.220644\n",
      "### EPOCH 29 ###\n",
      "Training loss = 1.123481\n",
      "Validation loss = 1.220815\n",
      "### EPOCH 30 ###\n",
      "Training loss = 1.124960\n",
      "Validation loss = 1.216459\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl4VdW9//H3NzOEBAgJYZ7nScAIKqAoTigtTqA41FoVa6Xa9tp6byfb29ufHbTaKlVxnocKDuCEMyIIBJBJZIYwJyRhSCDjWb8/VlBUQhJyTk6S83k9T54kZ++cfDebfPbaa6+9tjnnEBGRyBEV7gJERKRuKfhFRCKMgl9EJMIo+EVEIoyCX0Qkwij4RUQijIJfRCTCKPhFRCKMgl9EJMLEhLuAo0lNTXVdunQJdxkiIg3G4sWL9zjn0qqzbr0M/i5dupCZmRnuMkREGgwz21LdddXVIyISYRT8IiIRRsEvIhJhFPwiIhFGwS8iEmEU/CIiEUbBLyISYerlOH4RqQbnYOnTENsUOp0MzTuEuyJpIBT8Ig3Vqhnw+k+//j65A3QaDp1OgY7DIb0/REWHrz6ptxT8Isfr0F5Y8yYc2AUDJ0CLjnX7u9/+H2g7GL53L2xdCFnzYcs8WDndrxOXBB1Pgo4n+zOC9AEQHQtRMUd8NODe3oN5kNCiYW9DmJhzLtw1fEdGRobTlA1SLx0O+1WvwoYPIFDqX7co6DUWTroOup0R+jB6478g8zG44QNoN+Tr152DvVmwdYE/EGQtgOwvgMr+zu1bB4Jof3BoOxh6ng09zoJW3UO7Lcdj9Sz4zw+hZWc45WY4YRLENgl3VWFlZoudcxnVWlfBL1KFQ3thzVuw6pWvw755R+g3HvpfDImpsORJWPwkHNwDKd0g4zoYfAU0TQl+PdsWwyNjYPiNMPav1at/2yLIXQ+BsiM+yis+jvy+DEoLYct8yNvgfz6le8VB4GzoMiL8AbvqVZh+nT+DAdj5OTRtBSfdAMNu8PsjAin4RWqraN83w7685Iiwvwjanwhm3/yZsmJYPRMWPeJb2zEJMOASfxbQ/sTg1FVeBg+PhsI9cPNCSEgOzvseTd5GWPcerJsNmz+BsiKIaQJdR/mDQM+z/EGuLq14GWZMhg4nwZX/gfgk2PIpzLsP1r7t/81PmOTPAlJ71m1tYRbU4Dezx4BxQLZzbsBRll8J3F7xbQFwk3NumZl1BJ4C2gABYJpz7p/VKUrBL2G1ayU8cQEU7fUXTPtfWHnYH+s9Mh+FZS/6FnS7IXDS9TBwIsTEHX9t8+6H2b+BiU/5g1BdKT0Emz+F9e/6A0HeRv966/4w6hf+3yfUF5KXvQiv/thfvL7iJYhv9s3lOWth/v2w7AUoL4be58OpP/XrV3e/NWDBDv7T8IH+VCXBfyqw2jmXb2ZjgT8454abWVugrXNuiZklAYuBC51zX1RVlII/ggUCkDXPt+z2bfMt5n7jIa5p3fz+vVvh0bMBgwmPQ4dhteuvL9oPy1/0ZwE5X0L3MXD5cxCbcHy1TR0GXUbBFS+GN8xyN8C6d2HxE5CzGlJ7wWm/ggEXh+YAsPRZeO1mf7Yx6QWIS6x83YIcWPQwLHwYDuVBu6Ew/MfQe2xwzpAKsuGL12DnMn8WVFbkz/Yq++wCcPJNMOJnId1nQe/qMbMuwKyjBf+31msJrHTOtT/KsteA+51z71b1+xT8EcY52LHUj0ZZOQMO7PBj0xNT/YXK+GQYeCkM/YG/6BiqP56DefDYuXBgN/zobUjvF7z3ds6H5Kyf+W6Sy5+FmPiavcfzk2DDh3DzAn9Rsz4IBGD1a/Dx3/xF5FY94bRf+gN2dJAGDS5+EmbeCt1G+4NmdRsBJQdh2fMwf6q/XhEd5y+89/2ePxtIbFX9Ggr3wOrX/f/PLZ/6ME9s7Q9AMQl+X8Y28Z8Pf3/4875tvrtwwKXw/ftC1ogJZ/DfBvRxzl1/lJ+fAwxwzu2v5GcnA5MBOnXqdOKWLdV+poA0VDlrfMt+5cu+6yAq1l9EHHCJb53FNvXDE5c85VtYZYcgfaA/AAyaAE1aBq+W0kPw1HjY8TlcPQO6jAzeex9p8ZMw8xboeS5c9nT1w3/1LHjxSjjrjzDyZ6GprTYCAfhypj8A7F7pLwif/isfdrU5ACx6xI9g6nE2XPbM8Z0pBQKwbaG//rL6dd+YsCjoPAL6fh/6XADNv9NW9Q2BL2f5sN80B1w5tOrhL+gPuBha963e73cO5v4D3v8TtB3kD14huNkuLMFvZmcA/wZGOudyj3i9GfAx8Gfn3IzqFKUWfyO2bxus+A+smA67VwDmT98HTvAtscrC/NBef4BY8rQfxREdD/2+D0Ou9l0ftemOKS+Dl672F3MnPhn6vvPMx33Lv9dY31dfVZ9/8QGYOtyPWb/xYz/csr4KBGDNG/DRX/3+TenmzwAGTqz5AWDBQ/DWr6DXeRX/TjU8Qzoa52DX8oqDwEzf/QbQPsP//+txll++cgZs/NCPcmrZ1Qd9/4v8SKLjPeNc8zZMv94fvC57xt9bEUR1HvxmNgh4BRjrnFt7xOuxwCzgHefcP6pTEDTQ4M/6zJ8CjviZ7pY8Gud8n+vs3/gRMh1O8q3B/hdCUpuavdfOZf4AsOIlP/qmZRcYdRsMvrLmBwDnfAgvfgLG/h2GT67Zzx+vRY/CG7+A3hfAhCeOHf5v/xo++zdcNxs6Dqub+morEPD3O3z8F9i1wodnr/N891nr/tC6z7H76edPhXd+DX3GwaWP1+6C+LHkrPVnKqtn+u7Gw5p3ggEX+bAPZvdizhrfZbc3Cy64C078YXDelzoOfjPrBHwA/MA5N++I1w14EshzztXo3LRBBb9zfijZe3/wp4Jn/y+MuDXcVdWOc1CYAwd2+ot2tR23fTDPTy3w5SzoeQ6M/RukdK19naWH/B/sgodge6a/EHvB3f50uro++it89P9g5C/grDtqX1NNLHwY3rzNh9uEJ47ekt+5DKaNhqHX+Dt0Gxrn/JnU/Pt9sJYerFhg/oCd3h9a9/v6gJDSza/73h3+zOuSR+vuDGdvFmz8yNdTkxFcNXUoH16+Dja87+89OO/OoGxjsEf1PA+MBlKB3cAdQCyAc+5BM3sEuAQ43Clf5pzLMLORwCfACvxwToBfO+ferKqoBhP8Rfv8SIPVM/1pYqDcj3S44YOahU9dKyuB/dth31Y/UmTfNtiX5T8f/r682K+b1A7G/A4GXXZ8ZzJZC/zNNgd2wVl/8OOrg/0HFQj4i3jv/t6P4hg2Gc74NSQ0P/bPHe5vP+EKuPDf4Rklc7g7o+/34dLHvhkAgXJ45Cy/n6YsCu41jXAIBCB/k78IvPsLyF7lP+dt8BdLwXfhlRf76zwXTQveBeL6JlDuD27z7vNdlROerNnF5qPQDVx1YddK3y+cvwXO/iOcMsW3bB84FZq0gMkfBecOR+eCE0jO+dbM/Km+pXH4D+2wZun+BqXmHfycM807+j7lBQ/CjiXQZiCc/Sfofkb1fl8gAJ/eAx/82b/fpY8F7yamyhzK9xfQMh+DZq3hnD/70UBH+/db8xa8cIUfXjnp+fD2m8//N7zzP9DvwooWbkXYLZgGb/0SLn7EX8xurEoP+b723V/4g0KTFjDi54039I+07AV4/RZISofLn4c2xxw/c0yRG/w7PvdX3b99Y0ewff48zPq5b1FOeBw6n/r1svXvwTOX+HHD1bmdvjLO+RbBosegz/kwaCJ0HV3zP4ayYj9Mcv5UP9oisTUMnuS7cA4HffMOlV84CwT8LJDv/9GfCvc423dnHWuoY0G2v7ty44d+BMT37q269R1M2xf7kSA7lvrW1AV3Q1rvr5dvXQhPft/3M18zK/T/X6rj8I1Z/S+Gix/2XW33nwQdToSrX42IG5Ai1vbF8MKVvgfhogePe3BBZAb/wTy4d5C/+DXphdBcDCotgrdv9xcCu4zyrbOk9O+u99btvqV81XQ/SuB4zLkLPviTn1kxZ7X/T5GY5k+BB06E9kOPHQYH83zLd+HDULDL91uecrMfPXM8oyPKimHhNJjzdz/KZMjVvjvl2xdmN3wAM26E4v3+wDf0mvCEVqDc76f3/xdKCvwZ2em/gn3b4bFzfLfJj2ZDs7S6r60yn/4L3v2d38flpbD2HfjJ/Po5SZoE14Fd8OJVvnH10yXH1RiJzOAHP9Lj9Sl+tMjFDwd3hsT8LfDSD/xQwhE/gzN/V3nru/SQvyB3KB9uml/zvrvDIz4GToSLHvKTgq2bDctf8mFQXuzHSQ+c4M8EjgyG3A1+BMjSZ/249+5jfOB3PzM4AXwwz4f/wof9DTEjboVTp/ivP/x/MPce37q+9PHg3gB1vAr3wLt3wOfP+OkXwP/7XfducC4wB9vce/xAAYAzfuMPVhIZyoohf/M3z05rIHKDH77+wxlWMXNhMMJu7WyYcYPvfrnoQd/1UpWdy+HhM6H3eTDx6erXsXK6v+Lf8xx/d+e3+54P7fU3oSx/CTbPBZzvO+833g8pXfOW/5lBE+Hkm0MXvrkbfPfPF69BszaQ3NZ3rQz9AZz317qbYqG6sj7z3T/5W+CHM785lXF9M3+q37cTngjO2HWJCJEd/M7B7N/6IWFn/tbfPHK8AuXw0Z2+hZs+EC57qmazEX76Tz/SZPxUGHJV1euvfw+euxw6ZMBVM6oOz33b/YFi+Uv+ZpkmKX4isGE3+IubdSFrge+bzlkD4+7xF1Prq0A5lBSGdkZLkTCJ7OAHf0Hy1Ztg+Qsw7l7IuLbm77F/p7/LbstcH9rn31XzUTqB8oppAJbCj+ceu2shawE8faHvwvnhLD+yoSb2ZvlrAOGYK905f1OWWqciYVOT4G+czyyLioLx9/vukjd+4bsjamL9e/DgSD+M8cIHfYv9eAI1KhoufAAsGl650U8NcDS7V8FzE/yF0qtn1Dz0AVp0Ct8DMswU+iINSOMMfvD93BOe9FMDTL/eT7JUlfIyeO+Pfjhms9Z+LP7gSbWro0VHGPcP/yi8ufd8d3neJnj6IohN9MP26qqLRkQiVuMNfvB95JNe8N0nz1/hx/lXZt92eHKcn0Vv6A/g+veP++r6dwy81I/A+ehO/9i8ww7s8t075SVw9Sv1Z6pdEWnUGnfwg3/m6dUz/LjtZy7xo1G+be1s37Wza4W/SzIUc2affxcktfWjg0oK/VDPpy/2D424crq/mUhEpA40/uAHSG7nW9Q438Lev9O/Xl4Ks3/n+9eT28Pkj0N3a3yTFn4oaN5GePOX8NxlkLvOD9nsEOKpDEREjhAZwQ+Q2gOufNnfgPTMJX6uncfPh3n/gowfwfXv+nVCqesoGHELfP4sbFsElzxS/blvRESCJAJmQTpC+6G+hf3sBHhwBMQl+cnDBlxSdzWc8Rv/aL8eY+r2YdkiIhUiK/jBP7fz0sfh8+fgnD/V/TwoMfFw8UN1+ztFRI4QecEP0Hec/xARiUCR08cvIiKAgl9EJOIo+EVEIoyCX0Qkwij4RUQijIJfRCTCKPhFRCKMgl9EJMIo+EVEIoyCX0Qkwij4RUQijIJfRCTCKPhFRCKMgl9EJMIo+EVEIkyVwW9mj5lZtpmtrGT5lWa2vOJjnpmdcMSy88xsjZmtN7P/DmbhIiJyfKrT4n8COO8YyzcBpzvnBgF/AqYBmFk0MBUYC/QDJplZv1pVKyIitVZl8Dvn5gB5x1g+zzmXX/HtZ0CHiq+HAeudcxudcyXAC4AeMisiEmbB7uO/Dnir4uv2wNYjlm2reE1ERMIoaM/cNbMz8ME/8vBLR1nNHePnJwOTATp16hSsskRE5FuC0uI3s0HAI8B451xuxcvbgI5HrNYB2FHZezjnpjnnMpxzGWlpacEoS0REjqLWwW9mnYAZwNXOubVHLFoE9DSzrmYWB1wOvF7b3yciIrVTZVePmT0PjAZSzWwbcAcQC+CcexD4PdAK+LeZAZRVtNzLzGwK8A4QDTzmnFsVkq0QEZFqM+cq7XYPm4yMDJeZmRnuMkREGgwzW+ycy6jOurpzV0Qkwij4RUQijIJfRCTCKPhFRCKMgl9EJMIo+EVEIoyCX0Qkwij4RUQijIJfRCTCKPhFRCKMgl9EJMIo+EVEIoyCX0Qkwij4RUQijIJfRCTCKPhFRCKMgl9EJMIo+EVEIoyCX0Qkwij4RUQijIJfRCTCKPhFRCJMowr+vQdLwl2CiEi912iCv6i0nPP/+QnXPr6Qldv3hbscEZF6q9EEP8BVp3RmSdZext03l5ueWcy63QfCXZKISL1jzrlw1/AdGRkZLjMz87h+dn9RKY9+solH526isKSMCwe359YxPemSmhjkKkVE6g8zW+ycy6jWuo0t+A/LKyzhoTkbeHLeZkrLHRMzOjDlzJ60b9EkSFWKiNQfCv4jZB8o4t8fbuC5BVkAXDG8Ez85ozutkxKC8v4iIvWBgv8otu89xP0frOOlzG3ERhvXjujKrWN6khAbHdTfIyISDjUJ/kZ1cfdY2rdowp0XD+L9X5zO2AFteeCjDYy7b65GAIlIxKky+M3sMTPLNrOVlSzvY2bzzazYzG771rKfm9kqM1tpZs+bWdj7V7qkJnLPZYN55rrhFBSVceHUT7n/g3WUlQfCXZqISJ2oTov/CeC8YyzPA24B7jryRTNrX/F6hnNuABANXH58ZQbfyJ6pvPOz0zh/YFvumr2WiQ/NZ0tuYbjLEhEJuSqD3zk3Bx/ulS3Pds4tAkqPsjgGaGJmMUBTYMfxFhoKzZvG8q9JQ/jn5YNZn13A2H9+wvMLs6iP1z1ERIIlZH38zrnt+LOALGAnsM85NztUv682xg9uzzs/P40hnVrwPzNWcP2TmeQcKA53WSIiIRGy4DezlsB4oCvQDkg0s6uOsf5kM8s0s8ycnJxQlVWpts2b8PSPhnPH9/oxd/0ezr13Du+s2lXndYiIhFooR/WcBWxyzuU450qBGcCpla3snJvmnMtwzmWkpaWFsKzKRUX5YZ6zfjqSdi0SuPHpxfzyP8soKC4LSz0iIqEQyuDPAk42s6ZmZsAYYHUIf1/Q9ExPYsZNI5hyRg+mL9nG71876oAmEZEGKaaqFczseWA0kGpm24A7gFgA59yDZtYGyASSgYCZ/Qzo55xbYGYvA0uAMmApMC0kWxECcTFR3HZub0rKAzzyyUZ+Mro7PVonhbssEZFai5g7d49XXmEJo/76AaN7t2bqlUPDXY6IyFHpzt0gSkmM47qRXXljxU7d5SsijYKCvxquG9WN5IQY7nl3bbhLERGpNQV/NTRvEsuNp3fn/S+zWZKVH+5yRERqRcFfTT88tQupzeK4e/aacJciIlIrCv5qSoyP4abRPfh0fS7zNuwJdzkiIsdNwV8DVw7vRJvkBO6evVbz+YhIg6Xgr4GE2GimnNmDxVvy+Wht3U8rISISDAr+GpqY0ZGOKU24e/YatfpFpEFS8NdQXEwUt47pxcrt+zWJm4g0SAr+43DRkPZ0T0vkH++upTygVr+INCwK/uMQHWX8/OxerN1dwMxl9erZMiIiVVLwH6fzB7Slb9tk7n1vLaV6Xq+INCAK/uMUFWX819m92Jx7kOmLt4W7HBGRalPw18KYvq05oWML/vX+OorLysNdjohItSj4a8HM+OU5vdmxr4gXFm4NdzkiItWi4K+lET1aMbxrCvd/uJ5DJWr1i0j9p+CvJTPjtnN7k3OgmKfmbw53OSIiVVLwB8FJXVI4vVcaUz9cT+bmvHCXIyJyTAr+IPnj9/uTkhjH5dM+46n5mzWdg4jUWwr+IOmSmshrU0Zyeq80fv/aKv7rP8soKlWfv4jUPwr+IGreJJaHf5DBz8/qxStLt3PJA/PYmncw3GWJiHyDgj/IoqKMW8/qyaPXZJCVd5Dv3T+XOZrCWUTqEQV/iJzZJ52ZU0aSnpTANY8vZOqH69XvLyL1goI/hLqkJvLKzacyblA7/v7OGn78zGIOFJWGuywRiXAK/hBrGhfDvy4fzG8v6Mt7q7MZP/VT1mcfCHdZIhLBYsJdQCQwM64f1Y3+7Zoz5bkljL//U84d0IZWiXGkJMaTkhhb8TmOVolxtEyMIzkhBjMLd+ki0ggp+OvQKd1bMeuWkfx6xgo+25BLbmEJxWVHn9I5Ntpo2TSOts0TGNKpJSd1SeGkLi1pnZxQx1WLSGNj9fGCY0ZGhsvMzAx3GSHnnONQaTm5BSXkHywht7CEvCO+zi8sYXNuIZ9v3UtRqT9AdG7VlIzO/iCQ0SWF7mmJOjMQEcxssXMuozrrqsUfRmZG07gYmqbE0DGlaaXrlZYHWLVjP5mb81i0OY+P1mQzfYl/BkBKYhwndm7JSV1actGQDqQlxddV+SLSQKnF3wA559i0p5DMzfks2pxH5pZ8Nu0pJLVZPPdfMYSTu7UKd4kiUsdq0uJX8DcSX+7az0+eXcKW3IP88tze3HhaN3UBiUSQmgR/lcM5zewxM8s2s5WVLO9jZvPNrNjMbvvWshZm9rKZfWlmq83slOptgtRUnzbJvD5lJOf1b8Nf3vqSyU8vZt8h3TMgIt9VnXH8TwDnHWN5HnALcNdRlv0TeNs51wc4AVhd0wKl+prFx3D/FUP4/bh+fPhlNt+/fy6rduwLd1kiUs9UGfzOuTn4cK9sebZzbhHwjealmSUDpwGPVqxX4pzbW7typSpmxo9GduXFG0+mqLSci/89j5cy9VhIEflaKO/c7QbkAI+b2VIze8TMEitb2cwmm1mmmWXm5GhSs9o6sXMKb9wyihM7t+RXLy/n9peXa5poEQFCG/wxwFDgAefcEKAQ+O/KVnbOTXPOZTjnMtLS0kJYVuRIbRbP09cNZ8oZPXgxcyuXPDCPrFxNEy0S6UIZ/NuAbc65BRXfv4w/EEgdio7yzwR+9JoMtuYdZNx9n/DeF7vDXZaIhFHIgt85twvYama9K14aA3wRqt8nxzambzpv3DKKTq2acv1Tmdz55mpKy48+XYSING5VjuM3s+eB0UAqsBu4A4gFcM49aGZtgEwgGQgABUA/59x+MxsMPALEARuBa51z+VUVpXH8oVNUWs7/vfEFz3yWxdBOLbj/iqG0a9Ek3GWJSC3pBi6p0sxlO/ifGSuIiTb+MfEEzuyTHu6SRKQWgnoDlzRO3zuhHTN/OpJ2zZvwoycyufMtdf2IRAoFfwTrmprIjJ+cypXDO/HQxxuZNO0zduw9FO6yRCTEFPwRLiE2mj9fNJB/Xj6Y1Tv3c8G/PuHDL7PDXZaIhJCCXwAYP7g9M386kvTkBK59YhF/eetLdf2INFIKfvlKt7RmvHrzCCYN68SDH29g0rTP2LSnMNxliUiQKfjlGxJio7nz4q+7fsbc/RE3P7uEFds02ZtIY6EncMlRjR/cnlO6t+LxTzfzzPwtvLFiJyN7pHLT6O6c2r2V5voXacA0jl+qtL+olOcWZPHo3E3kHChmYPvm3DS6O+f2b0N0lA4AIvWBbuCSkCgqLeeVpduZNmcjm/YU0jU1kRtGdePioe1JiI0Od3kiEU3BLyFVHnDMXrWLBz7ewPJt+0hLimfKGT245tQu4S5NJGLVJPjVxy81Fh1ljB3YlvMGtGH+hlzu+2A9d7y+itZJ8Ywd2Dbc5YlIFTSqR46bmXFqj1Seum4YA9s357evriS3oDjcZYlIFRT8Umux0VHcPfEEDhSV8dtXV1Ifuw9F5GsKfgmKXulJ/OKcXry1chczl+8MdzkicgwKfgmaG0Z1Y0inFvz+tZVkHygKdzkiUgkFvwRNdJRx14QTOFRSzq9nrFCXj0g9peCXoOqe1oxfndeH91ZnM33J9nCXIyJHoeCXoLv21C4M65LCH2euYuc+ze8vUt8o+CXooqKMv08YRFm54/bp6vIRqW8U/BISnVsl8uvz+zBnbQ4vLNoa7nJE5AgKfgmZK4d35tTurfi/WV+wNe9guMsRkQoKfgmZqCjjb5cOwsy4ffpyAgF1+YjUBwp+CakOLZvy2wv6Mm9DLs8s2BLuckQEBb/UgctO6sjpvdK4880v2axHOYqEnYJfQs7M+MslA4mJNn758jLK1eUjElYKfqkTbZs34Q/f68+izfn84fVVlJYHwl2SSMTSfPxSZy4e2p4vdu7n0bmb+HLXfqZeMZTWyQnhLksk4qjFL3XGzPjduH788/LBrNy+nwvum8vCTXnhLksk4ij4pc6NH9yeV28eQbP4GCY9/BmPzt2ku3tF6pCCX8Kid5skXpsygjF9WvOnWV/w0+eXUlhcFu6yRCJClcFvZo+ZWbaZraxkeR8zm29mxWZ221GWR5vZUjObFYyCpfFITojloatP5Pbz+vDmip1cOPVT1mcXhLsskUavOi3+J4DzjrE8D7gFuKuS5bcCq2tWlkQKM+Om0d15+rrh5BWWMP7+uby1Qk/wEgmlKoPfOTcHH+6VLc92zi0CSr+9zMw6ABcAj9SmSGn8RvRIZeZPR9IzPYmbnl3CnW+upkxDPkVCItR9/PcCvwKq/As2s8lmlmlmmTk5OSEuS+qjdi2a8OKNJ3P1yZ15aM5GLn1wPi8uyiK/sCTcpYk0KiELfjMbB2Q75xZXZ33n3DTnXIZzLiMtLS1UZUk9Fx8TzZ8uHMA9l51AbmExt09fwUl/fo+rH13ACwuzyNNBQKTWQnkD1wjg+2Z2PpAAJJvZM865q0L4O6WRuGhIBy4c3J6V2/fzxoqdvLliJ/89YwW/eXUlp3ZvxdgBbTm3fzqtmsWHu1SRBseqM37azLoAs5xzA46xzh+AAufcdy7ymtlo4Dbn3LjqFJWRkeEyMzOrs6pECOccq3Z8fRDYknuQ6Cjj5G4pnD+wLad0a0WXVolERVm4SxUJCzNb7JzLqNa6VQW/mT0PjAZSgd3AHUAsgHPuQTNrA2QCyfi+/AKgn3Nu/xHvMRoFvwSJc44vdu7nzRU7eXPFLjZVzPiZFB9Dv3bJDGzfnAEVH91SdTCQyBDU4A8HBb9Ul3OOtbsL+HxrPiu272PF9v2s3rmfkjI/niAxLpr+7Q4fCJLpmpqIAwIBR1nAffW53DnKyys+BxwB52jRJI42zeNpnZxAUnwMZjqASP1Vk+DXJG3SoJkZvdsk0btNEpeA+34SAAALpklEQVSd5F8rLQ+wPruAFdv3sXL7PlZs38dzC7dQVHr8w0ObxkXTJjmB9OQE0pPjSW+eQJtk/9EtrRm90pvpwCANhoJfGp3Y6Cj6tk2mb9tkJmZ0BKCsPMCGnEK25ftrA9FRRrTZ119/6yPKjLzCEnbvL2L3/iJ27Sv2n/cXsWhzPtkHiigt//psuVd6M39Bekg72jZvEq5NF6kWdfWIHIdAwJF/sIRd+4tYkrWXV5duZ/GWfMzglG6tuHBIe8YOaENSQmy13m9b/kEWbspjwcY8Fm7OY3v+IVKbxZGWnEBas3haJ8fTOimetKR4WiclfPV1arN44mI05Zaoj18kLLbkFvLK0u28snQ7W3IPEh8TxTn923DRkHaM6plGbLQPaOccW3IPsmBTLgsqwn773kMAJCfEMKxrCt3SmpFbUEL2gSJyDhSTc6CY3EruYUhPjqdTSlM6pjSlY8umdEppSqdW/uvWSfG6uB0hFPwiYeScY+nWvbyyZDszl+9g78FSWiXGcf7Atuw9VMrCTbns3l8MQKvEOIZ1TWF41xSGdW1FnzZJlQZ1aXngq4NB9v5icgp899P2/ENk5R1ka95Bdu4v4sg/6biYKDq2bELHlKYMaNecH47oQqrufWiUFPwi9URJWYCP1mTzytLtvL86m5aJsQzv2ophXVM4uVsK3dOCe1G4uKycHXuLvjoQbM07SFbFx+qd+0mIjebaEV2YPKo7zZtWrxtKGgYFv0g9VFoeICbKwjb6Z312Afe+t5ZZy3eSnBDD5NO6ce2IriTGa4xHY6DgF5FKfbFjP/94dw3vrc6mVWIcN43uzlUndyYhNjrcpUktKPhFpEpLsvL5x+y1zF2/hzbJCUw5swcTMzpqlFADpeAXkWqbvyGXu2avYfGWfDqmNOHWMb3onZ5EUVk5xaUBikrLKSorp+jw16XlFJf5r2OiouiY0oTOrZrSKSWR1GZxupEtTBT8IlIjzjk+WpPDXbPXsGrH/qp/oBKJcdF0TGlKl1aJ/mDQqimdU/zXHVo20UEhhDRlg4jUiJlxRp/WnN4rjc825VJYXE5CbBQJsdHEx/jPCTHRJMRGER/rP8dFR1FcFmBb/iGy8grZknuw4qOQtdkH+ODLbEqOeIrayd1S+N24fvRv1zyMWyqgFr+IhEh5wLFrfxFbcgtZsW0fD368gb2HSrn8pI784uzepCXpfoJgUlePiNQ7+w6Wct8H63hi3mYSYqO5+YweXDuii0YTBUlNgl+X70WkTjRvGstvx/Vj9s9P4+Rurfjr219y9j0f89aKndTHBmhjpuAXkTrVLa0Zj1yTwTPXDadpbAw3PbuEy6Z9xsrt+6r186XlAbbvPcTybXs5WFIW4mobJ3X1iEjYlJUHeDFzK3fPXkv+wRImnNiBa07twr5Dpeza56fB3rWv6Kuvd+4rYk9B8VfzESXERjGmTzoXDGrLGb1b0yQucruN1McvIg3KvkOlTP1wPY9/uukbzzkAaN4k1j/05vDDb5r7jxZNYpm3IZe3Vu5kT0EJTeOiGdM3nXGD2nJ6r7SIu3ag4BeRBmlLbiGfb91LWlI8bZs3oU1yQpWt+PKAY8HGXGat2MnbK3eRV1hCs/gYzurbmnGD2jGqVyrxMY3/IKDgF5GIVFYeYP7GXGYt28nbq3ax71ApSfExnNYrjT5tkuiZ3oye6Ul0TmlKTHTjusSp4BeRiFdaHmDu+j3MWraTBZty2ZZ/6KtlcdFRdEtLpEfrZvRsnUSv9Gb0TG9G51aJXz0wp6HRnbsiEvFio6M4o3drzujdGoCDJWWszy5g3e4C1mUXsG73AZZv28cbK3Z+dbE4JspolhDz9V3K37hbOZqEw3cxx0bRsWVTLh/WqUHeiKYWv4hEtEMl5WzIKWBd9gHWZxdwoKjMT05XVl4xKd3hieoCFJd+/druA0XERkdx8ZD2XD+qKz1aJ4V1O9TiFxGppiZx0Qxo35wB7Ws2h9DGnAIenbuJlxdv44VFWxnTpzU3nNaN4V1TajQZ3YGiUuZtyOWTdTnkFpTwwFUn1nQTakwtfhGRWsgtKOaZz7J4av5mcgtLGNShOTeM6sbYAW2OegE5EHCs3LGPOWtzmLN2D0uy8ikLOJrGRTOiRyoPXDn0uC486+KuiEgdKyotZ8aS7TzyyUY27imkfYsmXDeyKxNP6khhcZkP+nV7mLsuh/yDpQAMaJ/MaT3TGNUzjRM7t6zVQ3AU/CIiYRIION7/MpuH52xk4eY8EmKjKCr101OnNovntJ6pnNYrjZE9U0ltFrwLw+rjFxEJk6go4+x+6ZzdL52lWflMX7KN9i2aclqvVPq2SSYqKvwPo1Hwi4iEyJBOLRnSqWW4y/iOhnmngoiIHLcqg9/MHjOzbDNbWcnyPmY238yKzey2I17vaGYfmtlqM1tlZrcGs3ARETk+1WnxPwGcd4zlecAtwF3fer0M+C/nXF/gZOBmM+t3PEWKiEjwVBn8zrk5+HCvbHm2c24RUPqt13c655ZUfH0AWA20r125IiJSW3XSx29mXYAhwIK6+H0iIlK5kAe/mTUDpgM/c87tP8Z6k80s08wyc3JyQl2WiEjECmnwm1ksPvSfdc7NONa6zrlpzrkM51xGWlpaKMsSEYloIQt+87MUPQqsds79I1S/R0REaqbKKRvM7HlgNJAK7AbuAGIBnHMPmlkbIBNIBgJAAdAPGAR8AqyoeB3g1865N6ssyiwH2FLzzYGKOvcc58/WR41te6DxbVNj2x5ofNvU2LYHvrtNnZ1z1eouqZdz9dSGmWVWd76KhqCxbQ80vm1qbNsDjW+bGtv2QO22SXfuiohEGAW/iEiEaYzBPy3cBQRZY9seaHzb1Ni2BxrfNjW27YFabFOj6+MXEZFja4wtfhEROYZGE/xmdp6ZrTGz9Wb23+GuJxjMbLOZrTCzz82sQT6S7Gizu5pZipm9a2brKj7XvwnLK1HJ9vzBzLZX7KfPzez8cNZYE5XNotvA91Fl29Qg95OZJZjZQjNbVrE9f6x4vauZLajYRy+aWVy137MxdPWYWTSwFjgb2AYsAiY5574Ia2G1ZGabgQznXIMdf2xmp+Hv7XjKOTeg4rW/AXnOub9UHKRbOuduD2ed1VXJ9vwBKHDOfXuG2nrPzNoCbZ1zS8wsCVgMXAj8kIa7jyrbpok0wP1UcTNsonOuoGI2hLnArcAvgBnOuRfM7EFgmXPugeq8Z2Np8Q8D1jvnNjrnSoAXgPFhrkmodHbX8cCTFV8/if+jbBCqmq22oTnGLLoNeR81qpmBnVdQ8W1sxYcDzgRerni9RvuosQR/e2DrEd9vowHv6CM4YLaZLTazyeEuJojSnXM7wf+RAq3DXE8wTDGz5RVdQQ2mW+RI35pFt1Hso6PMDNwg95OZRZvZ50A28C6wAdjrnCurWKVGmddYgv9oTy9u+H1YMMI5NxQYi3+QzWnhLkiO6gGgOzAY2AncHd5yaq66s+g2JEfZpga7n5xz5c65wUAHfA9H36OtVt33ayzBvw3oeMT3HYAdYaolaJxzOyo+ZwOv4Hd4Y7C7oh/2cH9sdpjrqRXn3O6KP8wA8DANbD9VMotug95HR9umhr6fAJxze4GP8E81bGFmMRWLapR5jSX4FwE9K65yxwGXA6+HuaZaMbPEigtTmFkicA5w1OceN0CvA9dUfH0N8FoYa6m1wwFZ4SIa0H46xiy6DXYfVbZNDXU/mVmambWo+LoJcBb+usWHwKUVq9VoHzWKUT0AFUOz7gWigcecc38Oc0m1Ymbd8K18gBjguYa4TZXM7voq8BLQCcgCJjjnGsQF00q2ZzS++8ABm4EbD/eP13dmNpKjzKKL7xNvqPuosm2aRAPcT2Y2CH/xNhrfWH/JOfe/FRnxApACLAWucs4VV+s9G0vwi4hI9TSWrh4REakmBb+ISIRR8IuIRBgFv4hIhFHwi4hEGAW/iEiEUfCLiEQYBb+ISIT5//eoC23kZ11rAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training loss = 1.123481\n",
      "Best validation loss = 1.210508\n"
     ]
    }
   ],
   "source": [
    "main_rbm(F=5, learning_rate_type='time', learning_rate_k=0.5, stopping=True, momentum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_train_loss\n",
    "best_validation_loss\n",
    "min(best_train_predictions)\n",
    "max(best_train_predictions)\n",
    "min(best_validation_predictions)\n",
    "max(best_validation_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(posprods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlStats[\"users\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(vl_r_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(validation_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
